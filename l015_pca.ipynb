{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4734ff01-ef87-45a5-a83e-c857f86e00a0",
   "metadata": {},
   "source": [
    "# Principal Components Analysis (PCA)\n",
    "\n",
    "PCA is an **unsupervised learning algorithm** that reduces the number of features (or dimensions) in a dataset while retaining as much of the variability (information) as possible. It is especially useful when you have high-dimensional data (e.g., 50, 1000, or more features) and want to visualize it on a 2D or 3D plot.\n",
    "\n",
    "- **Purpose**: Simplify datasets for visualization, data compression, or even (historically) to speed up some learning algorithms.\n",
    "- **Key Idea**: Transform the original features into a new set of axes (principal components) that capture the largest variance.\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition and Examples\n",
    "\n",
    "### Visualization with Cars and Countries\n",
    "\n",
    "Imagine you have data on passenger cars with various features such as length, width, height, and wheel diameter.\n",
    "\n",
    "**Simple Case (2 Features):**  \n",
    "- $x_1$: Length (varies a lot)  \n",
    "- $x_2$: Width (varies little)  \n",
    "- **Observation:** PCA might simply select the axis corresponding to the length because it carries most of the variability.\n",
    "\n",
    "**More Complex Case (2 Varying Features):**  \n",
    "- $x_1$: Length  \n",
    "- $x_2$: Height  \n",
    "- **Observation:** Both features are important. Instead of choosing one over the other, PCA finds a new axis (call it the $z$-axis) that is a combination of both. This new axis better captures the overall “size” of the car.\n",
    "\n",
    "**High-Dimensional Data (e.g., Countries):** \n",
    "    For a dataset with 50 features (like GDP, per capita GDP, Human Development Index, etc.), PCA reduces these to 2 or 3 principal components ($Z_1$, $Z_2$, $Z_3$). For instance:\n",
    "- $Z_1$ might loosely capture the total GDP (reflecting country size and economic scale).\n",
    "- $Z_2$ might capture per capita economic activity.\n",
    "  \n",
    "These reduced dimensions let you plot countries on a 2D graph and reveal clusters or trends.\n",
    "\n",
    "---\n",
    "\n",
    "## How PCA Works – The Process\n",
    "\n",
    "### 1. **Preprocessing**\n",
    "\n",
    "Before applying PCA, it is crucial to **normalize your features**:\n",
    "- **Mean Normalization**: Subtract the mean so each feature has zero mean.\n",
    "- **Feature Scaling**: Scale features so they have comparable ranges, especially if the features are measured in different units (e.g., square feet vs. number of bedrooms).\n",
    "\n",
    "### 2. **Choosing the New Axes**\n",
    "\n",
    "**Projection Idea:**  \n",
    "  Originally, data points are expressed as coordinates in the original feature space (e.g., $(x_1, x_2)$). PCA finds a new axis (or axes) such that when the data is projected onto these axes, the variance is maximized.\n",
    "  \n",
    "**Principal Component (PC):**  \n",
    "  The **first principal component** is the direction along which the projected data has the largest variance. If reducing to one dimension, you choose this axis. For additional components (e.g., second PC), each is chosen to be **orthogonal (perpendicular)** to the previous ones, ensuring that each new axis adds new, uncorrelated information.\n",
    "\n",
    "### 3. **Mathematical Explanation and Projection**\n",
    "\n",
    "Consider a simple example with two features:\n",
    "- A data point: $(x_1, x_2) = (2, 3)$.\n",
    "- A chosen principal axis represented by a **unit vector** $\\mathbf{w} = \\begin{pmatrix}0.71 \\\\ 0.71\\end{pmatrix}$.\n",
    "\n",
    "The projection of the data point onto this axis is computed using the **dot product**:\n",
    "\n",
    "$$\n",
    "z = \\mathbf{x} \\cdot \\mathbf{w} = 2 \\times 0.71 + 3 \\times 0.71 \\approx 3.55.\n",
    "$$\n",
    "\n",
    "This value, $z$, is the one-dimensional representation of the original two-dimensional point.\n",
    "\n",
    "#### **Reconstruction**\n",
    "\n",
    "Although the exact original coordinates cannot be recovered with a single principal component, you can approximate them:\n",
    "\n",
    "$$\n",
    "\\text{Reconstructed point} = z \\times \\mathbf{w} \\approx 3.55 \\times \\begin{pmatrix}0.71 \\\\ 0.71\\end{pmatrix} \\approx \\begin{pmatrix}2.52 \\\\ 2.52\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "The difference between the original and reconstructed point shows the loss of information due to dimensionality reduction.\n",
    "\n",
    "### 4. **Variance and Information Retention**\n",
    "\n",
    "When PCA projects data onto the principal components, it calculates the **explained variance ratio**. For example:\n",
    "- If reducing 2D data to 1D and obtaining an explained variance ratio of 0.992, it means 99.2% of the original variability is retained.\n",
    "- When using two principal components to represent 2D data, you might get ratios like 0.992 and 0.008, summing to 1 (or 100% variance).\n",
    "\n",
    "---\n",
    "\n",
    "## PCA vs. Linear Regression\n",
    "\n",
    "Although both techniques use lines and projections, they serve very different purposes:\n",
    "\n",
    "1. **Linear Regression**:\n",
    "  - **Supervised Learning**: Involves a target variable $y$.\n",
    "  - **Objective**: Fit a line to predict $y$ by minimizing vertical (error) distances.\n",
    "  - **Feature Treatment**: One feature is special because it is used to predict $y$.\n",
    "\n",
    "2. **PCA**:\n",
    "  - **Unsupervised Learning**: No target variable; all features are treated equally.\n",
    "  - **Objective**: Find new axes (principal components) that maximize the variance (spread) of the projected data.\n",
    "  - **Projection**: Data is projected orthogonally onto the new axes, and the process is symmetric among features.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementing PCA with Scikit-Learn\n",
    "\n",
    "Here is a step-by-step breakdown for using PCA in code:\n",
    "\n",
    "1. **Preprocessing**:\n",
    "   - Normalize data (mean normalization is done automatically by many PCA implementations, including scikit-learn’s).\n",
    "   - Optionally perform feature scaling if the feature ranges differ significantly.\n",
    "\n",
    "2. **Fitting the PCA Model**:\n",
    "   - Choose the number of principal components, e.g., 1, 2, or 3 depending on whether you need a 1D, 2D, or 3D visualization.\n",
    "   - Use the `fit` method to compute the principal components.\n",
    "\n",
    "3. **Examining Explained Variance**:\n",
    "   - After fitting, check the `explained_variance_ratio_` attribute to understand how much of the total variance is captured by each component.\n",
    "\n",
    "4. **Transforming Data**:\n",
    "   - Use the `transform` method to project the original data onto the new axes.\n",
    "   - Each data point is then represented by a smaller set of numbers (e.g., one number for 1D, two for 2D, etc.).\n",
    "\n",
    "### **Example Code**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Example dataset: each row is a training example\n",
    "X = np.array([\n",
    "    [1, 1],\n",
    "    [2, 3],\n",
    "    [3, 2],\n",
    "    [4, 5],\n",
    "    [5, 4],\n",
    "    [6, 6]\n",
    "])\n",
    "\n",
    "# Instantiate PCA to reduce the data to one principal component\n",
    "pca_1 = PCA(n_components=1)\n",
    "pca_1.fit(X)\n",
    "\n",
    "# Print the explained variance ratio\n",
    "print(\"Explained Variance Ratio (1 component):\", pca_1.explained_variance_ratio_)\n",
    "\n",
    "# Transform the data to one dimension\n",
    "X_transformed = pca_1.transform(X)\n",
    "print(\"Transformed Data (1D):\", X_transformed)\n",
    "\n",
    "# For 2 components (even if original data is 2D, useful for understanding PCA)\n",
    "pca_2 = PCA(n_components=2)\n",
    "pca_2.fit(X)\n",
    "print(\"Explained Variance Ratio (2 components):\", pca_2.explained_variance_ratio_)\n",
    "print(\"Transformed Data (2D):\", pca_2.transform(X))\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- **1D PCA**: Projects each 2D point to a single number (the coordinate along the first principal component), ideally capturing most of the data's variability.\n",
    "- **2D PCA**: Retains all information, where the first component might capture the vast majority of the variance (e.g., 99.2%) and the second a very small fraction.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Applications and Advice\n",
    "\n",
    "- **Visualization**: PCA is most commonly used to reduce high-dimensional data for plotting and visual analysis. It helps in spotting clusters, outliers, and trends.\n",
    "- **Data Compression**: Reducing storage and transmission costs by lowering the number of features (though modern storage and networks often make this less critical).\n",
    "- **Speeding Up Training**: Historically used to speed up algorithms by reducing dimensionality, though with modern methods like deep learning, direct high-dimensional data is more commonly used.\n",
    "\n",
    "**Tip:** Always inspect the explained variance ratios to ensure that the reduced dimensions still capture enough of the original data's variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe00ae97-de97-40fa-8aeb-e1618a9a1c64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
