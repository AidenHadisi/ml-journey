{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Linear regression is one of the simplest and most widely used machine learning models. It models the relationship between an independent variable ($x$) and a dependent variable ($y$) using a linear function.\n",
    "\n",
    "The linear regression model is defined as:\n",
    "\n",
    "$$\n",
    "f_{(w,b)}(x) = wx + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$x$**: \n",
    "    - The input variable (also called an independent variable or feature).\n",
    "    - Represents the value for which we want to predict an output.\n",
    "- **$w$**:\n",
    "    - The weight (also called slope or coefficient).\n",
    "    - Determines the strength and direction of the relationship between $x$ and $y$.\n",
    "- **$b$**:\n",
    "    - The bias (also called the intercept).\n",
    "    - Represents the value of $y$ when $x = 0$, i.e., where the line crosses the y-axis.\n",
    "- **$f(x)$**: \n",
    "    - The model function.\n",
    "    - Returns the estimated or predicted value ($\\hat{y}$).\n",
    "    - Maps the input variable $x$ to the predicted output using learned parameters $(w,b)$.\n",
    "- **$\\hat{y}$**:\n",
    "    - The predicted value.\n",
    "    - Represents the output value predicted by the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Simple Implementation\n",
    "\n",
    "Below is a simple Python implementation of a linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x: float, w: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Computes the predicted value for a given input using a linear function.\n",
    "\n",
    "    Args:\n",
    "        x (float): The input value (independent variable).\n",
    "        w (float): The weight (slope of the line).\n",
    "        b (float): The bias (y-intercept of the line).\n",
    "\n",
    "    Returns:\n",
    "        float: The predicted output value.\n",
    "    \"\"\"\n",
    "    \n",
    "    return x * w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this function to predict a value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 3050\n"
     ]
    }
   ],
   "source": [
    "x = 1000\n",
    "w = 3\n",
    "b = 50\n",
    "\n",
    "y_hat = predict(x, w, b)\n",
    "print(f'Prediction: {y_hat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model\n",
    "\n",
    "Training a model refers to the process of learning the optimal values of the weights ($w$) and bias ($b$) to map inputs ($x$) to outputs ($y$). The learning process involves finding the values of $w$ and $b$ that minimize the error between the predicted output ($\\hat{y}$) and the actual output ($y$).\n",
    "\n",
    "To achieve this, we need to define a loss function that quantifies the error between the predicted and actual values. The loss function measures the difference between the predicted and actual values for a given input. The goal of training the model is to find the values of $w$ and $b$ that minimize this loss function.\n",
    "\n",
    "### Squared Loss Function\n",
    "\n",
    "The squared loss function is a common choice for linear regression problems. It calculates the squared difference between the predicted and actual values for each data point.\n",
    "\n",
    "The squared loss function is defined as:\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = (y - \\hat{y})^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$L$**: The loss function. Measures the error between the predicted ($\\hat{y}$) and actual ($y$) values.\n",
    "- **$y$**: The actual output value.\n",
    "- **$\\hat{y}$**: The predicted output value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "The cost function is a measure of how well the model performs on the entire training set. It is calculated as the average loss over all the training examples. The cost function is dependent on the model's parameters $(w,b)$ and is used to guide the training process by updating the weights to minimize the cost.\n",
    "\n",
    "$$\n",
    "J_{(w,b)} = \\frac{1}{2m} \\sum_{i=1}^{m}(\\hat{y^i} - y^i)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "J_{(w,b)} = \\frac{1}{2m} \\sum_{i=1}^{m}(f_{(x,w)}(x^i) - y^i)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$m$**: Is the size of (number of items in) the training set $(x,y)$\n",
    "- **Training set $(x,y)$**: The set of input-output pairs used to train the model.\n",
    "- **$y^i$**: The actual output value for the $i^{th}$ input.\n",
    "- **$\\hat{y^i}$**: The predicted output value for the $i^{th}$ input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple Python implementation of the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(\n",
    "    x: NDArray[np.float64], y: NDArray[np.float64], w: float, b: float\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes the cost function for a given linear regression model with a single feature.\n",
    "\n",
    "    Args:\n",
    "        x: The input values (independent variables).\n",
    "        y: The output values (dependent variables).\n",
    "        w: The weight (slope of the line).\n",
    "        b: The bias (y-intercept of the line).\n",
    "\n",
    "    Returns:\n",
    "        The cost value.\n",
    "    \"\"\"\n",
    "\n",
    "    m = x.shape[0]  # Number of training examples\n",
    "    return sum((predict(x[i], w, b) - y[i]) ** 2 for i in range(m)) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use this function to calculate the cost for a given set of parameters $(w,b)$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (100,)\n",
      "y_train shape: (100,)\n",
      "x_train: [0.74908024 1.90142861 1.46398788 1.19731697 0.31203728]\n",
      "y_train: [6.33428778 9.40527849 8.48372443 5.60438199 4.71643995]\n",
      "Cost: 2116.924499915305\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "x_train: NDArray[np.float64] = 2 * np.random.rand(100)  # Feature\n",
    "y_train: NDArray[np.float64] = 4 + 3 * x_train + np.random.randn(100)  # Target with some noise\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f'x_train: {x_train[:5]}')\n",
    "print(f'y_train: {y_train[:5]}')\n",
    "\n",
    "w = 3\n",
    "b = 50\n",
    "\n",
    "cost = compute_cost(x_train, y_train, w, b)\n",
    "print(f'Cost: {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing the Cost Function\n",
    "\n",
    "We need to find the values of $w$ and $b$ that minimize the cost function $J_{(w,b)}$ so that the model can make accurate predictions.\n",
    "For a simple linear regression model, we can easily solve for the optimal values of $w$ and $b$ by finding the slope and intercept of the line that best fits the data.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Our cost is too high, which means our model is not fitting the data well. We need to adjust the parameters $(w,b)$ to minimize the cost.\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient.\n",
    "\n",
    "We will use the gradient of the cost function to update the parameters $(w,b)$ in the direction that minimizes the cost.\n",
    "\n",
    "The gradient descent algorithm is defined as:\n",
    "\n",
    "$$  \n",
    "w = w - \\alpha \\frac{\\partial J_{(w,b)}}{\\partial w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = b - \\alpha \\frac{\\partial J_{(w,b)}}{\\partial b}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$\\alpha$**: The learning rate.\n",
    "    - A hyperparameter that controls how much we are adjusting the weights of our network with respect to the loss gradient.\n",
    "    - A small learning rate requires more training epochs, but a large learning rate may cause the model to converge too quickly to a suboptimal solution.\n",
    "\n",
    "The partial derivatives of the cost function with respect to the parameters $(w,b)$ are:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{(w,b)}}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} (f_{(w,b)}(x^i) - y^i) x^i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{(w,b)}}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (f_{(w,b)}(x^i) - y^i)\n",
    "$$\n",
    "\n",
    "Below is a simple Python implementation of the gradient descent algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(\n",
    "    x_train: NDArray[np.float64],\n",
    "    y_train: NDArray[np.float64],\n",
    "    n_iterations: int = 1000,\n",
    "    learning_rate: float = 0.001,\n",
    ") -> tuple[float, float, list[float]]:\n",
    "\n",
    "    theta_0: float = np.random.randn()  # Initialize theta_0 randomly\n",
    "    theta_1: float = np.random.randn()  # Initialize theta_1 randomly\n",
    "    cost_history: list[float] = []\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        predictions = x_train * theta_1 + theta_0\n",
    "        errors = predictions - y_train\n",
    "        \n",
    "        theta_0 -= learning_rate * float(np.mean(errors))\n",
    "        theta_1 -= learning_rate * float(np.mean(errors * x_train))\n",
    "        \n",
    "        cost = compute_cost(x_train, y_train, theta_1, theta_0)\n",
    "        \n",
    "        cost_history.append(cost)\n",
    "\n",
    "    return (theta_0, theta_1, cost_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta 0: 3.2419638326756366\n",
      "Theta 1: 2.9968563036742184\n",
      "Final Cost: 1.4020688013462077\n"
     ]
    }
   ],
   "source": [
    "b, w, cost_history = gradient_descent(x_train, y_train)\n",
    "print(f'Theta 0: {b}')\n",
    "print(f'Theta 1: {w}')\n",
    "\n",
    "cost = compute_cost(x_train, y_train, w, b)\n",
    "print(f\"Final Cost: {cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Features\n",
    "\n",
    "In the simple linear regression model, we used only one feature $(x)$ to predict the output $(y)$. However, in practice, we often have multiple features that can be used to make predictions.\n",
    "\n",
    "The linear regression model can be extended to multiple features by adding a weight $(w_i)$ for each feature $(x_i)$ and a single bias term $(b)$:\n",
    "\n",
    "$$\n",
    "f_{(w,b)}(x) = \\sum_{i=1}^{n} w_i x_i + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$n$**: The number of features.\n",
    "- **$x_i$**: The $i^{th}$ feature.\n",
    "- **$w_i$**: The weight corresponding to the $i^{th}$ feature.\n",
    "\n",
    "\n",
    "This can also be written as:\n",
    "\n",
    "$$\n",
    "f_{(w,b)}(x) = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "Vectorization is a technique used to speed up the code by replacing explicit loops with matrix operations. It allows us to perform operations on entire arrays at once, making the code more efficient and easier to read.\n",
    "\n",
    "We can rewrite the linear regression model using vectorized operations as follows:\n",
    "\n",
    "$$\n",
    "f_{(\\mathbf{w},b)}(X) = X \\cdot \\mathbf{w} + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$X$**: The matrix of input features.\n",
    "- **$\\mathbf{w}$**: The vector of weights.\n",
    "- **$b$**: The bias term.\n",
    "\n",
    "This equation uses the dot product to calculate the predicted values for all the input examples in a single operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's redefine our `predict` function using vectorized operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x: NDArray[np.float64], w: NDArray[np.float64], b: float) -> float:\n",
    "    \"\"\"\n",
    "    Computes the predicted value for a given input using a linear function.\n",
    "    \n",
    "    Args:\n",
    "        x: The input values (independent variables).\n",
    "        w: The weights (slopes of the line).\n",
    "        b: The bias (y-intercept of the line).\n",
    "        \n",
    "    Returns:\n",
    "        The predicted output value.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.dot(x, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 14050\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1000, 2000, 3000])  # an input with 3 features\n",
    "w = np.array([1, 2, 3])  # weights for each feature\n",
    "b = 50  # bias\n",
    "\n",
    "y_hat = predict(x, w, b)\n",
    "print(f\"Prediction: {y_hat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function In Matrix Form\n",
    "\n",
    "The cost function can also be updated as follows for multiple features:\n",
    "\n",
    "$$\n",
    "J_{(\\mathbf{w},b)} = \\frac{1}{2m} \\sum_{i=1}^{m}(\\hat{y^i} - y^i)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "J_{(\\mathbf{w},b)} = \\frac{1}{2m} \\sum_{i=1}^{m}(f_{(\\mathbf{w},b)}(X^i) - y^i)^2\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Since taking squaring a matrix means taking its transpose times itself, we can rewrite the cost function in matrix form as:\n",
    "\n",
    "$$\n",
    "J_{(\\theta)} = \\frac{1}{2m} (X \\cdot \\theta - y)^T \\cdot (X \\cdot \\theta - y)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$X$**: The matrix of input features.\n",
    "- **$\\theta$**: The vector of parameters $(\\mathbf{w},b)$.\n",
    "- **$y$**: The vector of actual output values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(\n",
    "    X: NDArray[np.float64], y: NDArray[np.float64], theta: NDArray[np.float64]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes the cost function for a given linear regression model with multiple features.\n",
    "    \n",
    "    Args:\n",
    "        X: The input values (independent variables).\n",
    "        y: The output values (dependent variables).\n",
    "        theta: The weights and bias.\n",
    "        \n",
    "    Returns:\n",
    "        The cost value.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = X @ theta\n",
    "    errors = predictions - y\n",
    "\n",
    "    # cost = float(np.mean(errors.T @ errors)/2) \n",
    "\n",
    "    # More efficient way to compute the cost\n",
    "    cost = float(np.dot(errors, errors) / (2 * X.shape[0]))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Steps\n",
    "\n",
    "Here is detailed steps of how our `compute_cost` function works:\n",
    "\n",
    "Let's say we have **three training examples** and **two actual features** (plus a bias term), meaning `X` has three columns.\n",
    "\n",
    "#### **Given Data**\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "1 & 4 & 5 \\\\\n",
    "1 & 6 & 7\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- The **first column** is the bias term (`1`).\n",
    "- The **second column** is the first actual feature.\n",
    "- The **third column** is the second actual feature.\n",
    "\n",
    "The actual target values are:\n",
    "\n",
    "$$\n",
    "y =\n",
    "\\begin{bmatrix}\n",
    "10 \\\\\n",
    "20 \\\\\n",
    "30\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The parameter vector (Î¸) is:\n",
    "\n",
    "$$\n",
    "\\theta =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 1: Compute Predictions**\n",
    "We compute:\n",
    "\n",
    "$$\n",
    "X \\cdot \\theta =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "1 & 4 & 5 \\\\\n",
    "1 & 6 & 7\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Perform matrix multiplication:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "(1 \\times 1) + (2 \\times 2) + (3 \\times 3) \\\\\n",
    "(1 \\times 1) + (4 \\times 2) + (5 \\times 3) \\\\\n",
    "(1 \\times 1) + (6 \\times 2) + (7 \\times 3)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 + 4 + 9 \\\\\n",
    "1 + 8 + 15 \\\\\n",
    "1 + 12 + 21\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "14 \\\\\n",
    "24 \\\\\n",
    "34\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These are our **predicted values**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Compute Errors**\n",
    "Now, subtract the actual `y` values from the predictions:\n",
    "\n",
    "$$\n",
    "\\text{errors} = X\\theta - y =\n",
    "\\begin{bmatrix}\n",
    "14 \\\\\n",
    "24 \\\\\n",
    "34\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "10 \\\\\n",
    "20 \\\\\n",
    "30\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "4 \\\\\n",
    "4 \\\\\n",
    "4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Compute the Squared Errors**\n",
    "Square each error term:\n",
    "\n",
    "$$\n",
    "\\text{errors}^2 =\n",
    "\\begin{bmatrix}\n",
    "4^2 \\\\\n",
    "4^2 \\\\\n",
    "4^2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "16 \\\\\n",
    "16 \\\\\n",
    "16\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 4: Compute the Mean and Final Cost**\n",
    "First, sum the squared errors:\n",
    "\n",
    "$$\n",
    "\\sum \\text{errors}^2 = 16 + 16 + 16 = 48\n",
    "$$\n",
    "\n",
    "Take the mean:\n",
    "\n",
    "$$\n",
    "\\frac{1}{m} \\sum \\text{errors}^2 = \\frac{48}{3} = 16\n",
    "$$\n",
    "\n",
    "Now, divide by **2** to get the final cost:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2} \\times 16 = 8\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent In Matrix Form\n",
    "\n",
    "The gradient of the cost function with respect to the parameters $(\\mathbf{w},b)$ can be updated as follows for multiple features:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{(\\theta)}}{\\partial \\mathbf{w}} = \\frac{1}{m} X^T \\cdot (X \\cdot \\theta - y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(\n",
    "    X: NDArray[np.float64],\n",
    "    y: NDArray[np.float64],\n",
    "    epoch: int = 1_000,\n",
    "    alpha: float = 0.001,\n",
    ") -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Performs the gradient descent optimization algorithm to learn the weights and bias\n",
    "    \n",
    "    Args:\n",
    "        X: The input values (independent variables).\n",
    "        y: The output values (dependent variables).\n",
    "        epoch: The number of iterations to update the weights and bias.\n",
    "        alpha: The learning rate.\n",
    "        \n",
    "    Returns:\n",
    "        The weights and bias that minimize the cost function.\n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape  # Number of samples (m) and features (n)\n",
    "    theta = np.random.randn(n, 1)\n",
    "\n",
    "    for _ in range(epoch):\n",
    "        predictions = X @ theta\n",
    "        errors = predictions - y\n",
    "\n",
    "        gradients = (X.T @ errors) / m\n",
    "\n",
    "        theta -= alpha * gradients\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual weights:\n",
      "[[ 0.49671415]\n",
      " [-0.1382643 ]\n",
      " [ 0.64768854]\n",
      " [ 1.52302986]]\n",
      "Predicted weights:\n",
      "[[0.05967946]\n",
      " [0.46571906]\n",
      " [1.28957003]\n",
      " [0.66230754]]\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "m, n = 100, 3  # 100 samples, 3 features (excluding bias)\n",
    "theta_actual = np.random.randn(n + 1, 1)  # True weights (including bias)\n",
    "\n",
    "# Generate random training data\n",
    "X_train = 2 * np.random.rand(m, n)  # Features\n",
    "X_train = np.c_[np.ones((m, 1)), X_train]  # Add bias column\n",
    "\n",
    "# Generate target values with some noise\n",
    "y_train = X_train @ theta_actual + np.random.randn(m, 1)\n",
    "\n",
    "print(f\"Actual weights:\\n{theta_actual}\")\n",
    "\n",
    "# Train using gradient descent\n",
    "theta_predicted = gradient_descent(X_train, y_train)\n",
    "\n",
    "print(f\"Predicted weights:\\n{theta_predicted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
