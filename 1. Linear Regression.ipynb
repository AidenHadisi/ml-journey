{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Linear regression is one of the simplest and most widely used machine learning models. It models the relationship between an independent variable ($x$) and a dependent variable ($y$) using a linear function.\n",
    "\n",
    "The linear regression model is defined as:\n",
    "\n",
    "$$\n",
    "f_{(w,b)}(x) = wx + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$x$**: \n",
    "    - The input variable (also called an independent variable or feature).\n",
    "    - Represents the value for which we want to predict an output.\n",
    "- **$w$**:\n",
    "    - The weight (also called slope or coefficient).\n",
    "    - Determines the strength and direction of the relationship between $x$ and $y$.\n",
    "- **$b$**:\n",
    "    - The bias (also called the intercept).\n",
    "    - Represents the value of $y$ when $x = 0$, i.e., where the line crosses the y-axis.\n",
    "- **$f(x)$**: \n",
    "    - The model function.\n",
    "    - Returns the estimated or predicted value ($\\hat{y}$).\n",
    "    - Maps the input variable $x$ to the predicted output using learned parameters $(w,b)$.\n",
    "- **$\\hat{y}$**:\n",
    "    - The predicted value.\n",
    "    - Represents the output value predicted by the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Simple Implementation\n",
    "\n",
    "Below is a simple Python implementation of a linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x: float, w: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Computes the predicted value for a given input using a linear function.\n",
    "\n",
    "    Args:\n",
    "        x (float): The input value (independent variable).\n",
    "        w (float): The weight (slope of the line).\n",
    "        b (float): The bias (y-intercept of the line).\n",
    "\n",
    "    Returns:\n",
    "        float: The predicted output value.\n",
    "    \"\"\"\n",
    "    \n",
    "    return x * w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this function to predict a value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 3050\n"
     ]
    }
   ],
   "source": [
    "x = 1000\n",
    "w = 3\n",
    "b = 50\n",
    "\n",
    "y_hat = predict(x, w, b)\n",
    "print(f'Prediction: {y_hat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model\n",
    "\n",
    "Training a model refers to the process of learning the optimal values of the weights ($w$) and bias ($b$) to map inputs ($x$) to outputs ($y$). The learning process involves finding the values of $w$ and $b$ that minimize the error between the predicted output ($\\hat{y}$) and the actual output ($y$).\n",
    "\n",
    "To achieve this, we need to define a loss function that quantifies the error between the predicted and actual values. The loss function measures the difference between the predicted and actual values for a given input. The goal of training the model is to find the values of $w$ and $b$ that minimize this loss function.\n",
    "\n",
    "### Squared Loss Function\n",
    "\n",
    "The squared loss function is a common choice for linear regression problems. It calculates the squared difference between the predicted and actual values for each data point.\n",
    "\n",
    "The squared loss function is defined as:\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = (y - \\hat{y})^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$L$**: The loss function. Measures the error between the predicted ($\\hat{y}$) and actual ($y$) values.\n",
    "- **$y$**: The actual output value.\n",
    "- **$\\hat{y}$**: The predicted output value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "The cost function is a measure of how well the model performs on the entire training set. It is calculated as the average loss over all the training examples. The cost function is dependent on the model's parameters $(w,b)$ and is used to guide the training process by updating the weights to minimize the cost.\n",
    "\n",
    "$$\n",
    "J_{(w,b)} = \\frac{1}{2m} \\sum_{i=1}^{m}(\\hat{y^i} - y^i)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "J_{(w,b)} = \\frac{1}{2m} \\sum_{i=1}^{m}(f_{(x,w)}(x^i) - y^i)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$m$**: Is the size of (number of items in) the training set $(x,y)$\n",
    "- **Training set $(x,y)$**: The set of input-output pairs used to train the model.\n",
    "- **$y^i$**: The actual output value for the $i^{th}$ input.\n",
    "- **$\\hat{y^i}$**: The predicted output value for the $i^{th}$ input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple Python implementation of the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(\n",
    "    x: NDArray[np.float64], y: NDArray[np.float64], w: float, b: float\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes the cost function for a given linear regression model with a single feature.\n",
    "\n",
    "    Args:\n",
    "        x: The input values (independent variables).\n",
    "        y: The output values (dependent variables).\n",
    "        w: The weight (slope of the line).\n",
    "        b: The bias (y-intercept of the line).\n",
    "\n",
    "    Returns:\n",
    "        The cost value.\n",
    "    \"\"\"\n",
    "\n",
    "    m = x.shape[0]  # Number of training examples\n",
    "    return sum((predict(x[i], w, b) - y[i]) ** 2 for i in range(m)) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use this function to calculate the cost for a given set of parameters $(w,b)$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (100,)\n",
      "y_train shape: (100,)\n",
      "x_train: [0.74908024 1.90142861 1.46398788 1.19731697 0.31203728]\n",
      "y_train: [6.33428778 9.40527849 8.48372443 5.60438199 4.71643995]\n",
      "Cost: 2116.924499915305\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "x_train: NDArray[np.float64] = 2 * np.random.rand(100)  # Feature\n",
    "y_train: NDArray[np.float64] = 4 + 3 * x_train + np.random.randn(100)  # Target with some noise\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f'x_train: {x_train[:5]}')\n",
    "print(f'y_train: {y_train[:5]}')\n",
    "\n",
    "w = 3\n",
    "b = 50\n",
    "\n",
    "cost = compute_cost(x_train, y_train, w, b)\n",
    "print(f'Cost: {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing the Cost Function\n",
    "\n",
    "We need to find the values of $w$ and $b$ that minimize the cost function $J_{(w,b)}$ so that the model can make accurate predictions.\n",
    "For a simple linear regression model, we can easily solve for the optimal values of $w$ and $b$ by finding the slope and intercept of the line that best fits the data.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Our cost is too high, which means our model is not fitting the data well. We need to adjust the parameters $(w,b)$ to minimize the cost.\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient.\n",
    "\n",
    "We will use the gradient of the cost function to update the parameters $(w,b)$ in the direction that minimizes the cost.\n",
    "\n",
    "The gradient descent algorithm is defined as:\n",
    "\n",
    "$$  \n",
    "w = w - \\alpha \\frac{\\partial J_{(w,b)}}{\\partial w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = b - \\alpha \\frac{\\partial J_{(w,b)}}{\\partial b}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$\\alpha$**: The learning rate.\n",
    "    - A hyperparameter that controls how much we are adjusting the weights of our network with respect to the loss gradient.\n",
    "    - A small learning rate requires more training epochs, but a large learning rate may cause the model to converge too quickly to a suboptimal solution.\n",
    "\n",
    "The partial derivatives of the cost function with respect to the parameters $(w,b)$ are:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{(w,b)}}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} (f_{(w,b)}(x^i) - y^i) x^i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{(w,b)}}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (f_{(w,b)}(x^i) - y^i)\n",
    "$$\n",
    "\n",
    "Below is a simple Python implementation of the gradient descent algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(\n",
    "    x_train: NDArray[np.float64],\n",
    "    y_train: NDArray[np.float64],\n",
    "    n_iterations: int = 1000,\n",
    "    learning_rate: float = 0.001,\n",
    ") -> tuple[float, float, list[float]]:\n",
    "\n",
    "    theta_0: float = np.random.randn()  # Initialize theta_0 randomly\n",
    "    theta_1: float = np.random.randn()  # Initialize theta_1 randomly\n",
    "    cost_history: list[float] = []\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        predictions = x_train * theta_1 + theta_0\n",
    "        errors = predictions - y_train\n",
    "        \n",
    "        theta_0 -= learning_rate * float(np.mean(errors))\n",
    "        theta_1 -= learning_rate * float(np.mean(errors * x_train))\n",
    "        \n",
    "        cost = compute_cost(x_train, y_train, theta_1, theta_0)\n",
    "        \n",
    "        cost_history.append(cost)\n",
    "\n",
    "    return (theta_0, theta_1, cost_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta 0: 2.4580759579293456\n",
      "Theta 1: 3.6853537392610805\n",
      "Final Cost: 1.9036198210248854\n"
     ]
    }
   ],
   "source": [
    "b, w, cost_history = gradient_descent(x_train, y_train)\n",
    "print(f'Theta 0: {b}')\n",
    "print(f'Theta 1: {w}')\n",
    "\n",
    "cost = compute_cost(x_train, y_train, w, b)\n",
    "print(f\"Final Cost: {cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Features\n",
    "\n",
    "In the simple linear regression model, we used only one feature $(x)$ to predict the output $(y)$. However, in practice, we often have multiple features that can be used to make predictions.\n",
    "\n",
    "The linear regression model can be extended to multiple features by adding a weight $(w_i)$ for each feature $(x_i)$ and a single bias term $(b)$:\n",
    "\n",
    "$$\n",
    "f_{(w,b)}(x) = \\sum_{i=1}^{n} w_i x_i + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$n$**: The number of features.\n",
    "- **$x_i$**: The $i^{th}$ feature.\n",
    "- **$w_i$**: The weight corresponding to the $i^{th}$ feature.\n",
    "\n",
    "\n",
    "This can also be written as:\n",
    "\n",
    "$$\n",
    "f_{(w,b)}(x) = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "Vectorization is a technique used to speed up the code by replacing explicit loops with matrix operations. It allows us to perform operations on entire arrays at once, making the code more efficient and easier to read.\n",
    "\n",
    "We can rewrite the linear regression model using vectorized operations as follows:\n",
    "\n",
    "$$\n",
    "f_{(\\mathbf{w},b)}(X) = X \\cdot \\mathbf{w} + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$X$**: The matrix of input features.\n",
    "- **$\\mathbf{w}$**: The vector of weights.\n",
    "- **$b$**: The bias term.\n",
    "\n",
    "This equation uses the dot product to calculate the predicted values for all the input examples in a single operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's redefine our `predict` function using vectorized operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x: NDArray[np.float64], w: NDArray[np.float64], b: float) -> float:\n",
    "    \"\"\"\n",
    "    Computes the predicted value for a given input using a linear function.\n",
    "    \n",
    "    Args:\n",
    "        x: The input values (independent variables).\n",
    "        w: The weights (slopes of the line).\n",
    "        b: The bias (y-intercept of the line).\n",
    "        \n",
    "    Returns:\n",
    "        The predicted output value.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.dot(x, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 14050\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1000, 2000, 3000])  # an input with 3 features\n",
    "w = np.array([1, 2, 3])  # weights for each feature\n",
    "b = 50  # bias\n",
    "\n",
    "y_hat = predict(x, w, b)\n",
    "print(f\"Prediction: {y_hat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function In Matrix Form\n",
    "\n",
    "The cost function can also be updated as follows for multiple features:\n",
    "\n",
    "$$\n",
    "J_{(\\mathbf{w},b)} = \\frac{1}{2m} \\sum_{i=1}^{m}(\\hat{y^i} - y^i)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "J_{(\\mathbf{w},b)} = \\frac{1}{2m} \\sum_{i=1}^{m}(f_{(\\mathbf{w},b)}(X^i) - y^i)^2\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Since taking squaring a matrix means taking its transpose times itself, we can rewrite the cost function in matrix form as:\n",
    "\n",
    "$$\n",
    "J_{(\\theta)} = \\frac{1}{2m} (X \\cdot \\theta - y)^T \\cdot (X \\cdot \\theta - y)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$X$**: The matrix of input features.\n",
    "- **$\\theta$**: The vector of parameters $(\\mathbf{w},b)$.\n",
    "- **$y$**: The vector of actual output values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(\n",
    "    X: NDArray[np.float64], y: NDArray[np.float64], theta: NDArray[np.float64]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes the cost function for a given linear regression model with multiple features.\n",
    "    \n",
    "    Args:\n",
    "        X: The input values (independent variables).\n",
    "        y: The output values (dependent variables).\n",
    "        theta: The weights and bias.\n",
    "        \n",
    "    Returns:\n",
    "        The cost value.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = X @ theta\n",
    "    errors = predictions - y\n",
    "\n",
    "    # cost = float(np.mean(errors.T @ errors)/2) \n",
    "\n",
    "    # More efficient way to compute the cost\n",
    "    cost = float(np.dot(errors, errors) / (2 * X.shape[0]))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Steps\n",
    "\n",
    "Here is detailed steps of how our `compute_cost` function works:\n",
    "\n",
    "Let's say we have **three training examples** and **two actual features** (plus a bias term), meaning `X` has three columns.\n",
    "\n",
    "#### **Given Data**\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "1 & 4 & 5 \\\\\n",
    "1 & 6 & 7\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- The **first column** is the bias term (`1`).\n",
    "- The **second column** is the first actual feature.\n",
    "- The **third column** is the second actual feature.\n",
    "\n",
    "The actual target values are:\n",
    "\n",
    "$$\n",
    "y =\n",
    "\\begin{bmatrix}\n",
    "10 \\\\\n",
    "20 \\\\\n",
    "30\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The parameter vector (θ) is:\n",
    "\n",
    "$$\n",
    "\\theta =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 1: Compute Predictions**\n",
    "We compute:\n",
    "\n",
    "$$\n",
    "X \\cdot \\theta =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "1 & 4 & 5 \\\\\n",
    "1 & 6 & 7\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Perform matrix multiplication:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "(1 \\times 1) + (2 \\times 2) + (3 \\times 3) \\\\\n",
    "(1 \\times 1) + (4 \\times 2) + (5 \\times 3) \\\\\n",
    "(1 \\times 1) + (6 \\times 2) + (7 \\times 3)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 + 4 + 9 \\\\\n",
    "1 + 8 + 15 \\\\\n",
    "1 + 12 + 21\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "14 \\\\\n",
    "24 \\\\\n",
    "34\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These are our **predicted values**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Compute Errors**\n",
    "Now, subtract the actual `y` values from the predictions:\n",
    "\n",
    "$$\n",
    "\\text{errors} = X\\theta - y =\n",
    "\\begin{bmatrix}\n",
    "14 \\\\\n",
    "24 \\\\\n",
    "34\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "10 \\\\\n",
    "20 \\\\\n",
    "30\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "4 \\\\\n",
    "4 \\\\\n",
    "4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Compute the Squared Errors**\n",
    "Square each error term:\n",
    "\n",
    "$$\n",
    "\\text{errors}^2 =\n",
    "\\begin{bmatrix}\n",
    "4^2 \\\\\n",
    "4^2 \\\\\n",
    "4^2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "16 \\\\\n",
    "16 \\\\\n",
    "16\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 4: Compute the Mean and Final Cost**\n",
    "First, sum the squared errors:\n",
    "\n",
    "$$\n",
    "\\sum \\text{errors}^2 = 16 + 16 + 16 = 48\n",
    "$$\n",
    "\n",
    "Take the mean:\n",
    "\n",
    "$$\n",
    "\\frac{1}{m} \\sum \\text{errors}^2 = \\frac{48}{3} = 16\n",
    "$$\n",
    "\n",
    "Now, divide by **2** to get the final cost:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2} \\times 16 = 8\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent In Matrix Form\n",
    "\n",
    "The gradient of the cost function with respect to the parameters $(\\mathbf{w},b)$ can be updated as follows for multiple features:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{(\\theta)}}{\\partial \\mathbf{w}} = \\frac{1}{m} X^T \\cdot (X \\cdot \\theta - y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(\n",
    "    X: NDArray[np.float64],\n",
    "    y: NDArray[np.float64],\n",
    "    epoch: int = 1_000,\n",
    "    alpha: float = 0.001,\n",
    ") -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Performs the gradient descent optimization algorithm to learn the weights and bias\n",
    "    \n",
    "    Args:\n",
    "        X: The input values (independent variables).\n",
    "        y: The output values (dependent variables).\n",
    "        epoch: The number of iterations to update the weights and bias.\n",
    "        alpha: The learning rate.\n",
    "        \n",
    "    Returns:\n",
    "        The weights and bias that minimize the cost function.\n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape  # Number of samples (m) and features (n)\n",
    "    theta = np.random.randn(n, 1)\n",
    "\n",
    "    for _ in range(epoch):\n",
    "        predictions = X @ theta\n",
    "        errors = predictions - y\n",
    "\n",
    "        gradients = (X.T @ errors) / m\n",
    "\n",
    "        theta -= alpha * gradients\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual weights:\n",
      "[[ 0.49671415]\n",
      " [-0.1382643 ]\n",
      " [ 0.64768854]\n",
      " [ 1.52302986]]\n",
      "Predicted weights:\n",
      "[[0.05967946]\n",
      " [0.46571906]\n",
      " [1.28957003]\n",
      " [0.66230754]]\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "m, n = 100, 3  # 100 samples, 3 features (excluding bias)\n",
    "theta_actual = np.random.randn(n + 1, 1)  # True weights (including bias)\n",
    "\n",
    "# Generate random training data\n",
    "X_train = 2 * np.random.rand(m, n)  # Features\n",
    "X_train = np.c_[np.ones((m, 1)), X_train]  # Add bias column\n",
    "\n",
    "# Generate target values with some noise\n",
    "y_train = X_train @ theta_actual + np.random.randn(m, 1)\n",
    "\n",
    "print(f\"Actual weights:\\n{theta_actual}\")\n",
    "\n",
    "# Train using gradient descent\n",
    "theta_predicted = gradient_descent(X_train, y_train)\n",
    "\n",
    "print(f\"Predicted weights:\\n{theta_predicted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equation\n",
    "\n",
    "When the number of features is not very large, we can use the normal equation to find the optimal values of the parameters $(\\mathbf{w},b)$ directly.\n",
    "\n",
    "The normal equation is given by:\n",
    "\n",
    "$$\n",
    "\\theta = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$X$**: The matrix of input features.\n",
    "- **$y$**: The vector of actual output values.\n",
    "- **$\\theta$**: The vector of parameters $(\\mathbf{w},b)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation(\n",
    "    X: NDArray[np.float64],\n",
    "    y: NDArray[np.float64],\n",
    ") -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Computes the weights using the normal equation method.\n",
    "\n",
    "    Args:\n",
    "        X: The input values (independent variables).\n",
    "        y: The output values (dependent variables).\n",
    "\n",
    "    Returns:\n",
    "        The weights that minimize the cost function.\n",
    "    \"\"\"\n",
    "\n",
    "    theta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted weights:\n",
      "[[0.07158197]\n",
      " [0.08773787]\n",
      " [0.91882845]\n",
      " [1.49947743]]\n"
     ]
    }
   ],
   "source": [
    "# Train using normal eq\n",
    "theta_predicted = normal_equation(X_train, y_train)\n",
    "\n",
    "print(f\"Predicted weights:\\n{theta_predicted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Tests\n",
    "\n",
    "Convergence tests are used to determine when to stop the training process. They help prevent overfitting and ensure that the model generalizes well to unseen data.\n",
    "\n",
    "### Learning Curve\n",
    "\n",
    "\n",
    "A common approach is to use the Learning Curve, which plots the cost function over the training iterations. The learning curve helps visualize how the cost decreases over time and can be used to identify when the model has converged.\n",
    "\n",
    "When checking the learning curve, we look for the following signs of convergence:\n",
    "- The cost decreases with each iteration.\n",
    "- The cost stabilizes and flattens out over time as the model converges.\n",
    "\n",
    "If the cost continues to decrease without stabilizing, it may indicate that the learning rate is too high, causing the model to overshoot the optimal values.\n",
    "\n",
    "### Epsilon Threshold\n",
    "\n",
    "Another approach is to use an epsilon threshold to check if the change in the cost function between iterations is below a certain threshold. If the change is less than the threshold, the model is considered to have converged.\n",
    "\n",
    "The epsilon threshold is defined as:\n",
    "\n",
    "$$\n",
    "\\text{if } |J_{(\\theta)}^{(i)} - J_{(\\theta)}^{(i-1)}| < \\epsilon \\text{, then the model has converged}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$J_{(\\theta)}^{(i)}$**: The cost function at iteration $i$.\n",
    "- **$J_{(\\theta)}^{(i-1)}$**: The cost function at the previous iteration.\n",
    "- **$\\epsilon$**: The threshold value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4), layout=\"constrained\")\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"# Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Feature scaling is a preprocessing step used to standardize the range of independent variables or features of the data. It is important for algorithms that rely on the magnitude of values, such as linear regression, to ensure that all features contribute equally to the model.\n",
    "\n",
    "There are three methods for feature scaling:\n",
    "\n",
    "### Min-Max Scaling\n",
    "\n",
    "Min-Max scaling (also known as normalization) scales the data to a fixed range, usually between 0 and 1. It is calculated as:\n",
    "\n",
    "$$\n",
    "X_{\\text{norm}} = \\frac{X}{X_{\\text{max}}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$X$**: The original feature value.\n",
    "- **$X_{\\text{min}}$**: The minimum value of the feature.\n",
    "- **$X_{\\text{max}}$**: The maximum value of the feature.\n",
    "\n",
    "### Standardization\n",
    "\n",
    "Standardization scales the data to have a mean of 0 and a standard deviation of 1. It is calculated as:\n",
    "\n",
    "$$\n",
    "X_{\\text{std}} = \\frac{X - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$X$**: The original feature value.\n",
    "- **$\\mu$**: The mean of the feature.\n",
    "- **$\\sigma$**: The standard deviation of the feature.\n",
    "\n",
    "### Mean Normalization\n",
    "\n",
    "Mean normalization scales the data to have a mean of 0 and a range of -1 to 1. It is calculated as:\n",
    "\n",
    "$$\n",
    "X_{\\text{norm}} = \\frac{X - \\mu}{X_{\\text{max}} - X_{\\text{min}}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$X$**: The original feature value.\n",
    "- **$\\mu$**: The mean of the feature.\n",
    "- **$X_{\\text{max}}$**: The maximum value of the feature.\n",
    "- **$X_{\\text{min}}$**: The minimum value of the feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(X: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Normalizes the input features using the simple min-max scaling method.\n",
    "\n",
    "    Args:\n",
    "        X: The input values (independent variables).\n",
    "\n",
    "    Returns:\n",
    "        The normalized input values.\n",
    "    \"\"\"\n",
    "\n",
    "    max_vals = np.max(X, axis=0)\n",
    "    return X / max_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(X: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Standardizes the input features using the z-score method.\n",
    "\n",
    "    Args:\n",
    "        X: The input values (independent variables).\n",
    "\n",
    "    Returns:\n",
    "        The standardized input values.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_vals = np.mean(X, axis=0)\n",
    "    std_devs = np.std(X, axis=0)\n",
    "    return (X - mean_vals) / std_devs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_normalization(X: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Normalizes the input features using the mean normalization method.\n",
    "\n",
    "    Args:\n",
    "        X: The input values (independent variables).\n",
    "\n",
    "    Returns:\n",
    "        The normalized input values.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_vals = np.mean(X, axis=0)\n",
    "    max_vals = np.max(X, axis=0)\n",
    "    min_vals = np.min(X, axis=0)\n",
    "    return (X - mean_vals) / (max_vals - min_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with Scikit-Learn\n",
    "\n",
    "Scikit-Learn is a popular machine learning library in Python that provides a wide range of tools for building machine learning models. It includes a simple and efficient implementation of linear regression that can be used to train and evaluate models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted weights: [[0.         0.08773787 0.91882845 1.49947743]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "print(f\"Predicted weights: {model.coef_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
