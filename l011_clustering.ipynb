{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4cf531-2119-427c-a407-a8caab95fc7e",
   "metadata": {},
   "source": [
    "# Clustering & the K-Means\n",
    "\n",
    "Clustering is a fundamental unsupervised learning technique used to group data points based on similarity. Unlike supervised learning, where both inputs and target labels are provided, clustering relies solely on input data to uncover hidden structures and patterns.\n",
    "\n",
    "Clustering is the process of partitioning a dataset into groups (or clusters) such that:\n",
    "- **Intra-cluster similarity** is high: Data points in the same cluster are similar to each other.\n",
    "- **Inter-cluster similarity** is low: Data points in different clusters are dissimilar.\n",
    "\n",
    "### **Contrast with Supervised Learning**\n",
    "\n",
    "- **Supervised Learning:**  \n",
    "  You have input features ($x$) and corresponding labels ($y$). The model learns a mapping from $x$ to $y$, such as in binary classification where you might use logistic regression or neural networks to find a decision boundary.\n",
    "\n",
    "- **Unsupervised Learning (Clustering):**  \n",
    "  Only the features $x$ are provided. There is no “correct” answer given. Instead, the algorithm must infer structure, grouping similar data points together. When visualized, the dataset is simply a collection of dots, without predefined classes.\n",
    "\n",
    "### **Real-World Examples of Clustering**\n",
    "\n",
    "- **News Article Grouping:** Automatically grouping similar news articles (e.g., technology, sports, politics).\n",
    "- **Market Segmentation:** Categorizing customers or learners based on behaviors or preferences.\n",
    "- **Genetic Analysis:** Grouping individuals based on DNA expression data to identify similar genetic traits.\n",
    "- **Astronomy:** Grouping celestial bodies to identify galaxies or detect patterns in space.\n",
    "- **Image Compression:** Reducing the number of colors in an image by clustering similar color values.\n",
    "\n",
    "---\n",
    "\n",
    "## The K-Means Clustering Algorithm\n",
    "\n",
    "K-means is one of the simplest and most widely used clustering algorithms. Its goal is to partition a dataset into **K clusters** by minimizing the variance within each cluster.\n",
    "\n",
    "K-means works iteratively through two primary steps:\n",
    "\n",
    "### **Step A: Assignment (Cluster Membership)**\n",
    "1. **Initialization:**  \n",
    "   Randomly choose $K$ data points as the initial cluster centroids:\n",
    "\n",
    "$$\\mu_1, \\mu_2, \\dots, \\mu_K$$\n",
    "\n",
    "   These centroids are vectors with the same dimensions as the data points. They represent the \"center\" of each cluster.\n",
    "\n",
    "2. **Assign Points to Nearest Centroid:**  \n",
    "   For each data point $x^{(i)}$, determine its closest centroid using the squared Euclidean distance. Mathematically, assign:\n",
    "\n",
    "$$C^{(i)} = \\arg\\min_{k} \\| x^{(i)} - \\mu_k \\|^2$$\n",
    "\n",
    "   where $C^{(i)}$ is the cluster index (from 1 to $K$) for the data point $x^{(i)}$.\n",
    "\n",
    "### **Step B: Update (Centroid Recalculation)**\n",
    "1. **Recompute Centroids:**  \n",
    "   After assigning every point to a cluster, update each centroid by computing the mean (average) of all data points assigned to that cluster:\n",
    "\n",
    "$$\\mu_k = \\frac{1}{|S_k|} \\sum_{x^{(i)} \\in S_k} x^{(i)}$$\n",
    "\n",
    "   where $S_k$ is the set of points assigned to cluster $k$.\n",
    "\n",
    "2. **Repeat Until Convergence:**  \n",
    "   Alternate between the assignment and update steps. Convergence is reached when the centroids no longer move significantly, or when the cluster assignments stop changing.\n",
    "\n",
    "\n",
    "### **Handling Special Cases**\n",
    "- **Empty Cluster:**  \n",
    "  If a centroid ends up with no points assigned (i.e., $S_k$ is empty), you can:\n",
    "  - **Eliminate the cluster,** reducing $K$, or\n",
    "  - **Reinitialize the centroid** randomly to hope it captures a new group in the next iteration.\n",
    "\n",
    "---\n",
    "\n",
    "## The Cost (Distortion) Function\n",
    "\n",
    "K-means optimizes a cost function that quantifies the “tightness” of clusters. This cost function, often called the **distortion function**, is defined as:\n",
    "\n",
    "$$\n",
    "J(C, \\mu) = \\frac{1}{m} \\sum_{i=1}^{m} \\| x^{(i)} - \\mu_{C^{(i)}} \\|^2\n",
    "$$\n",
    "\n",
    "- **Interpretation:**  \n",
    "  For each data point, calculate the squared distance to its assigned centroid, sum these values for all points, and take the average. A lower $J$ indicates that points are closer to their centroids, which implies better clustering.\n",
    "\n",
    "- **Optimization:**  \n",
    "  - **Assignment Step:** Minimizes $J$ with respect to the cluster assignments $C^{(i)}$, keeping $\\mu_k$ fixed.\n",
    "  - **Update Step:** Minimizes $J$ with respect to the centroids $\\mu_k$, keeping $C^{(i)}$ fixed.\n",
    "\n",
    "Because each iterative step is designed to reduce (or at worst, not increase) $J$, the algorithm is guaranteed to converge to a local minimum of the cost function.\n",
    "\n",
    "---\n",
    "\n",
    "## Enhancing K-Means: Advanced Considerations\n",
    "\n",
    "### **Multiple Random Initializations**\n",
    "- **Problem:**  \n",
    "  A poor initial guess may lead to a local optimum that does not represent the best clustering.\n",
    "- **Solution:**  \n",
    "  Run K-means several times (often 50–1000 iterations) with different random starting centroids. For each run, compute the cost function $J$. Select the clustering result with the lowest $J$ as the final solution.\n",
    "\n",
    "### **Choosing the Number of Clusters, $K$**\n",
    "- **Ambiguity:**  \n",
    "  In many datasets, the \"true\" number of clusters is not obvious.\n",
    "- **Elbow Method:**  \n",
    "  Run K-means for a range of $K$ values and plot the cost function $J$ against $K$. Look for an \"elbow\" in the curve where the rate of decrease sharply slows—this can be a heuristic for the optimal $K$.\n",
    "- **Application-Driven:**  \n",
    "  The choice of $K$ might depend on downstream tasks. For example:\n",
    "  - **T-Shirt Sizing:**  \n",
    "    Three clusters might correspond to small, medium, and large sizes. Five clusters could allow extra small and extra large options, balancing fit quality with manufacturing costs.\n",
    "  - **Image Compression:**  \n",
    "    More clusters mean higher image quality but less compression. Fewer clusters yield higher compression at the cost of quality.\n",
    "\n",
    "### **Initialization Techniques**\n",
    "- **K-means++:**  \n",
    "  An enhancement over random initialization, K-means++ spreads out the initial centroids to reduce the chances of poor clustering and speed up convergence.\n",
    "\n",
    "### **Limitations of K-Means**\n",
    "- **Cluster Shape:**  \n",
    "  K-means assumes clusters are spherical (or convex) and similar in size. It may perform poorly if clusters are irregularly shaped or vary widely in density.\n",
    "- **Scaling:**  \n",
    "  Since the algorithm is based on distance measurements, feature scaling (e.g., standardization) is crucial.\n",
    "- **Local Optima:**  \n",
    "  K-means can get stuck in local minima, making multiple initializations necessary.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Details and Practical Tips\n",
    "\n",
    "### **Algorithm Pseudocode**\n",
    "\n",
    "```pseudo\n",
    "Initialize centroids μ₁, μ₂, ..., μₖ randomly (or using K-means++)\n",
    "Repeat until convergence:\n",
    "    For each data point x⁽ⁱ⁾:\n",
    "        Assign C⁽ⁱ⁾ = argminₖ || x⁽ⁱ⁾ - μₖ ||²\n",
    "    For each cluster k:\n",
    "        Update μₖ = average of all x⁽ⁱ⁾ with C⁽ⁱ⁾ = k\n",
    "```\n",
    "\n",
    "### **Cost Function Monitoring**\n",
    "- **Convergence Test:**  \n",
    "  Monitor the cost function $J$. If it stops decreasing (or decreases very slowly), the algorithm has likely converged.\n",
    "- **Debugging:**  \n",
    "  If $J$ increases at any step, it is a sign of an implementation error.\n",
    "\n",
    "### **Real-World Application Example: Image Compression**\n",
    "- **How It Works:**  \n",
    "  Each pixel's color is treated as a data point in a color space (e.g., RGB). K-means groups similar colors together, and each pixel is then approximated by the nearest centroid.\n",
    "- **Trade-Off:**  \n",
    "  More clusters mean the compressed image is closer to the original, but requires more storage for the color palette. Fewer clusters lead to greater compression but may result in noticeable quality loss.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "- **Clustering** is a method for discovering groups within unlabeled data.\n",
    "- **K-means** is an iterative algorithm that partitions data into $K$ clusters by alternating between:\n",
    "  - **Assignment:** Assigning each data point to its nearest centroid.\n",
    "  - **Update:** Recalculating centroids as the mean of the points in each cluster.\n",
    "- The algorithm minimizes a **cost function** (distortion function), defined as the average squared distance between data points and their assigned centroids.\n",
    "- **Advanced considerations** include running multiple random initializations, choosing $K$ via methods like the elbow method, and enhancements such as K-means++.\n",
    "- **Limitations:** K-means works best for spherical, equally sized clusters and is sensitive to feature scaling and initial conditions.\n",
    "\n",
    "By understanding these detailed concepts and techniques, one can effectively implement and apply K-means clustering to a wide variety of practical problems—from market segmentation and genetic analysis to image compression and beyond.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
