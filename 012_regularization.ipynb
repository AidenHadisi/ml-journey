{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0401e36a-7dd3-492c-b415-f2196062b076",
   "metadata": {},
   "source": [
    "# Underfitting vs. Overfitting in Machine Learning\n",
    "\n",
    "**Key Problem Areas**\n",
    "- **Underfitting (High Bias):** The model is too simple and fails to capture the underlying trend.\n",
    "- **Overfitting (High Variance):** The model fits the training data too closely, capturing noise and failing to generalize.\n",
    "\n",
    "---\n",
    "\n",
    "## Regression Example: Predicting Housing Prices\n",
    "\n",
    "- **Input Feature:** $x$ (size of the house)\n",
    "- **Output Value:** $y$ (price of the house)\n",
    "\n",
    "### Models Considered\n",
    "1. **Simple Linear Model:**\n",
    "   - **Equation:** $y = \\theta_0 + \\theta_1 x$\n",
    "   - **Observation:** The straight-line fit doesn't capture the flattening trend seen in the data.\n",
    "   - **Result:** **Underfitting (High Bias)** – the model has a strong linear preconception, ignoring the curvature in data.\n",
    "\n",
    "2. **Quadratic Model:**\n",
    "   - **Features:** $x$ and $x^2$\n",
    "   - **Equation:** $y = \\theta_0 + \\theta_1 x + \\theta_2 x^2$\n",
    "   - **Observation:** The curve fits the data better without perfectly matching every training point.\n",
    "   - **Result:** **Just Right** – balances fitting the training data while generalizing well to unseen examples.\n",
    "\n",
    "3. **Fourth-Order Polynomial Model:**\n",
    "   - **Features:** $x, x^2, x^3, x^4$\n",
    "   - **Observation:** The model passes through all training points exactly.\n",
    "   - **Result:** **Overfitting (High Variance)** – the model is too wiggly, capturing noise and unlikely to generalize.\n",
    "\n",
    "### Visual Summary\n",
    "\n",
    "| Model Type          | Features Used                | Fit to Training Data       | Generalization    |\n",
    "|---------------------|------------------------------|----------------------------|-------------------|\n",
    "| Underfitting        | Linear ($x$)                 | Poor (misses curvature)    | Poor              |\n",
    "| Just Right          | Quadratic ($x$, $x^2$)         | Good (balanced fit)        | Good              |\n",
    "| Overfitting         | Fourth-order ($x$, $x^2$, $x^3$, $x^4$) | Perfect fit (too complex) | Poor              |\n",
    "\n",
    "---\n",
    "\n",
    "## Classification Example: Classifying Tumors\n",
    "\n",
    "**Features:**\n",
    "- $x_1$: Tumor size\n",
    "- $x_2$: Patient age\n",
    "\n",
    "**Task:** \n",
    "- Classify tumors as malignant or benign.\n",
    "\n",
    "### Logistic Regression Models\n",
    "1. **Simple Logistic Regression:**\n",
    "   - **Model Formulation:** \n",
    "\n",
    "$$ g(z) = \\frac{1}{1+e^{-z}} \\quad \\text{where} \\quad z = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 $$\n",
    "\n",
    "   - **Decision Boundary:** A straight line where $z=0$.\n",
    "   - **Result:** **Underfitting (High Bias)** – the boundary is too simple and may misclassify data points.\n",
    "\n",
    "2. **Quadratic Logistic Regression:**\n",
    "   - **Extended Features:** Include quadratic terms (e.g., $x_1^2$, $x_2^2$, and possibly $x_1x_2$).\n",
    "   - **Decision Boundary:** Curved (e.g., an ellipse) that better separates the classes.\n",
    "   - **Result:** **Just Right** – offers a balance between fitting the data and generalizing well.\n",
    "\n",
    "3. **High-Order Polynomial Logistic Regression:**\n",
    "   - **Extended Features:** Many higher-order polynomial terms.\n",
    "   - **Observation:** The decision boundary becomes very contorted to perfectly fit the training examples.\n",
    "   - **Result:** **Overfitting (High Variance)** – although it classifies the training set perfectly, it is unlikely to perform well on new data.\n",
    "\n",
    "---\n",
    "\n",
    "## The Goldilocks Analogy\n",
    "\n",
    "- **Underfitting:** Like a bowl of porridge that's too cold – the model is too simple.\n",
    "- **Overfitting:** Like a bowl of porridge that's too hot – the model is excessively complex.\n",
    "- **Just Right:** Like a bowl of porridge that's perfectly warm – the model achieves the ideal balance between bias and variance.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "**Underfitting (High Bias):**\n",
    "- Model cannot capture the underlying data trends.\n",
    "- Simplistic assumptions (e.g., assuming data is purely linear).\n",
    "- Poor performance on both training and new data.\n",
    "\n",
    "**Overfitting (High Variance):**\n",
    "- Model fits the training data excessively well, including noise.\n",
    "- Highly sensitive to minor changes in the training data.\n",
    "- Poor generalization to unseen data.\n",
    "\n",
    "**Ideal Model (Just Right):**\n",
    "- Achieves a balance between bias and variance.\n",
    "- Captures essential patterns without being overly complex.\n",
    "- Generalizes well to new, unseen examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7131eea3-e5a8-4a22-ac3c-c54125feb7f0",
   "metadata": {},
   "source": [
    "# Strategies to Address Overfitting\n",
    "\n",
    "Remember that overfitting occurs when a model fits the training data too well, capturing noise instead of the underlying distribution, leading to poor generalization to new data.\n",
    "\n",
    "**Three Main Strategies to Combat Overfitting:**\n",
    "1. **Collect More Training Data**\n",
    "2. **Feature Selection (Use Fewer Features)**\n",
    "3. **Regularization**\n",
    "\n",
    "## 1. Collect More Training Data\n",
    "\n",
    "Increasing the number of training examples helps the algorithm learn a function that is smoother and less wiggly.\n",
    "- **Advantage:** A larger dataset provides a better representation of the true underlying distribution, which can prevent the model from fitting noise.\n",
    "- **Limitation:** More data may not be available in some scenarios (e.g., limited sales data in a small market).\n",
    "\n",
    "**Practical Example:**  \n",
    "\n",
    "For housing price prediction: If you currently have limited data on house sizes and prices, adding more examples can help the model avoid overreacting to outliers or noise in a small dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Feature Selection (Using Fewer Features)\n",
    "\n",
    "By selecting only the most relevant features, you reduce the complexity of the model, thereby reducing its tendency to overfit.\n",
    "\n",
    "- **Manual Feature Selection:**  Use your domain intuition to select features that are most relevant.\n",
    "- **Pros:** Reduces overfitting by lowering model complexity.\n",
    "- **Cons:** May discard potentially useful information.\n",
    "- **Automated Methods:** Later in the course, you will explore algorithms that automatically choose the most appropriate set of features.\n",
    "\n",
    "**Example Scenario:**  \n",
    "- **With Many Features:** A model using 100 features (e.g., house size, number of bedrooms, age, income level in the neighborhood, distance to the nearest coffee shop, etc.) might overfit if there isn’t enough training data.\n",
    "- **With Fewer Features:** Choosing a subset like **size**, **number of bedrooms**, and **age** might help the model generalize better.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Regularization\n",
    "\n",
    "Regularization is a technique that shrinks the values of the model parameters, effectively reducing the impact of less important features without completely eliminating them.\n",
    "\n",
    "When using polynomial features (e.g., $x$, $x^2$, $x^3$, etc.), the parameters (weights) for higher-order terms can become very large. **Regularization** adds a penalty for large parameter values, encouraging the model to keep the parameters small and the decision boundary smoother.\n",
    "\n",
    "Imagine dimming a set of overly bright lights; you don't turn them off completely, but you reduce their intensity so they don't overwhelm the overall lighting.\n",
    "\n",
    "**Mathematical Formulation (General Idea):**  \n",
    "\n",
    "If the cost function for a model is:\n",
    "\n",
    "$$ J(\\theta) = \\text{Loss}(\\theta) $$\n",
    "\n",
    "Regularization adds a term such as:\n",
    "\n",
    "$$ J_{\\text{reg}}(\\theta) = \\text{Loss}(\\theta) + \\lambda \\sum_{j=1}^{n} \\theta_j^2 $$\n",
    "\n",
    "Here, $\\lambda$ is a hyperparameter that controls the amount of regularization.\n",
    "  \n",
    "**Parameter Considerations:**\n",
    "- Typically, only the weights $w_1, w_2, \\dots, w_n$ are regularized.\n",
    "- The bias term $b$ is usually excluded or not heavily regularized.\n",
    "\n",
    "**Benefits:**  \n",
    "- Allows the use of all features while preventing any single feature from having an overly large impact.\n",
    "- **Flexibility:** Unlike feature selection, regularization doesn't completely remove a feature; it just reduces its influence.\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Learning Resources: Overfitting Lab\n",
    "\n",
    "- **Interactive Lab Features:**\n",
    "  - Visualize different examples of overfitting in both regression and classification.\n",
    "  - Adjust the degree of the polynomial (e.g., $x$, $x^2$, $x^3$, etc.) and see how the model's fit changes.\n",
    "  - Experiment with adding more training data and using feature selection to observe their effects on overfitting.\n",
    "\n",
    "- **Purpose:**  \n",
    "  This lab will help build intuition around:\n",
    "  - How overfitting occurs.\n",
    "  - The impact of various techniques (data augmentation, feature selection, and regularization) in mitigating overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Addressing Overfitting:**\n",
    "  1. **Collect More Data:**  \n",
    "     More training examples help the model learn a less complex function.\n",
    "  2. **Feature Selection:**  \n",
    "     Using a subset of relevant features reduces complexity, mitigating overfitting.\n",
    "  3. **Regularization:**  \n",
    "     Shrinks parameter values to prevent features from having an overly large impact, without completely discarding any feature.\n",
    "\n",
    "- **Takeaway:**  \n",
    "  In practice, the best strategy often involves a combination of these approaches. Understanding when and how to apply each technique is critical for building models that generalize well to new data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab6cf5f-3131-4432-870e-ce7c187f3dea",
   "metadata": {},
   "source": [
    "# Regularization: Modifying the Cost Function\n",
    "\n",
    "**Goal:** Keep the parameters ($W_1, W_2, \\dots, W_N$) small to prevent the model from becoming overly complex and overfitting the training data.\n",
    "\n",
    "**Example Recap:**  \n",
    "- **Quadratic Fit:** A quadratic function can provide a good fit for the data.\n",
    "- **High-Order Polynomial Fit:** A high-order polynomial may overfit the data by being too wiggly.\n",
    "\n",
    "**Key Idea:** If you penalize large values for specific parameters (e.g., $W_3$ and $W_4$), they are forced to be close to zero, effectively reducing the contribution of higher-order (or less important) features.\n",
    "\n",
    "---\n",
    "\n",
    "## Modified Cost Function\n",
    "\n",
    "**Standard Linear Regression Cost Function:**\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2 $$\n",
    "\n",
    "**Modified Cost Function:**\n",
    "\n",
    "$$ J_{\\text{reg}}(\\theta) = \\frac{1}{2m} \\left[ \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2 \\right] $$\n",
    "  \n",
    "**Components:**\n",
    "- **Mean Squared Error Term:** Encourages the model to fit the training data well.\n",
    "- **Regularization Term:** $\\lambda \\sum_{j=1}^{n} \\theta_j^2$ penalizes large parameter values, effectively shrinking them.\n",
    "  \n",
    "**Notes on Conventions:**\n",
    "- The term is often scaled by $\\frac{\\lambda}{2m}$ so both components are on a similar scale.\n",
    "- By convention, **do not regularize the bias term** ($b$ or $\\theta_0$), though some implementations might include it; the difference is usually minimal.\n",
    "\n",
    "---\n",
    "\n",
    "## Trade-Off Controlled by $\\lambda$\n",
    "\n",
    "- **$\\lambda = 0$:**\n",
    "  - **Effect:** No regularization applied.\n",
    "  - **Result:** Model may overfit, especially if it is overly complex.\n",
    "  \n",
    "- **$\\lambda$ is Very Large (e.g., $10^{10}$):**\n",
    "  - **Effect:** Heavy penalty on parameter sizes forces all $\\theta_j$ (for $j \\geq 1$) to be near 0.\n",
    "  - **Result:** Model becomes overly simple (e.g., a horizontal line) and underfits the data.\n",
    "  \n",
    "- **Choosing $\\lambda$:**\n",
    "  - **Balance is Key:** A moderate value of $\\lambda$ ensures the model fits the data well while keeping parameters small enough to avoid overfitting.\n",
    "  - **Model Selection:** Later in the course, techniques for choosing the optimal $\\lambda$ will be discussed.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Summary and Takeaways\n",
    "\n",
    "- **Regularization Mechanism:**\n",
    "  - Penalizes large parameters by adding a term $\\lambda \\sum_{j=1}^{n} \\theta_j^2$ to the cost function.\n",
    "  - Encourages simpler models, similar in effect to reducing the number of features.\n",
    "\n",
    "- **Trade-Off in the Cost Function:**\n",
    "  - **Minimizing Mean Squared Error:** Fits the training data well.\n",
    "  - **Minimizing the Regularization Term:** Keeps parameters small to reduce overfitting.\n",
    "  \n",
    "- **Overall Impact: Proper $\\lambda$ Setting** Leads to a model that is neither too complex (overfitting) nor too simple (underfitting).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab0115-2459-472e-98c2-85532fd90b60",
   "metadata": {},
   "source": [
    "# Gradient Descent with Regularized Linear Regression\n",
    "\n",
    "The goal is to minimize a modified cost function that includes both the usual squared error term and an additional term to penalize large parameter values, thereby reducing overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Regularized Cost Function\n",
    "\n",
    "The **unregularized cost function** for linear regression is:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "The **modified (regularized) cost function** becomes:\n",
    "\n",
    "$$\n",
    "J_{\\text{reg}}(\\theta) = \\frac{1}{2m} \\left[ \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2 \\right]\n",
    "$$\n",
    "\n",
    "**Components:**\n",
    "- **Error Term:** Measures the squared difference between predictions and actual values.\n",
    "- **Regularization Term:** $\\lambda \\sum_{j=1}^{n} \\theta_j^2$ penalizes large weights to keep the model simpler.\n",
    "\n",
    "**Conventions:**\n",
    "- We scale by $\\frac{1}{2m}$ for both terms to keep them on a similar scale.\n",
    "- **Bias term ($b$ or $\\theta_0$)** is not regularized.\n",
    "\n",
    "---\n",
    "\n",
    "## Gradient Descent Updates\n",
    "\n",
    "For **standard gradient descent**:\n",
    "\n",
    "**For each weight $w_j$ ($j = 1, 2, \\dots, n$):**\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\cdot \\frac{\\partial J}{\\partial w_j}\n",
    "$$\n",
    "\n",
    "**For bias $b$:**\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\cdot \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "**With regularization**, the derivative for $w_j$ changes while $b$ remains the same.\n",
    "\n",
    "**New Partial Derivative for $w_j$:**\n",
    "\n",
    "$$\n",
    "  \\frac{\\partial J_{\\text{reg}}}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)x_j^{(i)} + \\frac{\\lambda}{m}w_j\n",
    "$$\n",
    "\n",
    "**Update Rule for $w_j$:**\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\left[ \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)x_j^{(i)} + \\frac{\\lambda}{m}w_j \\right]\n",
    "$$\n",
    "\n",
    "**Update Rule for $b$:**\n",
    "$$\n",
    "b := b - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "> **Note:** The bias term $b$ is not regularized.\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition Behind the Update\n",
    "\n",
    "The update rule for $w_j$ can be rearranged as follows:\n",
    "\n",
    "$$\n",
    "w_j := \\left(1 - \\alpha \\frac{\\lambda}{m}\\right) w_j - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)x_j^{(i)}\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "- **Shrinkage Factor:** The term $\\left(1 - \\alpha \\frac{\\lambda}{m}\\right)$ multiplies $w_j$ at every iteration, gradually shrinking it.  \n",
    "\n",
    "**Example:** With $\\alpha = 0.01$, $\\lambda = 1$, and $m = 50$, we get:\n",
    "\n",
    "$$\n",
    "1 - \\alpha \\frac{\\lambda}{m} = 1 - \\frac{0.01}{50} = 0.9998\n",
    "$$\n",
    "\n",
    "Thus, on each iteration, $w_j$ is scaled by approximately 0.9998.\n",
    "\n",
    "**Usual Gradient Descent Component:** The remaining term is the standard gradient descent update for unregularized linear regression.\n",
    "\n",
    "**Effect:** Regularization slowly reduces the magnitude of the weights, helping to control model complexity and prevent overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Derivative Calculation Overview\n",
    "\n",
    "Here's a brief look at how the derivative with respect to $w_j$ is derived:\n",
    "1. **Start with the cost function:**\n",
    "\n",
    "$$\n",
    "J_{\\text{reg}}(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2\n",
    "$$\n",
    "\n",
    "2. **Differentiate with respect to $w_j$:**\n",
    "\n",
    "For the error term, using the chain rule, you obtain:\n",
    "$$\n",
    "\\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)x_j^{(i)}\n",
    "$$\n",
    "\n",
    "For the regularization term:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_j} \\left( \\frac{\\lambda}{2m} w_j^2 \\right) = \\frac{\\lambda}{m} w_j\n",
    "$$\n",
    "\n",
    "3. **Combine the two derivatives to yield:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{\\text{reg}}}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)x_j^{(i)} + \\frac{\\lambda}{m} w_j\n",
    "$$\n",
    "\n",
    "> **Note:** The bias term $b$ is unaffected by regularization.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Summary\n",
    "\n",
    "- **Objective:** Modify gradient descent to work with a regularized cost function that prevents overfitting by keeping weights small.\n",
    "- **Key Changes:**\n",
    "  - **Cost Function:** Augmented with a regularization term: $\\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2$.\n",
    "  - **Gradient Update for Weights:** Incorporates an extra term $\\frac{\\lambda}{m}w_j$, leading to a multiplicative shrinkage factor.\n",
    "  - **Bias Term:** Remains unchanged (not regularized).\n",
    "\n",
    "- **Impact on Learning:**\n",
    "  - Regularization helps control model complexity.\n",
    "  - Choosing appropriate $\\lambda$ balances the trade-off between fitting the data and keeping the model simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738069ed-aab7-4004-845c-085d941b9075",
   "metadata": {},
   "source": [
    "# Regularized Logistic Regression\n",
    "\n",
    "How to implement regularized logistic regression using gradient descent? \n",
    "\n",
    "The approach is very similar to regularized linear regression, with the main difference being the use of the logistic (sigmoid) function for predictions.\n",
    "\n",
    "- **Problem:** Logistic regression with many features (especially high-order polynomial features) can overfit the training data, resulting in an overly complex decision boundary.\n",
    "- **Solution:** Apply regularization by adding a penalty term to the cost function to keep the parameter values small. This helps to generalize better to new data.\n",
    "\n",
    "---\n",
    "\n",
    "## Cost Function with Regularization\n",
    "\n",
    "- **Unregularized Cost Function:**\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log f(z^{(i)}) + (1-y^{(i)}) \\log (1 - f(z^{(i)})) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $f(z) = \\frac{1}{1 + e^{-z}}$ (the sigmoid function)\n",
    "- $z$ is typically a high-order polynomial in the features\n",
    "\n",
    "**Modified Cost Function:**\n",
    "\n",
    "$$\n",
    "J_{\\text{reg}}(\\theta) = J(\\theta) + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2\n",
    "$$\n",
    "\n",
    "**Explanation:**\n",
    "- The first part is the usual logistic regression cost.\n",
    "- The second part is the regularization term:\n",
    "  - $\\lambda$ is the regularization parameter controlling the strength of the penalty.\n",
    "  - The sum runs over all feature weights ($w_1, w_2, \\dots, w_n$), excluding the bias term $b$.\n",
    "\n",
    "**Effect:** Penalizes large weights, thereby simplifying the decision boundary and reducing overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Gradient Descent Updates for Regularized Logistic Regression\n",
    "\n",
    "**For each weight $w_j$ ($j = 1, 2, \\dots, n$):**\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\left[ \\frac{1}{m} \\sum_{i=1}^{m} \\left( f(z^{(i)}) - y^{(i)} \\right)x_j^{(i)} + \\frac{\\lambda}{m}w_j \\right]\n",
    "$$\n",
    "\n",
    "- The term $\\frac{\\lambda}{m}w_j$ is the additional derivative coming from the regularization term.\n",
    "- This update is similar to that of regularized linear regression, except that:\n",
    "    - $f(z)$ now represents the logistic function.\n",
    "    - The standard gradient descent term for logistic regression is used.\n",
    "\n",
    "**For the bias term $b$:**\n",
    "$$\n",
    "b := b - \\alpha \\left[ \\frac{1}{m} \\sum_{i=1}^{m} \\left( f(z^{(i)}) - y^{(i)} \\right) \\right]\n",
    "$$\n",
    "\n",
    "> **Note:** The bias term is **not regularized.**\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- **Similarity to Linear Regression:** The gradient update equations are nearly identical to those for regularized linear regression, with the only difference being the hypothesis function (sigmoid for logistic regression vs. linear for regression).\n",
    "- **Regularization Effect:** Regularization shrinks the weights by adding a term proportional to the weight value itself, thereby reducing overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Implementation\n",
    "\n",
    "**Implementation Tips:**\n",
    "- Ensure that the updates for all parameters ($w_j$ and $b$) are done **simultaneously.**\n",
    "- Choose an appropriate value for $\\lambda$:\n",
    "    - **Too low ($\\lambda = 0$):** No regularization; the model may overfit.\n",
    "    - **Too high (e.g., $\\lambda \\gg 1$):** Excessive shrinkage, leading to underfitting.\n",
    "- In the practice lab, you will have the opportunity to experiment with different $\\lambda$ values to observe their effect on the decision boundary.\n",
    "\n",
    "- **Code Insight:**\n",
    "  - Review the provided code for implementing regularized logistic regression.\n",
    "  - Understand how the regularization term is integrated into the gradient descent update.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Final Thoughts and Next Steps\n",
    "\n",
    "- **Real-World Impact:**  \n",
    "  Understanding when and how to reduce overfitting is a crucial skill in machine learning, leading to models that perform better on unseen data.\n",
    "  \n",
    "- **Future Learning:**\n",
    "  - This course has covered key concepts in linear and logistic regression.\n",
    "  - In the next course, you will learn about neural networks (deep learning), which build upon these fundamental techniques (cost functions, gradient descent, and regularization).\n",
    "\n",
    "- **Encouragement:**  \n",
    "  Congratulations on mastering these foundational concepts! Keep practicing with the labs and quizzes, and prepare for more advanced topics like neural networks in the coming materials.\n",
    "\n",
    "---\n",
    "\n",
    "> **[!TIP] Real-World Application**\n",
    ">  \n",
    "> Many companies leverage regularized logistic regression to ensure that their models generalize well, which is critical for applications such as fraud detection, medical diagnosis, and more. Your ability to implement and tune these models can lead to significant real-world value.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
