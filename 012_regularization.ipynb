{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0401e36a-7dd3-492c-b415-f2196062b076",
   "metadata": {},
   "source": [
    "# Underfitting vs. Overfitting in Machine Learning\n",
    "\n",
    "In machine learning, two common pitfalls during model training are **underfitting** and **overfitting**. Understanding these issues and how to address them is essential for building models that generalize well to new data.\n",
    "\n",
    "- **Underfitting (High Bias):**  \n",
    "  The model is too simple to capture the underlying trends in the data. This results in poor performance on both the training set and unseen data.\n",
    "\n",
    "- **Overfitting (High Variance):**  \n",
    "  The model learns the noise in the training data in addition to the underlying trend. Although it performs exceptionally well on the training set, its performance on new data is poor.\n",
    "\n",
    "- **Ideal Model (Just Right):**  \n",
    "  Strikes a balance between bias and variance. It captures the essential patterns without being overly complex, thereby generalizing well.\n",
    "\n",
    "### The Goldilocks Principle\n",
    "\n",
    "- **Underfitting:** Like a bowl of porridge that's too cold – the model is too simplistic.\n",
    "- **Overfitting:** Like a bowl of porridge that's too hot – the model is overly complex.\n",
    "- **Just Right:** Like a bowl of porridge that's perfectly warm – the model balances complexity and simplicity.\n",
    "\n",
    "---\n",
    "\n",
    "## Illustrative Examples\n",
    "\n",
    "### 1. Regression Example: Predicting Housing Prices\n",
    "\n",
    "Consider predicting housing prices with different levels of model complexity. The table below summarizes the effects:\n",
    "\n",
    "| **Model Type**     | **Features Used**                | **Fit to Training Data**       | **Generalization** |\n",
    "|--------------------|----------------------------------|--------------------------------|--------------------|\n",
    "| **Underfitting**   | Linear ($x$)                     | Poor (misses curvature)        | Poor               |\n",
    "| **Just Right**     | Quadratic ($x$, $x^2$)            | Good (balanced fit)            | Good               |\n",
    "| **Overfitting**    | Fourth-order ($x$, $x^2$, $x^3$, $x^4$) | Perfect fit (excessively complex) | Poor         |\n",
    "\n",
    "*Example Explanation:*  \n",
    "A linear model may fail to capture the curvature in housing prices, leading to underfitting, whereas a fourth-order polynomial might capture every fluctuation in the training data (including noise), resulting in overfitting. A quadratic model might offer the right balance.\n",
    "\n",
    "### 2. Classification Example: Classifying Tumors\n",
    "\n",
    "Suppose we want to classify tumors (malignant vs. benign) using two features:\n",
    "- $x_1$: Tumor size  \n",
    "- $x_2$: Patient age\n",
    "\n",
    "#### Logistic Regression Variants\n",
    "\n",
    "1. **Simple Logistic Regression:**  \n",
    "   - **Model:**  \n",
    "\n",
    "$$\n",
    "g(z) = \\frac{1}{1+e^{-z}}, \\quad \\text{with} \\quad z = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2.\n",
    "$$\n",
    "\n",
    "   - **Decision Boundary:** A straight line ($z=0$).  \n",
    "   - **Outcome:** **Underfitting** – the simple linear boundary may not capture the true separation between classes.\n",
    "\n",
    "2. **Quadratic Logistic Regression:**  \n",
    "   - **Model Extension:** Include quadratic terms such as $x_1^2$, $x_2^2$, and interaction $x_1x_2$.  \n",
    "   - **Decision Boundary:** A curved line (e.g., elliptical), which can better separate the classes.  \n",
    "   - **Outcome:** **Balanced fit** – the model generalizes well without being too simple or overly complex.\n",
    "\n",
    "3. **High-Order Polynomial Logistic Regression:**  \n",
    "   - **Model Extension:** Incorporate many higher-order polynomial features.  \n",
    "   - **Outcome:** **Overfitting** – the decision boundary becomes overly contorted, fitting the training data too closely and likely failing on new data.\n",
    "\n",
    "---\n",
    "\n",
    "## Strategies to Address Overfitting\n",
    "\n",
    "Overfitting arises when a model learns the noise in the training data rather than the underlying distribution. Three primary strategies to mitigate overfitting are:\n",
    "\n",
    "### 1. Collect More Training Data\n",
    "\n",
    "- **Idea:**  \n",
    "  More data provides a better representation of the true data distribution, which helps the model learn a smoother, more general function.\n",
    "  \n",
    "- **Example:**  \n",
    "  In housing price prediction, increasing the dataset size can help the model avoid being misled by outliers.\n",
    "\n",
    "### 2. Feature Selection (Use Fewer Features)\n",
    "\n",
    "- **Idea:**  \n",
    "  By using only the most relevant features, you reduce model complexity, thereby decreasing the risk of overfitting.\n",
    "  \n",
    "- **Approaches:**\n",
    "  - **Manual Selection:** Use domain expertise to choose key features.\n",
    "  - **Automated Methods:** Algorithms can help identify the most informative features.\n",
    "  \n",
    "- **Example:**  \n",
    "  Instead of using 100 features, selecting features like house size, number of bedrooms, and age can improve the model's ability to generalize.\n",
    "\n",
    "### 3. Regularization\n",
    "\n",
    "Regularization adds a penalty to the cost function to discourage overly large weights, thereby controlling model complexity.\n",
    "\n",
    "---\n",
    "\n",
    "## Regularization in Machine Learning Models\n",
    "\n",
    "Regularization can be applied to both linear and logistic regression models. The core idea is to modify the cost function by adding a penalty term proportional to the square of the weights.\n",
    "\n",
    "### Regularization in Linear Regression\n",
    "\n",
    "**Standard Cost Function:**\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2,\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $m$ is the number of training examples.\n",
    "- $h_\\theta(x)$ is the hypothesis function.\n",
    "\n",
    "**Regularized Cost Function:**\n",
    "\n",
    "$$\n",
    "J_{\\text{reg}}(\\theta) = \\frac{1}{2m} \\left[ \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2 \\right],\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\lambda$ is the regularization parameter.\n",
    "- The summation for regularization is typically applied to all weights $\\theta_j$ (excluding the bias term $\\theta_0$).\n",
    "\n",
    "**Gradient Descent Updates:**\n",
    "\n",
    "- **For each weight $w_j$ ($j \\geq 1$):**\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\left[ \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)x_j^{(i)} + \\frac{\\lambda}{m}w_j \\right],\n",
    "$$\n",
    "\n",
    "- **For the bias term $b$ (or $\\theta_0$):**\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\left[ \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) \\right].\n",
    "$$\n",
    "\n",
    "**Interpretation:**  \n",
    "The additional term $\\frac{\\lambda}{m}w_j$ in the gradient update acts as a **shrinkage factor** on the weights, gradually reducing their magnitude and thus preventing the model from becoming too complex.\n",
    "\n",
    "---\n",
    "\n",
    "### Regularization in Logistic Regression\n",
    "\n",
    "Logistic regression uses a sigmoid function for predictions. The approach to regularization is similar to linear regression.\n",
    "\n",
    "**Unregularized Logistic Regression Cost Function:**\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log f(z^{(i)}) + (1 - y^{(i)}) \\log \\left(1 - f(z^{(i)})\\right) \\right],\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $f(z) = \\frac{1}{1 + e^{-z}}$ is the sigmoid function.\n",
    "- $z = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots$\n",
    "\n",
    "**Regularized Cost Function:**\n",
    "\n",
    "$$\n",
    "J_{\\text{reg}}(\\theta) = J(\\theta) + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2.\n",
    "$$\n",
    "\n",
    "**Gradient Descent Updates:**\n",
    "\n",
    "- **For each weight $w_j$ ($j \\geq 1$):**\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\left[ \\frac{1}{m} \\sum_{i=1}^{m} \\left( f(z^{(i)}) - y^{(i)} \\right)x_j^{(i)} + \\frac{\\lambda}{m}w_j \\right],\n",
    "$$\n",
    "\n",
    "- **For the bias term $b$:**\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\left[ \\frac{1}{m} \\sum_{i=1}^{m} \\left( f(z^{(i)}) - y^{(i)} \\right) \\right].\n",
    "$$\n",
    "\n",
    "> **Note:** In both linear and logistic regression, the bias term is **not regularized**.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Underfitting** arises when the model is too simple (high bias), while **overfitting** occurs when the model is too complex (high variance).\n",
    "- The goal is to achieve a **balanced model** that generalizes well.\n",
    "- **Regularization** is an effective strategy to prevent overfitting by penalizing large weights, which keeps the model simpler.\n",
    "- Both linear and logistic regression models can be regularized by modifying their cost functions and gradient descent updates.\n",
    "\n",
    "By applying these strategies—collecting more data, selecting relevant features, and using regularization—you can build robust models that perform well on both training and unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7131eea3-e5a8-4a22-ac3c-c54125feb7f0",
   "metadata": {},
   "source": [
    "# Strategies to Address Overfitting\n",
    "\n",
    "Remember that overfitting occurs when a model fits the training data too well, capturing noise instead of the underlying distribution, leading to poor generalization to new data.\n",
    "\n",
    "**Three Main Strategies to Combat Overfitting:**\n",
    "1. **Collect More Training Data**\n",
    "2. **Feature Selection (Use Fewer Features)**\n",
    "3. **Regularization**\n",
    "\n",
    "## 1. Collect More Training Data\n",
    "\n",
    "Increasing the number of training examples helps the algorithm learn a function that is smoother and less wiggly.\n",
    "- **Advantage:** A larger dataset provides a better representation of the true underlying distribution, which can prevent the model from fitting noise.\n",
    "- **Limitation:** More data may not be available in some scenarios (e.g., limited sales data in a small market).\n",
    "\n",
    "**Practical Example:**  \n",
    "\n",
    "For housing price prediction: If you currently have limited data on house sizes and prices, adding more examples can help the model avoid overreacting to outliers or noise in a small dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Feature Selection (Using Fewer Features)\n",
    "\n",
    "By selecting only the most relevant features, you reduce the complexity of the model, thereby reducing its tendency to overfit.\n",
    "\n",
    "- **Manual Feature Selection:**  Use your domain intuition to select features that are most relevant.\n",
    "- **Pros:** Reduces overfitting by lowering model complexity.\n",
    "- **Cons:** May discard potentially useful information.\n",
    "- **Automated Methods:** Later in the course, you will explore algorithms that automatically choose the most appropriate set of features.\n",
    "\n",
    "**Example Scenario:**  \n",
    "- **With Many Features:** A model using 100 features (e.g., house size, number of bedrooms, age, income level in the neighborhood, distance to the nearest coffee shop, etc.) might overfit if there isn’t enough training data.\n",
    "- **With Fewer Features:** Choosing a subset like **size**, **number of bedrooms**, and **age** might help the model generalize better.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Regularization\n",
    "\n",
    "Regularization is a technique that shrinks the values of the model parameters, effectively reducing the impact of less important features without completely eliminating them.\n",
    "\n",
    "When using polynomial features (e.g., $x$, $x^2$, $x^3$, etc.), the parameters (weights) for higher-order terms can become very large. **Regularization** adds a penalty for large parameter values, encouraging the model to keep the parameters small and the decision boundary smoother.\n",
    "\n",
    "Imagine dimming a set of overly bright lights; you don't turn them off completely, but you reduce their intensity so they don't overwhelm the overall lighting.\n",
    "\n",
    "**Mathematical Formulation (General Idea):**  \n",
    "\n",
    "If the cost function for a model is:\n",
    "\n",
    "$$ J(\\theta) = \\text{Loss}(\\theta) $$\n",
    "\n",
    "Regularization adds a term such as:\n",
    "\n",
    "$$ J_{\\text{reg}}(\\theta) = \\text{Loss}(\\theta) + \\lambda \\sum_{j=1}^{n} \\theta_j^2 $$\n",
    "\n",
    "Here, $\\lambda$ is a hyperparameter that controls the amount of regularization.\n",
    "  \n",
    "**Parameter Considerations:**\n",
    "- Typically, only the weights $w_1, w_2, \\dots, w_n$ are regularized.\n",
    "- The bias term $b$ is usually excluded or not heavily regularized.\n",
    "\n",
    "**Benefits:**  \n",
    "- Allows the use of all features while preventing any single feature from having an overly large impact.\n",
    "- **Flexibility:** Unlike feature selection, regularization doesn't completely remove a feature; it just reduces its influence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab6cf5f-3131-4432-870e-ce7c187f3dea",
   "metadata": {},
   "source": [
    "# Regularization: Modifying the Cost Function\n",
    "\n",
    "**Goal:** Keep the parameters ($W_1, W_2, \\dots, W_N$) small to prevent the model from becoming overly complex and overfitting the training data.\n",
    "\n",
    "**Example Recap:**  \n",
    "- **Quadratic Fit:** A quadratic function can provide a good fit for the data.\n",
    "- **High-Order Polynomial Fit:** A high-order polynomial may overfit the data by being too wiggly.\n",
    "\n",
    "**Key Idea:** If you penalize large values for specific parameters (e.g., $W_3$ and $W_4$), they are forced to be close to zero, effectively reducing the contribution of higher-order (or less important) features.\n",
    "\n",
    "---\n",
    "\n",
    "## Modified Cost Function\n",
    "\n",
    "**Standard Linear Regression Cost Function:**\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2 $$\n",
    "\n",
    "**Modified Cost Function:**\n",
    "\n",
    "$$ J_{\\text{reg}}(\\theta) = \\frac{1}{2m} \\left[ \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2 \\right] $$\n",
    "  \n",
    "**Components:**\n",
    "- **Mean Squared Error Term:** Encourages the model to fit the training data well.\n",
    "- **Regularization Term:** $\\lambda \\sum_{j=1}^{n} \\theta_j^2$ penalizes large parameter values, effectively shrinking them.\n",
    "  \n",
    "**Notes on Conventions:**\n",
    "- The term is often scaled by $\\frac{\\lambda}{2m}$ so both components are on a similar scale.\n",
    "- By convention, **do not regularize the bias term** ($b$ or $\\theta_0$), though some implementations might include it; the difference is usually minimal.\n",
    "\n",
    "---\n",
    "\n",
    "## Trade-Off Controlled by $\\lambda$\n",
    "\n",
    "- **$\\lambda = 0$:**\n",
    "  - **Effect:** No regularization applied.\n",
    "  - **Result:** Model may overfit, especially if it is overly complex.\n",
    "  \n",
    "- **$\\lambda$ is Very Large (e.g., $10^{10}$):**\n",
    "  - **Effect:** Heavy penalty on parameter sizes forces all $\\theta_j$ (for $j \\geq 1$) to be near 0.\n",
    "  - **Result:** Model becomes overly simple (e.g., a horizontal line) and underfits the data.\n",
    "  \n",
    "- **Choosing $\\lambda$:**\n",
    "  - **Balance is Key:** A moderate value of $\\lambda$ ensures the model fits the data well while keeping parameters small enough to avoid overfitting.\n",
    "  - **Model Selection:** Later in the course, techniques for choosing the optimal $\\lambda$ will be discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab0115-2459-472e-98c2-85532fd90b60",
   "metadata": {},
   "source": [
    "# Gradient Descent with Regularized Linear Regression\n",
    "\n",
    "The goal is to minimize a modified cost function that includes both the usual squared error term and an additional term to penalize large parameter values, thereby reducing overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Regularized Cost Function\n",
    "\n",
    "The **unregularized cost function** for linear regression is:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "The **modified (regularized) cost function** becomes:\n",
    "\n",
    "$$\n",
    "J_{\\text{reg}}(\\theta) = \\frac{1}{2m} \\left[ \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2 \\right]\n",
    "$$\n",
    "\n",
    "**Components:**\n",
    "- **Error Term:** Measures the squared difference between predictions and actual values.\n",
    "- **Regularization Term:** $\\lambda \\sum_{j=1}^{n} \\theta_j^2$ penalizes large weights to keep the model simpler.\n",
    "\n",
    "**Conventions:**\n",
    "- We scale by $\\frac{1}{2m}$ for both terms to keep them on a similar scale.\n",
    "- **Bias term ($b$ or $\\theta_0$)** is not regularized.\n",
    "\n",
    "---\n",
    "\n",
    "## Gradient Descent Updates\n",
    "\n",
    "For **standard gradient descent**:\n",
    "\n",
    "**For each weight $w_j$ ($j = 1, 2, \\dots, n$):**\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\cdot \\frac{\\partial J}{\\partial w_j}\n",
    "$$\n",
    "\n",
    "**For bias $b$:**\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\cdot \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "**With regularization**, the derivative for $w_j$ changes while $b$ remains the same.\n",
    "\n",
    "**New Partial Derivative for $w_j$:**\n",
    "\n",
    "$$\n",
    "  \\frac{\\partial J_{\\text{reg}}}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)x_j^{(i)} + \\frac{\\lambda}{m}w_j\n",
    "$$\n",
    "\n",
    "**Update Rule for $w_j$:**\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\left[ \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)x_j^{(i)} + \\frac{\\lambda}{m}w_j \\right]\n",
    "$$\n",
    "\n",
    "**Update Rule for $b$:**\n",
    "$$\n",
    "b := b - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "> **Note:** The bias term $b$ is not regularized.\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition Behind the Update\n",
    "\n",
    "The update rule for $w_j$ can be rearranged as follows:\n",
    "\n",
    "$$\n",
    "w_j := \\left(1 - \\alpha \\frac{\\lambda}{m}\\right) w_j - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)x_j^{(i)}\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "- **Shrinkage Factor:** The term $\\left(1 - \\alpha \\frac{\\lambda}{m}\\right)$ multiplies $w_j$ at every iteration, gradually shrinking it.  \n",
    "\n",
    "**Example:** With $\\alpha = 0.01$, $\\lambda = 1$, and $m = 50$, we get:\n",
    "\n",
    "$$\n",
    "1 - \\alpha \\frac{\\lambda}{m} = 1 - \\frac{0.01}{50} = 0.9998\n",
    "$$\n",
    "\n",
    "Thus, on each iteration, $w_j$ is scaled by approximately 0.9998.\n",
    "\n",
    "**Usual Gradient Descent Component:** The remaining term is the standard gradient descent update for unregularized linear regression.\n",
    "\n",
    "**Effect:** Regularization slowly reduces the magnitude of the weights, helping to control model complexity and prevent overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Derivative Calculation Overview\n",
    "\n",
    "Here's a brief look at how the derivative with respect to $w_j$ is derived:\n",
    "1. **Start with the cost function:**\n",
    "\n",
    "$$\n",
    "J_{\\text{reg}}(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2\n",
    "$$\n",
    "\n",
    "2. **Differentiate with respect to $w_j$:**\n",
    "\n",
    "For the error term, using the chain rule, you obtain:\n",
    "$$\n",
    "\\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)x_j^{(i)}\n",
    "$$\n",
    "\n",
    "For the regularization term:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_j} \\left( \\frac{\\lambda}{2m} w_j^2 \\right) = \\frac{\\lambda}{m} w_j\n",
    "$$\n",
    "\n",
    "3. **Combine the two derivatives to yield:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{\\text{reg}}}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)x_j^{(i)} + \\frac{\\lambda}{m} w_j\n",
    "$$\n",
    "\n",
    "> **Note:** The bias term $b$ is unaffected by regularization.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Summary\n",
    "\n",
    "- **Objective:** Modify gradient descent to work with a regularized cost function that prevents overfitting by keeping weights small.\n",
    "- **Key Changes:**\n",
    "  - **Cost Function:** Augmented with a regularization term: $\\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2$.\n",
    "  - **Gradient Update for Weights:** Incorporates an extra term $\\frac{\\lambda}{m}w_j$, leading to a multiplicative shrinkage factor.\n",
    "  - **Bias Term:** Remains unchanged (not regularized).\n",
    "\n",
    "- **Impact on Learning:**\n",
    "  - Regularization helps control model complexity.\n",
    "  - Choosing appropriate $\\lambda$ balances the trade-off between fitting the data and keeping the model simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738069ed-aab7-4004-845c-085d941b9075",
   "metadata": {},
   "source": [
    "# Regularized Logistic Regression\n",
    "\n",
    "How to implement regularized logistic regression using gradient descent? \n",
    "\n",
    "The approach is very similar to regularized linear regression, with the main difference being the use of the logistic (sigmoid) function for predictions.\n",
    "\n",
    "- **Problem:** Logistic regression with many features (especially high-order polynomial features) can overfit the training data, resulting in an overly complex decision boundary.\n",
    "- **Solution:** Apply regularization by adding a penalty term to the cost function to keep the parameter values small. This helps to generalize better to new data.\n",
    "\n",
    "---\n",
    "\n",
    "## Cost Function with Regularization\n",
    "\n",
    "- **Unregularized Cost Function:**\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log f(z^{(i)}) + (1-y^{(i)}) \\log (1 - f(z^{(i)})) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $f(z) = \\frac{1}{1 + e^{-z}}$ (the sigmoid function)\n",
    "- $z$ is typically a high-order polynomial in the features\n",
    "\n",
    "**Modified Cost Function:**\n",
    "\n",
    "$$\n",
    "J_{\\text{reg}}(\\theta) = J(\\theta) + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2\n",
    "$$\n",
    "\n",
    "**Explanation:**\n",
    "- The first part is the usual logistic regression cost.\n",
    "- The second part is the regularization term:\n",
    "  - $\\lambda$ is the regularization parameter controlling the strength of the penalty.\n",
    "  - The sum runs over all feature weights ($w_1, w_2, \\dots, w_n$), excluding the bias term $b$.\n",
    "\n",
    "**Effect:** Penalizes large weights, thereby simplifying the decision boundary and reducing overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Gradient Descent Updates for Regularized Logistic Regression\n",
    "\n",
    "**For each weight $w_j$ ($j = 1, 2, \\dots, n$):**\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\left[ \\frac{1}{m} \\sum_{i=1}^{m} \\left( f(z^{(i)}) - y^{(i)} \\right)x_j^{(i)} + \\frac{\\lambda}{m}w_j \\right]\n",
    "$$\n",
    "\n",
    "- The term $\\frac{\\lambda}{m}w_j$ is the additional derivative coming from the regularization term.\n",
    "- This update is similar to that of regularized linear regression, except that:\n",
    "    - $f(z)$ now represents the logistic function.\n",
    "    - The standard gradient descent term for logistic regression is used.\n",
    "\n",
    "**For the bias term $b$:**\n",
    "$$\n",
    "b := b - \\alpha \\left[ \\frac{1}{m} \\sum_{i=1}^{m} \\left( f(z^{(i)}) - y^{(i)} \\right) \\right]\n",
    "$$\n",
    "\n",
    "> **Note:** The bias term is **not regularized.**\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- **Similarity to Linear Regression:** The gradient update equations are nearly identical to those for regularized linear regression, with the only difference being the hypothesis function (sigmoid for logistic regression vs. linear for regression).\n",
    "- **Regularization Effect:** Regularization shrinks the weights by adding a term proportional to the weight value itself, thereby reducing overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Implementation\n",
    "\n",
    "**Implementation Tips:**\n",
    "- Ensure that the updates for all parameters ($w_j$ and $b$) are done **simultaneously.**\n",
    "- Choose an appropriate value for $\\lambda$:\n",
    "    - **Too low ($\\lambda = 0$):** No regularization; the model may overfit.\n",
    "    - **Too high (e.g., $\\lambda \\gg 1$):** Excessive shrinkage, leading to underfitting.\n",
    "- In the practice lab, you will have the opportunity to experiment with different $\\lambda$ values to observe their effect on the decision boundary.\n",
    "\n",
    "> **[!TIP] Real-World Application**\n",
    ">  \n",
    "> Many companies leverage regularized logistic regression to ensure that their models generalize well, which is critical for applications such as fraud detection, medical diagnosis, and more. Your ability to implement and tune these models can lead to significant real-world value.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae38edb-14df-4b63-a05f-61878313a100",
   "metadata": {},
   "source": [
    "# Adding Regularization To Logistic Regression Model\n",
    "\n",
    "Now let's add regularization to our algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f69e56e1-90a6-45ba-b99a-131d7b61461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "422b9a0e-d205-453d-b913-0bbecd1e0fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "y = np.array([0, 0, 0, 1, 1, 1]).reshape(-1, 1) # Ensure it is a 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "0415cf00-4482-4b43-99fd-961ee920cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid of a given value.\n",
    "    \"\"\"\n",
    "\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "8a3a02a7-8a73-4daa-8fa6-f308ddbad87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    \"\"\"\n",
    "    Predicts the target value for logistic regression using given input and parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    return sigmoid(X @ theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "20c505eb-8e67-41e9-a247-554ba32bba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta, lam = 0.1):\n",
    "    \"\"\"\n",
    "    Computes the cost for logistic regression model.\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    predictions = predict(X, theta)\n",
    "    cost = (-1/m) * (np.dot(y.T, np.log(predictions)) + np.dot((1 - y.T), np.log(1 - predictions)))\n",
    "    reg_cost = (lam / (2*m)) * np.sum(theta[1:] ** 2)\n",
    "\n",
    "    cost = cost + reg_cost\n",
    "    return cost.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "cab33144-e294-4b57-ae1d-e0bd498e41ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38353345307218406\n"
     ]
    }
   ],
   "source": [
    "theta_tmp = np.array([-3, 1, 1]).reshape(-1, 1)\n",
    "# theta_tmp = np.array([-3, 1, 1])[:, np.newaxis (Or None)]\n",
    "print(compute_cost(X, y, theta_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "620df68d-f3f0-4d14-af38-0a93a6145051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, epochs = 10_000, alpha=0.01, lam=0.1, epsilon=0.0001):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to find optimal model parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    m, n = X.shape\n",
    "    theta = np.random.rand(n, 1)\n",
    "    cost_history = []\n",
    "    previous_cost = float('inf')\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        # Compute predictions and error\n",
    "        predictions = predict(X, theta)\n",
    "        errors = predictions - y\n",
    "\n",
    "        # Compute gradient\n",
    "        grad = (X.T @ errors) / m\n",
    "        reg_grad = (lam / m) * np.r_[np.zeros((1, 1)), theta[1:]]\n",
    "        grad = grad + reg_grad\n",
    "        theta -= alpha * grad\n",
    "\n",
    "        # Compute cost\n",
    "        cost = compute_cost(X, y, theta, lam)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Check for early exit\n",
    "        if abs(previous_cost - cost) < epsilon:\n",
    "            print(f\"Convergence reached at epoch {i}\")\n",
    "            break\n",
    "\n",
    "        previous_cost = cost\n",
    "\n",
    "        \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "cfe0b412-e824-4112-a4df-b803e300c372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached at epoch 8395\n",
      "Optimal parameters are: [-4.68663494  1.83289516  1.61060147]\n",
      "Final Cost: 0.26931920403781223\n",
      "Model Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "theta, cost_history = gradient_descent(X, y, epsilon=0.00001)\n",
    "print(\"Optimal parameters are:\", theta.flatten())\n",
    "print(\"Final Cost:\", cost_history[-1])\n",
    "predictions = (predict(X, theta) >= 0.5).astype(int)\n",
    "print(\"Model Accuracy:\", np.mean(predictions == y) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "bb1523bb-0802-4534-9f84-180e215ed05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1610bcb90>"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(cost_history, c=\"red\", linestyle=\"--\", label=\"Cost\")\n",
    "plt.ylabel(\"Cost\", c=\"red\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5e46dc-b9c2-4076-b34c-39a24b02a2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6559bed-f4b4-4fbf-8d70-3ffcdd7e322a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
