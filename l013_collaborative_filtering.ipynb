{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ddbfd79-2c99-4cbb-ada4-5a708be23e4d",
   "metadata": {},
   "source": [
    "# Recommender Systems & Collaborative Filtering Notes\n",
    "\n",
    "Recommender systems power many popular online platforms—from shopping sites like Amazon to streaming services like Netflix—by suggesting products, movies, or articles based on users' past behavior. Despite their academic treatment being relatively modest, their commercial impact is enormous. The core idea is to use available rating data to predict what a user might like next, thus driving engagement and sales. In these notes, we break down the concepts with more detailed explanations, examples, and step-by-step walkthroughs.\n",
    "\n",
    "---\n",
    "\n",
    "## Basic Framework and Notation\n",
    "\n",
    "Imagine a movie streaming site where users rate movies. To build a recommender system, we first need to define our basic components:\n",
    "\n",
    "- **Users and Items**:  \n",
    "  - $n_u$: Number of users (e.g., Alice, Bob, Carol, Dave).  \n",
    "  - $n_m$: Number of movies (e.g., *Love at Last*, *Romance Forever*, etc.).  \n",
    "\n",
    "  *Example*: If there are 4 users and 5 movies, then $n_u = 4$ and $n_m = 5$.\n",
    "\n",
    "- **Ratings and Observations**:  \n",
    "  - $y(i,j)$: Rating given by user $j$ for movie $i$.  \n",
    "  - $r(i,j)$: An indicator variable; it equals 1 if user $j$ has rated movie $i$, and 0 otherwise.\n",
    "\n",
    "  *Step-by-Step*:\n",
    "  1. If Alice (user 1) rates *Love at Last* (movie 1) 5 stars, then $y(1,1) = 5$ and $r(1,1) = 1$.\n",
    "  2. If Bob (user 2) hasn’t rated *Romance Forever* (movie 2), then $r(2,2) = 0$.\n",
    "\n",
    "- **Features of Items**:  \n",
    "  When available, movies might have inherent features. For example:\n",
    "  - Feature $X_1$: Degree of romance.\n",
    "  - Feature $X_2$: Level of action.\n",
    "\n",
    "  *Example*:  \n",
    "  - *Love at Last* might have features $X(1) = [0.9, 0]$ (high romance, no action).\n",
    "  - *Nonstop Car Chases* might have features $X(4) = [0.1, 1.0]$ (low romance, high action).\n",
    "\n",
    "These definitions provide the building blocks for our prediction models.\n",
    "\n",
    "---\n",
    "\n",
    "## Prediction Using Linear Models\n",
    "\n",
    "### With Known Item Features\n",
    "\n",
    "When the features for each movie are provided, the rating prediction for user $j$ and movie $i$ is modeled with a linear function:\n",
    "\n",
    "$$\n",
    "\\hat{y}_{ij} = w^{(j)} \\cdot X(i) + b^{(j)}\n",
    "$$\n",
    "\n",
    "- **Parameters**:  \n",
    "  - $w^{(j)}$: Weight vector capturing user $j$'s preference for each feature.\n",
    "  - $b^{(j)}$: A bias term for user $j$ to account for overall rating tendencies.\n",
    "\n",
    "*Step-by-Step Example*:\n",
    "1. Assume for Alice ($j = 1$), we choose:\n",
    "   - $w^{(1)} = [5, 0]$ meaning she highly values the romance feature and ignores action.\n",
    "   - $b^{(1)} = 0$ (no additional bias).\n",
    "2. For a movie with features $X(i) = [0.99, 0]$:\n",
    "   $$\n",
    "   \\hat{y}_{i1} = 5 \\times 0.99 + 0 = 4.95\n",
    "   $$\n",
    "   This suggests that Alice would give nearly 5 stars for a very romantic movie.\n",
    "\n",
    "### Cost Function for Rating Prediction\n",
    "\n",
    "To learn the parameters $w^{(j)}$ and $b^{(j)}$ from data, we minimize the mean squared error (MSE) computed only over the movies that user $j$ has rated:\n",
    "\n",
    "$$\n",
    "J(w^{(j)}, b^{(j)}) = \\frac{1}{2m(j)} \\sum_{i: r(i,j)=1} \\left( w^{(j)} \\cdot X(i) + b^{(j)} - y(i,j) \\right)^2 + \\frac{\\lambda}{2m(j)} \\sum_{k=1}^{n} \\left( w_k^{(j)} \\right)^2\n",
    "$$\n",
    "\n",
    "- **Details**:  \n",
    "  - $m(j)$: Number of movies rated by user $j$.  \n",
    "  - $\\lambda$: Regularization parameter that helps prevent overfitting by penalizing large weights.\n",
    "  \n",
    "*Step-by-Step*:\n",
    "1. Calculate the error for each rated movie by subtracting the actual rating $y(i,j)$ from the predicted rating.\n",
    "2. Square each error, sum them, and divide by $2m(j)$.\n",
    "3. Add the regularization term, which is the sum of the squares of the weights (scaled by $\\lambda$ and $m(j)$).\n",
    "\n",
    "*Note*: During optimization, dividing by $m(j)$ does not affect the location of the minimum, so it can sometimes be dropped to simplify calculations.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Item Features: Collaborative Filtering\n",
    "\n",
    "When item features are not explicitly given, collaborative filtering techniques allow us to learn these features directly from the rating data.\n",
    "\n",
    "### The Idea Behind Collaborative Filtering\n",
    "\n",
    "Rather than relying on pre-defined item characteristics, we let the data “speak for itself” by inferring both:\n",
    "- The unknown feature vector $X(i)$ for each movie.\n",
    "- The user-specific parameters $w^{(j)}$ and $b^{(j)}$.\n",
    "\n",
    "*Analogy*:  \n",
    "Think of each movie as a “mystery box” with hidden traits. By observing how different friends (users) react to these mystery boxes (movies), you can infer the underlying characteristics (features) of the boxes.\n",
    "\n",
    "### Formulating the Cost Function for Item Features\n",
    "\n",
    "For a given movie $i$, the cost function to learn its feature vector $X(i)$ is:\n",
    "\n",
    "$$\n",
    "J(X(i)) = \\frac{1}{2} \\sum_{j: r(i,j)=1} \\left( w^{(j)} \\cdot X(i) - y(i,j) \\right)^2 + \\frac{\\lambda}{2} \\sum_{k=1}^{n} \\left( X_k(i) \\right)^2\n",
    "$$\n",
    "\n",
    "- **Explanation**:\n",
    "  - We sum the squared differences between the predicted ratings $w^{(j)} \\cdot X(i)$ and the actual ratings $y(i,j)$ for all users who rated movie $i$.\n",
    "  - The regularization term $\\frac{\\lambda}{2} \\sum_{k=1}^{n} \\left( X_k(i) \\right)^2$ prevents the inferred features from growing too large.\n",
    "\n",
    "*Step-by-Step*:\n",
    "1. For each movie $i$, consider only the users $j$ with $r(i,j) = 1$.\n",
    "2. Compute the prediction $w^{(j)} \\cdot X(i)$ and compare it to the actual rating.\n",
    "3. Sum these errors and add the regularization penalty.\n",
    "\n",
    "### Joint Learning: Users and Items\n",
    "\n",
    "We combine the cost functions for both user-specific parameters and item features into a unified cost function:\n",
    "\n",
    "$$\n",
    "J(w, b, X) = \\frac{1}{2} \\sum_{(i,j): r(i,j)=1} \\left( w^{(j)} \\cdot X(i) + b^{(j)} - y(i,j) \\right)^2 + \\text{regularization terms}\n",
    "$$\n",
    "\n",
    "- **Explanation**:\n",
    "  - This formulation accounts for all observed ratings by summing over every pair $(i,j)$ for which $r(i,j)=1$.\n",
    "  - Regularization is applied to both the user parameters and item features.\n",
    "  - The joint optimization problem can be solved using methods like gradient descent, where you update $w^{(j)}$, $b^{(j)}$, and $X(i)$ iteratively.\n",
    "\n",
    "*Step-by-Step Example*:\n",
    "1. **Initialization**: Start with random or small initial values for $w^{(j)}$, $b^{(j)}$, and $X(i)$ for all users and movies.\n",
    "2. **Prediction**: For each rating $y(i,j)$, compute the predicted rating.\n",
    "3. **Error Calculation**: Calculate the difference between the predicted and actual rating.\n",
    "4. **Update Parameters**: Adjust $w^{(j)}$, $b^{(j)}$, and $X(i)$ based on the gradient of the cost function.\n",
    "5. **Iteration**: Repeat until the cost function converges to a minimum.\n",
    "\n",
    "---\n",
    "\n",
    "## Extension to Binary Labels\n",
    "\n",
    "In many real-world applications, explicit ratings (like 1–5 stars) are replaced by binary signals (e.g., a “like” or a click).\n",
    "\n",
    "### Using a Logistic Regression Approach\n",
    "\n",
    "When ratings are binary, $y(i,j)$ is either 0 (non-engagement) or 1 (engagement). The model then predicts the probability that user $j$ likes movie $i$:\n",
    "\n",
    "$$\n",
    "P(y(i,j)=1) = g\\left( w^{(j)} \\cdot X(i) + b^{(j)} \\right)\n",
    "$$\n",
    "\n",
    "where the logistic (sigmoid) function is defined as:\n",
    "\n",
    "$$\n",
    "g(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "- **Explanation**:\n",
    "  - The logistic function squashes any real-valued number into the range $(0,1)$, making it suitable for probability estimation.\n",
    "  - For instance, if $w^{(j)} \\cdot X(i) + b^{(j)} = 2$, then\n",
    "    $$\n",
    "    g(2) = \\frac{1}{1+e^{-2}} \\approx 0.88,\n",
    "    $$\n",
    "    suggesting an 88% chance that the user likes the movie.\n",
    "\n",
    "### Modified Cost Function\n",
    "\n",
    "When using binary labels, we adapt the cost function to the cross-entropy loss, which is common in classification tasks:\n",
    "\n",
    "$$\n",
    "J(w, b, X) = - \\sum_{(i,j): r(i,j)=1} \\left[ y(i,j) \\log \\left(g\\left( w^{(j)} \\cdot X(i) + b^{(j)} \\right)\\right) + (1 - y(i,j)) \\log \\left(1 - g\\left( w^{(j)} \\cdot X(i) + b^{(j)} \\right)\\right) \\right] + \\text{regularization terms}\n",
    "$$\n",
    "\n",
    "*Step-by-Step*:\n",
    "1. **For each rating**:\n",
    "   - If $y(i,j)=1$: Add the term $-\\log(g(\\cdot))$.\n",
    "   - If $y(i,j)=0$: Add the term $-\\log(1 - g(\\cdot))$.\n",
    "2. **Sum over all rated pairs**.\n",
    "3. **Add regularization** to keep the parameters small and avoid overfitting.\n",
    "\n",
    "This approach shifts the problem from predicting an exact rating to estimating the probability of engagement, which is often more useful in scenarios like ad-click prediction or binary purchase signals.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Implementation & Insights\n",
    "\n",
    "- **Gradient Descent Updates**:  \n",
    "  Both the user parameters ($w^{(j)}, b^{(j)}$) and item features $X(i)$ are updated iteratively using gradient descent.  \n",
    "  *Step-by-Step*:\n",
    "  1. Compute the gradient (partial derivative) of the cost function with respect to each parameter.\n",
    "  2. Update each parameter by moving it slightly in the direction that reduces the cost.\n",
    "  3. Repeat until the cost function changes very little between iterations (convergence).\n",
    "\n",
    "- **Collaboration Across Users**:  \n",
    "  The term “collaborative filtering” stems from the idea that the system leverages the collective ratings of multiple users to infer item features.  \n",
    "  *Analogy*:  \n",
    "  Imagine you’re trying to decide which new restaurant to try based on reviews from many friends. Even if you have never visited that restaurant, you can predict whether you might enjoy it by understanding what your friends liked or disliked.\n",
    "\n",
    "- **Regularization**:  \n",
    "  Adding regularization (via the $\\lambda$ term) is crucial to prevent overfitting, especially when the number of features is large compared to the number of available ratings.  \n",
    "  *Explanation*:  \n",
    "  Regularization discourages the model from fitting noise in the training data by penalizing overly complex models (i.e., very large weights).\n",
    "\n",
    "- **Real-World Example Analogy**:  \n",
    "  Think of each movie as a “mystery box” with hidden traits. By observing multiple friends’ (users’) reactions to the box, you can infer what characteristics (like romance or action) the box might have. Later, when a new friend comes along with similar tastes, you can predict their reaction based on the shared inferred traits.\n",
    "\n",
    "- **Binary Feedback Use Cases**:  \n",
    "  In many applications like online shopping or ad-click predictions, the feedback is binary (purchase/no purchase, click/no click) rather than a detailed rating.  \n",
    "  *Explanation*:  \n",
    "  - Logistic regression is used to model these cases because it naturally handles binary outcomes by predicting probabilities.\n",
    "  - This adaptation expands the range of scenarios where collaborative filtering can be applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430bd545-d806-4c96-9e11-48b94e630bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
