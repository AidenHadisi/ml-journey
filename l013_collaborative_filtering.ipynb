{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ddbfd79-2c99-4cbb-ada4-5a708be23e4d",
   "metadata": {},
   "source": [
    "# Recommender Systems and Collaborative Filtering\n",
    "\n",
    "Recommender systems power many popular online platforms—from shopping sites like Amazon to streaming services like Netflix—by suggesting products, movies, or articles based on users' past behavior. Despite their academic treatment being relatively modest, their commercial impact is enormous. The core idea is to use available rating data to predict what a user might like next, thus driving engagement and sales.\n",
    "\n",
    "---\n",
    "\n",
    "## Basic Example\n",
    "\n",
    "Imagine you have a set of users and a set of movies. The data is typically organized in a matrix where:\n",
    "- **Rows:** Represent movies (or items).\n",
    "- **Columns:** Represent users.\n",
    "- **Entries:** The rating that a user gave to a movie.\n",
    "\n",
    "For example, suppose we have 4 users (Alice, Bob, Carol, Dave) and 5 movies. We define:\n",
    "- $n_u$: Number of users (e.g., Alice, Bob, Carol, Dave). In our case 4.\n",
    "- $n_m$: Number of movies (e.g., _Love at Last_, _Romance Forever_, etc.). In our case 5.\n",
    "\n",
    "We introduce two functions:\n",
    "**Indicator function:**  An indicator variable; it equals 1 if user $j$ has rated movie $i$, and 0 otherwise.\n",
    "\n",
    "$$\n",
    "r(i, j) = \n",
    "\\begin{cases}\n",
    "1 & \\text{if user } j \\text{ has rated movie } i, \\\\\n",
    "0 & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Rating function:** Rating given by user $j$ for movie $i$.\n",
    "\n",
    "$$y(i, j) = \\text{the rating user } j \\text{ gave movie } i.$$\n",
    "\n",
    "**Example:**  \n",
    "1. If Alice (user 1) rates _Love at Last_ (movie 1) 5 stars, then $y(1,1) = 5$ and $r(1,1) = 1$.\n",
    "2. If Bob (user 2) hasn’t rated _Romance Forever_ (movie 2), then $r(2,2) = 0$.\n",
    "\n",
    "### Prediction Using Linear Models\n",
    "\n",
    "We want to predict how a user might rate a movie they haven't seen yet. To do this, we assume that every movie can be described by a set of features (like how “romantic” or “action-packed” it is). Each user has their own “taste profile” defined by parameters that interact with these features.\n",
    "\n",
    "For user $j$ and movie $i$, the prediction is made using a linear model:\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(i, j)} = \\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{x}^{(i)}$ is the feature vector for movie $i$.\n",
    "- $\\mathbf{w}^{(j)}$ is the weight vector (user preferences) for user $j$.\n",
    "- $b^{(j)}$ is the bias (a constant that can shift the prediction).\n",
    "\n",
    "Suppose movies have two features:\n",
    "- $x_1$: How romantic the movie is.\n",
    "- $x_2$: How action-packed it is.\n",
    "\n",
    "For user Alice, who loves romance:\n",
    "- Let $\\mathbf{w}^{(Alice)} = [5, 0]$ (high weight for romance, zero weight for action).\n",
    "- Let $b^{(Alice)} = 0$.\n",
    "\n",
    "For a movie with features $\\mathbf{x}^{(i)} = [0.99, 0]$ (very romantic, no action), the predicted rating is:\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(i, Alice)} = 5 \\times 0.99 + 0 = 4.95\n",
    "$$\n",
    "\n",
    "This matches our intuition that a highly romantic movie should receive a high rating from Alice.\n",
    "\n",
    "---\n",
    "\n",
    "## The Cost Function\n",
    "\n",
    "To adjust the parameters ($\\mathbf{w}^{(j)}$ and $b^{(j)}$) so that our predictions are close to the actual ratings, we define a cost function. For user $j$, the cost function (using Mean Squared Error) is:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}^{(j)}, b^{(j)}) = \\frac{1}{2\\,m(j)} \\sum_{i: r(i,j)=1} \\Big( \\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)} - y(i,j) \\Big)^2 + \\frac{\\lambda}{2\\,m(j)} \\sum_{k=1}^{n} \\big(w^{(j)}_k\\big)^2\n",
    "$$\n",
    "\n",
    "- **$m(j)$**: Number of movies rated by user $j$.\n",
    "- **$\\lambda$**: Regularization parameter to prevent overfitting.\n",
    "- The first term is the squared error (difference between prediction and true rating) and the second term penalizes large weights.\n",
    "\n",
    "_Step-by-Step_:\n",
    "\n",
    "1. Calculate the error for each rated movie by subtracting the actual rating $y(i,j)$ from the predicted rating.\n",
    "2. Square each error, sum them, and divide by $2m(j)$.\n",
    "3. Add the regularization term, which is the sum of the squares of the weights (scaled by $\\lambda$ and $m(j)$).\n",
    "\n",
    "_Note_: During optimization, dividing by $m(j)$ does not affect the location of the minimum, so it can sometimes be dropped to simplify calculations.\n",
    "\n",
    "\n",
    "### For All Users\n",
    "\n",
    "To learn for every user, sum the cost over all users:\n",
    "\n",
    "$$\n",
    "J_{\\text{total}} = \\sum_{j=1}^{\\nu} J(\\mathbf{w}^{(j)}, b^{(j)})\n",
    "$$\n",
    "\n",
    "An optimization algorithm (like gradient descent) is then used to find the best parameters for all users.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Item Features: Collaborative Filtering\n",
    "\n",
    "Often, you might not have pre-defined features for movies. Collaborative filtering allows us to learn the features $\\mathbf{x}^{(i)}$ directly from the data.\n",
    "\n",
    "Rather than relying on pre-defined item characteristics, we let the data “speak for itself” by inferring both:\n",
    "- The unknown feature vector $X(i)$ for each movie.\n",
    "- The user-specific parameters $w^{(j)}$ and $b^{(j)}$.\n",
    "\n",
    "_Analogy_:  \n",
    "Think of each movie as a “mystery box” with hidden traits. By observing how different friends (users) react to these mystery boxes (movies), you can infer the underlying characteristics (features) of the boxes.\n",
    "\n",
    "### Step-by-Step Example\n",
    "\n",
    "Imagine for movie 1:\n",
    "- Alice and Bob rated it 5, while Carol and Dave rated it 0.\n",
    "- With given user parameters (for example, Alice’s $\\mathbf{w}^{(Alice)} = [5, 0]$, Bob’s similar, and Carol/Dave have weights that produce low scores for a given feature), a natural choice for $\\mathbf{x}^{(1)}$ might be $[1, 0]$.\n",
    "- Then:\n",
    "  - For Alice: $5 \\times 1 = 5$,\n",
    "  - For Carol: if her weight on the first feature is near 0, the prediction will be near 0.\n",
    "  \n",
    "We create a cost function for movie $i$:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{x}^{(i)}) = \\frac{1}{2} \\sum_{j: r(i,j)=1} \\Big( \\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} - y(i,j) \\Big)^2 + \\frac{\\lambda}{2} \\sum_{k=1}^{n} \\big(x^{(i)}_k\\big)^2\n",
    "$$\n",
    "\n",
    "The idea is to adjust $\\mathbf{x}^{(i)}$ (for every movie) so that, for all users who rated it, the predictions are as accurate as possible.\n",
    "\n",
    "_Step-by-Step_:\n",
    "\n",
    "1. For each movie $i$, consider only the users $j$ with $r(i,j) = 1$.\n",
    "2. Compute the prediction $w^{(j)} \\cdot X(i)$ and compare it to the actual rating.\n",
    "3. Sum these errors and add the regularization penalty.\n",
    "\n",
    "---\n",
    "\n",
    "## Joint Learning: Users and Items\n",
    "\n",
    "We combine the cost functions for both user-specific parameters and item features into a unified cost function:\n",
    "\n",
    "$$\n",
    "J(w, b, X) = \\frac{1}{2} \\sum_{(i,j): r(i,j)=1} \\left( w^{(j)} \\cdot X(i) + b^{(j)} - y(i,j) \\right)^2 + \\text{regularization terms}\n",
    "$$\n",
    "\n",
    "- **Explanation**:\n",
    "  - This formulation accounts for all observed ratings by summing over every pair $(i,j)$ for which $r(i,j)=1$.\n",
    "  - Regularization is applied to both the user parameters and item features.\n",
    "  - The joint optimization problem can be solved using methods like gradient descent, where you update $w^{(j)}$, $b^{(j)}$, and $X(i)$ iteratively.\n",
    "\n",
    "_Step-by-Step Example_:\n",
    "\n",
    "1. **Initialization**: Start with random or small initial values for $w^{(j)}$, $b^{(j)}$, and $X(i)$ for all users and movies.\n",
    "2. **Prediction**: For each rating $y(i,j)$, compute the predicted rating.\n",
    "3. **Error Calculation**: Calculate the difference between the predicted and actual rating.\n",
    "4. **Update Parameters**: Adjust $w^{(j)}$, $b^{(j)}$, and $X(i)$ based on the gradient of the cost function.\n",
    "5. **Iteration**: Repeat until the cost function converges to a minimum.\n",
    "\n",
    "---\n",
    "\n",
    "## Binary Labels\n",
    "\n",
    "Many recommender systems do not have star ratings. Instead, they use binary signals like \"liked\" (1) or \"not liked\" (0).\n",
    "\n",
    "For binary outcomes, we predict the probability that user $j$ likes movie $i$ using a logistic function:\n",
    "\n",
    "$$\n",
    "P\\big(y^{(i,j)}=1\\big) = g\\Big(\\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)}\\Big)\n",
    "$$\n",
    "\n",
    "where the logistic function is:\n",
    "\n",
    "$$\n",
    "g(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "For binary labels, the cost function uses **binary cross-entropy loss** instead of mean squared error:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}^{(j)}, b^{(j)}) = -\\frac{1}{m(j)} \\sum_{i: r(i,j)=1} \\left[ y(i,j)\\log\\big(g(z)\\big) + (1-y(i,j))\\log\\big(1-g(z)\\big) \\right] + \\text{regularization}\n",
    "$$\n",
    "\n",
    "This is similar to moving from linear to logistic regression.\n",
    "\n",
    "_Step-by-Step_:\n",
    "1. **For each rating**:\n",
    "   - If $y(i,j)=1$: Add the term $-\\log(g(\\cdot))$.\n",
    "   - If $y(i,j)=0$: Add the term $-\\log(1 - g(\\cdot))$.\n",
    "2. **Sum over all rated pairs**.\n",
    "3. **Add regularization** to keep the parameters small and avoid overfitting.\n",
    "\n",
    "This approach shifts the problem from predicting an exact rating to estimating the probability of engagement, which is often more useful in scenarios like ad-click prediction or binary purchase signals.\n",
    "\n",
    "---\n",
    "\n",
    "## Mean Normalization: Handling the Cold Start Problem\n",
    "\n",
    "When a new user (or an item) has very few ratings, predictions can be very poor. Mean normalization adjusts the ratings so that every movie’s ratings are centered around zero.\n",
    "\n",
    "1. **Compute the Mean Rating:**  \n",
    "   For each movie $i$, calculate:\n",
    "\n",
    "$$\\mu_i = \\frac{1}{\\text{number of ratings for } i} \\sum_{j: r(i,j)=1} y(i,j)$$\n",
    "\n",
    "   For example, if movie 1 has ratings 5, 5, 0, 0 from four users, then $\\mu_1 = 2.5$.\n",
    "\n",
    "3. **Normalize the Ratings:**  \n",
    "   Replace each rating with:\n",
    "\n",
    "$$y_{\\text{norm}}(i,j) = y(i,j) - \\mu_i$$\n",
    "\n",
    "   This centers the ratings so that on average, they are zero.\n",
    "\n",
    "5. **Make Predictions and Re-adjust:**  \n",
    "   Predict using the model:\n",
    "\n",
    "$$\\hat{y}_{\\text{norm}}^{(i,j)} = \\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)}$$\n",
    "\n",
    "   Then add the mean back to get the final prediction:\n",
    "\n",
    "$$\\hat{y}^{(i,j)} = \\hat{y}_{\\text{norm}}^{(i,j)} + \\mu_i$$\n",
    "\n",
    "### Why It Helps\n",
    "\n",
    "For a new user with no ratings, the parameters might default to zeros (because of regularization), leading to a prediction of 0 for every movie. By adding the movie’s mean rating back, the system instead predicts the average rating—much more reasonable than zero.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementing Collaborative Filtering in TensorFlow\n",
    "\n",
    "TensorFlow is not only for deep neural networks; it can also be used to implement collaborative filtering by automatically computing gradients.\n",
    "\n",
    "### Automatic Differentiation (Auto Diff)\n",
    "\n",
    "Instead of manually calculating derivatives for the cost function, TensorFlow’s **gradient tape** records operations and computes gradients automatically.\n",
    "  \n",
    "**Example Code Outline:**\n",
    "\n",
    "```python\n",
    "  import tensorflow as tf\n",
    "  \n",
    "  # Initialize variable w (for demonstration)\n",
    "  w = tf.Variable(3.0)\n",
    "  x = 1.0  # Example input\n",
    "  y = 1.0  # True value\n",
    "  alpha = 0.01  # Learning rate\n",
    "  \n",
    "  # Use gradient tape to compute derivative automatically\n",
    "  for iter in range(30):\n",
    "      with tf.GradientTape() as tape:\n",
    "          f = w * x\n",
    "          J = (f - y) ** 2\n",
    "      dJdw = tape.gradient(J, w)\n",
    "      w.assign_sub(alpha * dJdw)  # Update parameter w\n",
    "  print(w.numpy())  # Expected to approach 1.0\n",
    "```\n",
    "\n",
    "This example uses a simple quadratic cost function to illustrate gradient descent.\n",
    "\n",
    "### Applying to Collaborative Filtering\n",
    "\n",
    "In collaborative filtering:\n",
    "- **Parameters:** Include both user parameters ($\\mathbf{w}^{(j)}$, $b^{(j)}$) and item features ($\\mathbf{x}^{(i)}$).\n",
    "- **Cost Function:** Is a combination of prediction error (either squared error or cross-entropy) and regularization.\n",
    "- **Optimizer:** You can use basic gradient descent or more advanced ones like Adam, which TensorFlow supports.\n",
    "\n",
    "The key is writing the cost function in TensorFlow. The optimizer and gradient tape then handle computing the necessary updates for all parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## Finding Similar (Related) Items\n",
    "\n",
    "Once the model learns item features $\\mathbf{x}^{(i)}$, these vectors can be used to find similar items.\n",
    "\n",
    "Use the squared Euclidean distance between two feature vectors:\n",
    "\n",
    "$$\n",
    "d(\\mathbf{x}^{(i)}, \\mathbf{x}^{(k)}) = \\sum_{l=1}^{n} \\Big( x^{(i)}_l - x^{(k)}_l \\Big)^2\n",
    "$$\n",
    "\n",
    "A smaller distance indicates that the movies are more similar.\n",
    "\n",
    "### Practical Example\n",
    "\n",
    "- **Step 1:** For a given movie (say, \"Romantic Movie A\") with feature vector $\\mathbf{x}^{(A)}$, compute the distance to every other movie.\n",
    "- **Step 2:** Select the movies with the smallest distances.\n",
    "- **Analogy:**  \n",
    "  Imagine each movie as a point in a multi-dimensional space (each dimension representing an abstract quality like romance or action). Movies that lie close together in this space are likely to be similar in style or content.\n",
    "\n",
    "---\n",
    "\n",
    "## Limitations and Extensions\n",
    "\n",
    "### Cold Start Problem\n",
    "- **New Items:** With few ratings, it is hard to learn good features.\n",
    "- **New Users:** With little interaction data, the system might initially predict the average rating.\n",
    "- **Mitigation:**  \n",
    "  Techniques such as mean normalization help, and sometimes additional data (e.g., demographic or content information) is combined with collaborative filtering.\n",
    "\n",
    "### Integrating Side Information\n",
    "Collaborative filtering by itself does not naturally include additional details (like a movie’s genre, director, or user demographics). Extensions like **content-based filtering** or hybrid models combine both collaborative and side information to improve recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430bd545-d806-4c96-9e11-48b94e630bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
