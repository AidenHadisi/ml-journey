{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99d6f18a-466a-47a9-aaa4-1898d506c968",
   "metadata": {},
   "source": [
    "# Multiclass Classification\n",
    "\n",
    "In multiclass classification, the target variable $y$ can take on more than two discrete values (e.g., digits 0â€“9, various diseases, or different defect types).  \n",
    "\n",
    "Imagine plotting data points on a plane where each cluster represents a different class. Instead of just two clusters (binary), you may have several clusters, each with a unique marker (e.g., circles, triangles, squares).\n",
    "\n",
    "**Binary vs. Multiclass:**  \n",
    "- **Binary:** $y \\in \\{0, 1\\}$\n",
    "- **Multiclass:** $y \\in \\{1, 2, \\dots, n\\}$\n",
    "\n",
    "**Examples:**  \n",
    "- **Handwritten Digit Recognition:** Recognize digits $0$ through $9$ (10 classes).\n",
    "- **Medical Diagnosis:** Identify one disease among several possibilities.\n",
    "- **Quality Inspection:** Classify parts with different types of defects (e.g., scratch, discoloration, chip).\n",
    "\n",
    "---\n",
    "\n",
    "## Softmax Regression\n",
    "\n",
    "Softmax regression extends logistic regression to handle multiclass problems.\n",
    "\n",
    "\n",
    "**Logistic Regression (Binary):**  \n",
    "\n",
    "Compute:\n",
    "  \n",
    "$$z = \\mathbf{w} \\cdot \\mathbf{x} + b$$\n",
    "  \n",
    "Then apply the sigmoid function:\n",
    "  \n",
    "$$a = g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "  \n",
    "- where $a$ is the probability that $y = 1$. The probability for $y = 0$ is $1 - a$.\n",
    "\n",
    "\n",
    "**Softmax Regression (Multiclass):**  \n",
    "\n",
    "For each class $j$ (with $j = 1, 2, \\dots, n$), compute:\n",
    "  \n",
    "$$z_j = \\mathbf{w}_j \\cdot \\mathbf{x} + b_j.$$\n",
    "  \n",
    "Then, calculate the probabilities using the softmax function:\n",
    "  \n",
    "$$a_j = \\frac{e^{z_j}}{\\sum_{k=1}^{n} e^{z_k}}$$\n",
    "  \n",
    "The probabilities satisfy:\n",
    "\n",
    "$$\\sum_{j=1}^{n} a_j = 1$$\n",
    "\n",
    "### Example with 4 Classes\n",
    "\n",
    "For $n = 4$, compute:\n",
    "\n",
    "$$\n",
    "z_1 = \\mathbf{w}_1 \\cdot \\mathbf{x} + b_1,\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_2 = \\mathbf{w}_2 \\cdot \\mathbf{x} + b_2,\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_3 = \\mathbf{w}_3 \\cdot \\mathbf{x} + b_3,\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_4 = \\mathbf{w}_4 \\cdot \\mathbf{x} + b_4.\n",
    "$$\n",
    "\n",
    "Then, for each class:\n",
    "\n",
    "$$\n",
    "a_j = \\frac{e^{z_j}}{e^{z_1} + e^{z_2} + e^{z_3} + e^{z_4}} \\quad \\text{for } j=1,2,3,4.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Cost Function for Softmax Regression\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "**For Logistic Regression (Binary):**\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\Big[ y \\log(a) + (1-y) \\log(1-a) \\Big].\n",
    "$$\n",
    "\n",
    "**For Softmax Regression (Multiclass):**  \n",
    "\n",
    "If the true label is $y = j$, the loss for that example is:\n",
    "\n",
    "$$\n",
    "L = -\\log(a_j).\n",
    "$$\n",
    "\n",
    "- A high $a_j$ (close to 1) leads to a low loss.\n",
    "- A low $a_j$ results in a high loss.\n",
    "- The overall cost is the average loss over all training examples.\n",
    "\n",
    "**Cost**\n",
    "\n",
    "Note that only the line that corresponds to the target contributes to the loss, other lines are zero. To write the cost equation we need an 'indicator function' that will be 1 when the index matches the target and zero otherwise. \n",
    "\n",
    "$$\\mathbf{1}\\{y == n\\} = =\\begin{cases}\n",
    "1, & \\text{if $y==n$}.\\\\\n",
    "0, & \\text{otherwise}.\n",
    "\\end{cases}$$\n",
    "  \n",
    "Therefore the cost function can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J(\\mathbf{w},b) = -\\frac{1}{m} \\left[ \\sum_{i=1}^{m} \\sum_{j=1}^{N}  1\\left\\{y^{(i)} == j\\right\\} \\log \\frac{e^{z^{(i)}_j}}{\\sum_{k=1}^N e^{z^{(i)}_k} }\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Neural Networks with a Softmax Output Layer\n",
    "\n",
    "**Structure:**\n",
    "- **Input Layer:** Features $\\mathbf{X}$.\n",
    "- **Hidden Layers:** One or more layers with activations (e.g., ReLU).\n",
    "- **Output Layer:** $K$ neurons (one per class) with a softmax activation.\n",
    "\n",
    "### Forward Propagation in the Output Layer\n",
    "\n",
    "For a network with $K$ output classes, compute the logits:\n",
    "\n",
    "$$\n",
    "Z_j = \\mathbf{W}_j \\cdot a^{(L-1)} + b_j \\quad \\text{for } j = 1, \\dots, K,\n",
    "$$\n",
    "\n",
    "where $a^{(L-1)}$ are the activations from the previous layer. Then, apply softmax:\n",
    "\n",
    "$$\n",
    "a_j = \\frac{e^{Z_j}}{\\sum_{k=1}^{K} e^{Z_k}}.\n",
    "$$\n",
    "\n",
    "**Note:**  \n",
    "Each $a_j$ depends on all $Z_k$ values, unlike element-wise activations (e.g., sigmoid).\n",
    "\n",
    "### TensorFlow Implementation (Conceptual Overview)\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(25, activation='relu', input_shape=(input_dim,)),\n",
    "    tf.keras.layers.Dense(15, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')  # For 10 classes\n",
    "])\n",
    "```\n",
    "\n",
    "**Loss Function:**  \n",
    "\n",
    "Use `SparseCategoricalCrossentropy`:\n",
    "  \n",
    "```python\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Numerical Stability in Softmax Implementations\n",
    "\n",
    "- **Floating-Point Precision:** Exponential functions can produce very large or very small numbers, leading to round-off errors.\n",
    "- **Example:** Computing $2/10000$ directly versus through an alternative formulation can yield slight differences due to limited precision.\n",
    "\n",
    "### Improving Stability\n",
    "\n",
    "**Combine Computations:**  \n",
    "\n",
    "Instead of computing softmax probabilities and then applying cross-entropy, combine them. This allows frameworks like TensorFlow to rearrange operations for better numerical accuracy.\n",
    "\n",
    "**TensorFlow Example with Logits:**  \n",
    "\n",
    "Use `from_logits=True` to compute the loss more stably:\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(25, activation='relu', input_shape=(input_dim,)),\n",
    "    tf.keras.layers.Dense(15, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)  # Linear activation; outputs are logits\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Multi-label Classification\n",
    "\n",
    "**Multi-class vs. Multi-label:**\n",
    "- **Multi-class:** Each example is assigned exactly one label (even if there are many classes).\n",
    "- **Multi-label:** Each example can have **multiple labels** simultaneously.\n",
    "\n",
    "**Example:** In self-driving cars, an image may be labeled for:\n",
    "- Presence of a car\n",
    "- Presence of a bus\n",
    "- Presence of a pedestrian\n",
    "  \n",
    "Here, the output might be a vector such as $[1, 0, 1]$, indicating \"yes\" for car and pedestrian, and \"no\" for bus.\n",
    "\n",
    "### Neural Network Implementation for Multi-label Classification\n",
    "\n",
    "**Approach 1:**  \n",
    "\n",
    "Train separate binary classifiers for each label.\n",
    "\n",
    "**Approach 2:**  \n",
    "\n",
    "Use a single network with multiple outputs:\n",
    "- **Output Layer:** One neuron per label.\n",
    "- **Activation:** Use the sigmoid function for each output, as each label represents an independent binary decision.\n",
    "  \n",
    "**TensorFlow Example:**\n",
    "  \n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='sigmoid')  # 3 labels: car, bus, pedestrian\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',  # Suitable for multi-label classification\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6aa3af-a49e-4abe-8ecc-c10b9d48888d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
