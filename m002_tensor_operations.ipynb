{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1358f09-e14f-4cdf-b177-fd56530aa8b3",
   "metadata": {},
   "source": [
    "# Linear Algebra & Tensor Operations\n",
    "\n",
    "In this lecture, we explore fundamental tensor operations alongside classic techniques for solving systems of linear equations. The session covers practical code demos in popular libraries (NumPy, TensorFlow, and PyTorch) and hands-on paper–pencil methods using substitution and elimination.\n",
    "\n",
    "---\n",
    "\n",
    "## Tensor Transposition\n",
    "\n",
    "- **Scalar Transposition:**  \n",
    "  A zero-dimensional tensor (a scalar) remains unchanged when transposed.\n",
    "\n",
    "- **Vector Transposition:**  \n",
    "  Transposing a vector converts a column vector to a row vector (and vice versa). This was previously demonstrated in the Machine Learning Foundation series.\n",
    "\n",
    "- **Matrix Transposition:**  \n",
    "  The transposition of a matrix involves flipping the matrix over its **main diagonal** (the diagonal running from the top left to the bottom right).  \n",
    "\n",
    "  An element at position $(i,j)$ moves to position $(j,i)$.\n",
    "\n",
    "  Graphically, the top-left element stays in place while, for example, the top-right element becomes the bottom-left.\n",
    "\n",
    "- **Code Demo Highlights:**  \n",
    "  - In **NumPy**, appending `.T` to a matrix (e.g., `X.T`) returns its transpose.  \n",
    "  - In **PyTorch**, the same `.T` operator is available.  \n",
    "  - In **TensorFlow**, you call a dedicated transpose function to achieve the same result.\n",
    "\n",
    "---\n",
    "\n",
    "## Basic Tensor Arithmetic\n",
    "\n",
    "### Scalar Operations\n",
    "\n",
    "- **Addition & Multiplication:**  \n",
    "  Multiplying or adding a scalar applies the operation to every element, while preserving the original tensor shape.  \n",
    "  For instance, given a matrix $X$, multiplying by 2 or adding 2 applies the operation element-wise.  \n",
    "  Standard operator precedence applies: multiplication is performed before addition (e.g., $25 \\times 2 + 2 = 52$ for each element).\n",
    "\n",
    "- **Operator Overloading:**  \n",
    "  In PyTorch and TensorFlow, standard Python operators (like `+` and `*`) are overloaded to perform element-wise arithmetic, calling methods such as `torch.mul` or `tf.multiply` behind the scenes.\n",
    "\n",
    "### The Hadamard Product\n",
    "\n",
    "- **Definition:**  \n",
    "  The Hadamard product is the element-wise multiplication of two matrices (or tensors) of the same size.\n",
    "\n",
    "  \n",
    "$$\n",
    "(X \\odot A)_{ij} = X_{ij} \\times A_{ij}\n",
    "$$\n",
    "\n",
    "  \n",
    "  It is notated as a dot with a circle around it: $\\odot$.  \n",
    "  A code demo showed that using the asterisk (`*`) operator in NumPy, PyTorch, or TensorFlow executes the Hadamard product.\n",
    "\n",
    "---\n",
    "\n",
    "## Reduction Operations\n",
    "\n",
    "- **Summation:**  \n",
    "  Reducing a tensor by summing all its elements is common.  \n",
    "  For a vector $x$ of length $n$, the sum is:\n",
    "\n",
    "  \n",
    "$$\n",
    "\\text{sum}(x) = \\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "  \n",
    "  For a matrix with dimensions $M \\times N$, summing all elements yields a single scalar.\n",
    "\n",
    "- **Axis-specific Reduction:**  \n",
    "  - **Axis 0 (rows):** Sums across columns (e.g., sum of each column).  \n",
    "  - **Axis 1 (columns):** Sums across rows (e.g., sum of each row).\n",
    "\n",
    "- **Other Operations:**  \n",
    "  Similar reduction techniques apply for computing the maximum, minimum, mean, or product over selected axes.\n",
    "\n",
    "- **Library Methods:**  \n",
    "  - **NumPy:** Use the `.sum()` method.\n",
    "  - **PyTorch:** Use `torch.sum()`.\n",
    "  - **TensorFlow:** Use `tf.reduce_sum()`.\n",
    "\n",
    "---\n",
    "\n",
    "## The Dot Product\n",
    "\n",
    "- **Definition:**  \n",
    "  The dot product of two vectors $X$ and $y$ (of the same length $n$) involves:\n",
    "\n",
    "  1. **Element-wise multiplication:**  \n",
    "\n",
    "  \n",
    "$$\n",
    "\\text{product}_i = X_i \\times y_i\n",
    "$$\n",
    "\n",
    "  \n",
    "  2. **Reduction (summation):**  \n",
    "\n",
    "  \n",
    "$$\n",
    "X \\cdot y = \\sum_{i=1}^{n} X_i \\times y_i\n",
    "$$\n",
    "\n",
    "  \n",
    "- **Notations:**  \n",
    "  The dot product may be denoted as:\n",
    "  - $X \\cdot y$\n",
    "  - $X^\\top y$\n",
    "  - $\\langle X, y \\rangle$\n",
    "  \n",
    "- **Code Demos:**  \n",
    "  Demonstrations in NumPy, PyTorch, and TensorFlow showed the calculation of a dot product by first performing element-wise multiplication and then summing the results.\n",
    "\n",
    "---\n",
    "\n",
    "## Solving Systems of Linear Equations\n",
    "\n",
    "The lecture transitioned from tensor operations to algebraic methods for solving systems of equations, using two primary techniques.\n",
    "\n",
    "### Substitution Method\n",
    "\n",
    "- **When to Use:**  \n",
    "  Best applied when one variable has a coefficient of one.\n",
    "  \n",
    "- **Example System:**  \n",
    "\n",
    "  \n",
    "$$\n",
    "\\begin{cases}\n",
    "y = 3x \\\\\n",
    "-5x + 2y = 2\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "  \n",
    "- **Process:**  \n",
    "  Substitute $y = 3x$ into the second equation, simplify, and solve for $x$. Then substitute back to find $y$.\n",
    "\n",
    "### Elimination Method\n",
    "\n",
    "- **When to Use:**  \n",
    "  Useful when no variable has a coefficient of one. It uses the addition (or subtraction) property to cancel out one variable.\n",
    "  \n",
    "- **Example System:**  \n",
    "\n",
    "  \n",
    "$$\n",
    "\\begin{cases}\n",
    "2x - 3y = 15 \\\\\n",
    "4x + 10y = 14\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "  \n",
    "- **Steps:**\n",
    "  1. **Scale Equations:** Multiply one or both equations so that one variable’s coefficients are equal in magnitude but opposite in sign.\n",
    "     \n",
    "     For instance, multiply the first equation by $-2$ to obtain $-4x + 6y = -30$.\n",
    "     \n",
    "  2. **Add Equations:** Adding the scaled first equation to the second eliminates $x$, leaving an equation in $y$.\n",
    "  \n",
    "  3. **Solve:** Solve for $y$, then substitute back into one of the original equations to find $x$.\n",
    "  \n",
    "- **Edge Case:**  \n",
    "  A situation might arise where after elimination, you get an impossible statement (e.g., $0 = -15$). This indicates that the system has **no solution** (the lines are parallel).\n",
    "\n",
    "- **Additional Shortcut:**  \n",
    "  If an equation can be simplified (e.g., dividing by a common factor to yield a coefficient of one), substitution may become more straightforward than elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02e60f6-5287-4bcc-9e61-bd2290e56530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
