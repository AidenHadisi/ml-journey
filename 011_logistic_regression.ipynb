{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## Classification\n",
    "- **Definition:** Predicting a discrete category rather than a continuous value.\n",
    "- **Examples:**\n",
    "  - **Spam detection:** Is an email spam? (**Yes/No**)\n",
    "  - **Fraud detection:** Is a transaction fraudulent? (**Yes/No**)\n",
    "  - **Medical diagnosis:** Is a tumor malignant? (**Yes/No**)\n",
    "\n",
    "\n",
    "### Binary Classification\n",
    "- **Only two possible outcomes:** \n",
    "  - **0 (Negative class)** â†’ Absence of a property  \n",
    "  - **1 (Positive class)** â†’ Presence of a property  \n",
    "\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "**Logistic Regression** is one of the most widely used classification algorithms. It is often applied in medical diagnostics, spam detection, and online advertising. Unlike linear regression, logistic regression predicts a probability value and maps it to discrete class labels (0 or 1).  \n",
    "\n",
    "- **Linear regression:** Predicts continuous values.\n",
    "- **Logistic regression:** Predicts probabilities.\n",
    "- **Despite its name, Logistic Regression is used for Classification.**  \n",
    "- **Output:** Probability of the input data belonging to a certain category.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function\n",
    "\n",
    "The **Sigmoid Function** is used in Logistic Regression to map predictions to probabilities. It is an S-shaped curve that maps any real value to the range [0, 1]. The function is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $z$ is the input to the function. A linear combination of the input features.\n",
    "- $\\sigma(z)$ is the output, which is the probability of the input data belonging to the positive class.\n",
    "\n",
    "### ðŸ“‰ Properties of Sigmoid  \n",
    "| **Value of $z$**  | **$\\sigma(z)$ Output** |\n",
    "|--------------------|----------------|\n",
    "| $z \\to +\\infty$  | $\\sigma(z) \\to 1$  |\n",
    "| $z = 0$  | $\\sigma(z) = 0.5$ |\n",
    "| $z \\to -\\infty$  | $\\sigma(z) \\to 0$  |\n",
    "\n",
    "The sigmoid function **compresses** any input $z$ into a probability range of **(0,1)**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model\n",
    "\n",
    "The Logistic Regression model follows a 2-step process:\n",
    "1. **Linear Combination:** Compute the linear combination of the input features and weights.\n",
    "\n",
    "$$\n",
    "z = w \\cdot x + b\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "2. **Sigmoid Activation:** Apply the sigmoid function to the linear combination to get the probability.\n",
    "\n",
    "$$\n",
    "f(x) = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Therefore, the Logistic Regression model can be represented as:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-(w \\cdot x + b)}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $f(x)$ is the predicted probability of the input data belonging to the positive class.\n",
    "- $w$ is the weight vector.\n",
    "- $x$ is the input feature vector.\n",
    "- $b$ is the bias term.\n",
    "\n",
    "The output of logistic regression, $f(x)$, represents the **probability** of a class label being **1**:  \n",
    "$$\n",
    "P(y = 1 \\mid x) = f(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Decision: Choosing $\\hat{y}$\n",
    "\n",
    "To convert the probability into a class label, we apply a **threshold**:\n",
    "\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases} \n",
    "1 & \\text{if } f(x) \\geq 0.5 \\\\\n",
    "0 & \\text{if } f(x) < 0.5\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- If $f(x) \\geq 0.5$, predict **$y = 1$**.\n",
    "- If $f(x) < 0.5$, predict **$y = 0$**.\n",
    "\n",
    "This thresholding mechanism allows logistic regression to separate data into distinct classes.\n",
    "\n",
    "## Decision Boundary\n",
    "\n",
    "The decision boundary is the line that separates the classes in a classification problem. In Logistic Regression. It is the region where the model is **equally confident** about classifying a point as either class 0 or class 1.\n",
    "\n",
    "the model predicts **$y = 1$** whenever:\n",
    "\n",
    "$$\n",
    "w \\cdot x + b \\geq 0\n",
    "$$\n",
    "\n",
    "and **$y = 0$** whenever:\n",
    "\n",
    "$$\n",
    "w \\cdot x + b < 0\n",
    "$$\n",
    "\n",
    "\n",
    "## ðŸ” Example: Visualizing a Linear Decision Boundary\n",
    "\n",
    "Consider a dataset with **two features** ($x_1$, $x_2$). The logistic regression model computes:\n",
    "\n",
    "$$\n",
    "z = w_1 x_1 + w_2 x_2 + b\n",
    "$$\n",
    "\n",
    "If we assume:\n",
    "\n",
    "$$\n",
    "w_1 = 1, \\quad w_2 = 1, \\quad b = -3\n",
    "$$\n",
    "\n",
    "Then, the decision boundary occurs where:\n",
    "\n",
    "$$\n",
    "x_1 + x_2 - 3 = 0\n",
    "$$\n",
    "\n",
    "which simplifies to:\n",
    "\n",
    "$$\n",
    "x_1 + x_2 = 3\n",
    "$$\n",
    "\n",
    "ðŸ”¹ **Interpretation**:\n",
    "- **Points where** $x_1 + x_2 > 3$ â†’ Predict $y = 1$\n",
    "- **Points where** $x_1 + x_2 < 3$ â†’ Predict $y = 0$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression and Decision Boundaries\n",
    "\n",
    "## 1. Logistic Regression Recap\n",
    "\n",
    "### Model Overview\n",
    "- **Objective:** Estimate the probability that $y=1$ given input features $\\mathbf{x}$.\n",
    "- **Two-step Process:**\n",
    "  1. **Linear Combination:**\n",
    "     - Compute:\n",
    "\n",
    "$$z = \\mathbf{w} \\cdot \\mathbf{x} + b$$\n",
    "\n",
    "  2. **Activation via Sigmoid Function:**\n",
    "     - Apply the Sigmoid (or logistic) function:\n",
    "\n",
    "$$f(x) = g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "- **Interpretation:**  \n",
    "  $f(x)$ is interpreted as the probability $\\Pr(y=1 \\mid \\mathbf{x}; \\mathbf{w}, b)$, yielding a value between 0 and 1 (e.g., 0.7 or 0.3).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Prediction Threshold and Decision Rule\n",
    "\n",
    "### How to Decide the Class?\n",
    "- **Thresholding:**  \n",
    "  A common threshold is $0.5$.  \n",
    "  - If $f(x) \\geq 0.5$, **predict** $\\hat{y} = 1$.\n",
    "  - If $f(x) < 0.5$, **predict** $\\hat{y} = 0$.\n",
    "\n",
    "### Mathematical Explanation\n",
    "- Since $f(x) = g(z)$ and the sigmoid function satisfies:\n",
    "  $$g(z) \\geq 0.5 \\quad \\text{if and only if} \\quad z \\geq 0,$$\n",
    "  the prediction rule becomes:\n",
    "  - **Predict 1:** when \n",
    "    $$\\mathbf{w} \\cdot \\mathbf{x} + b \\geq 0$$\n",
    "  - **Predict 0:** when \n",
    "    $$\\mathbf{w} \\cdot \\mathbf{x} + b < 0$$\n",
    "\n",
    "> **Note:** The condition $\\mathbf{w} \\cdot \\mathbf{x} + b = 0$ defines the **decision boundary** â€” the point of neutrality between the two classes.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Visualizing Decision Boundaries with Two Features\n",
    "\n",
    "### Example: Linear Decision Boundary\n",
    "- **Scenario:** Classification problem with features $x_1$ and $x_2$.\n",
    "- **Training Data:**  \n",
    "  - **Positive examples ($y=1$):** Red crosses.\n",
    "  - **Negative examples ($y=0$):** Blue circles.\n",
    "  \n",
    "- **Logistic Regression Function:**\n",
    "  $$f(x) = g(z), \\quad \\text{with} \\quad z = w_1 x_1 + w_2 x_2 + b$$\n",
    "  \n",
    "- **Given Parameters:**\n",
    "  - $w_1 = 1$, $w_2 = 1$, $b = -3$\n",
    "  \n",
    "- **Decision Boundary Calculation:**\n",
    "  - Set $z = 0$:\n",
    "    $$w_1 x_1 + w_2 x_2 + b = 0 \\quad \\Rightarrow \\quad x_1 + x_2 - 3 = 0$$\n",
    "  - **Boundary Line:**  \n",
    "    $$x_1 + x_2 = 3$$\n",
    "  \n",
    "- **Interpretation:**\n",
    "  - **Right of the line ($\\mathbf{w} \\cdot \\mathbf{x} + b \\geq 0$):** Predict $y = 1$.\n",
    "  - **Left of the line ($\\mathbf{w} \\cdot \\mathbf{x} + b < 0$):** Predict $y = 0$.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Non-Linear Decision Boundaries with Polynomial Features\n",
    "\n",
    "### Extending Logistic Regression\n",
    "- **Idea:** Incorporate polynomial features to allow for non-linear decision boundaries.\n",
    "- **Example Function:**\n",
    "  $$z = w_1 x_1^2 + w_2 x_2^2 + b$$\n",
    "  \n",
    "- **Given Parameters:**\n",
    "  - $w_1 = 1$, $w_2 = 1$, $b = -1$\n",
    "  \n",
    "- **Decision Boundary Calculation:**\n",
    "  - Set $z = 0$:\n",
    "    $$x_1^2 + x_2^2 - 1 = 0 \\quad \\Rightarrow \\quad x_1^2 + x_2^2 = 1$$\n",
    "  - **Boundary Curve:**  \n",
    "    A circle with radius 1.\n",
    "  \n",
    "- **Interpretation:**\n",
    "  - **Outside the Circle ($x_1^2 + x_2^2 \\geq 1$):** Predict $y = 1$.\n",
    "  - **Inside the Circle ($x_1^2 + x_2^2 < 1$):** Predict $y = 0$.\n",
    "\n",
    "> **Tip:** By incorporating higher-order polynomial terms (e.g., $x_1x_2$, $x_1^2$, $x_2^2$, etc.), logistic regression can model complex decision boundaries such as ellipses or even more intricate shapes.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Complex Decision Boundaries\n",
    "\n",
    "### Combining Multiple Polynomial Terms\n",
    "- **General Form Example:**\n",
    "  $$z = w_1 x_1 + w_2 x_2 + w_3 x_1^2 + w_4 (x_1x_2) + w_5 x_2^2$$\n",
    "- **Result:**  \n",
    "  This can yield highly non-linear decision boundaries, which might take forms like ellipses or other irregular shapes.\n",
    "\n",
    "### Key Insight\n",
    "- **Without Polynomial Features:**  \n",
    "  Logistic regression will always produce a **linear (straight-line)** decision boundary.\n",
    "- **With Polynomial Features:**  \n",
    "  The model is capable of fitting **complex boundaries** to better separate classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
