{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4c7993b-4e3b-4c34-a0b9-6a2ec8622634",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "Reinforcement learning (RL) is a framework where an agent learns to make decisions by interacting with an environment. The agent receives feedback through rewards, and the ultimate goal is to develop a strategy that maximizes the total accumulated reward.\n",
    "\n",
    "## Fundamental Components\n",
    "\n",
    "### State ($s$)\n",
    "The **state** represents the current situation or configuration of the system. It may include various factors such as:\n",
    "- **Position and Orientation:** For an autonomous vehicle or robot, the state includes its current position, orientation, and speed.\n",
    "- **Environment Details:** In a game, the state could be the arrangement of pieces on a board or the configuration of a game level.\n",
    "\n",
    "### Action ($a$)\n",
    "An **action** is any decision or move that the agent can make. Actions affect the state of the system. For example:\n",
    "- **Control Inputs:** For an autonomous helicopter, actions might involve adjusting the control sticks.\n",
    "- **Movement Decisions:** For a Mars rover, actions could include moving left or right to reach a desired location.\n",
    "- **Game Moves:** In chess, an action is one of the legal moves available from a given board position.\n",
    "\n",
    "### Reward ($R(s)$)\n",
    "The **reward** is a numerical value that provides feedback on the outcome of an action. It indicates how favorable a particular state or action is:\n",
    "- **Positive Reward:** Signals that the agent is performing well (e.g., maintaining stable flight or achieving a winning move in a game).\n",
    "- **Negative Reward:** Indicates poor performance (e.g., crashing an aircraft or making a losing move).\n",
    "\n",
    "The reward function helps guide the learning process by reinforcing behaviors that lead to higher rewards.\n",
    "\n",
    "### Discount Factor ($\\gamma$)\n",
    "The **discount factor** ($\\gamma$) is a number between 0 and 1 that determines the importance of future rewards relative to immediate rewards. A discount factor close to 1 means future rewards are nearly as valuable as immediate ones, while a lower value makes the agent focus more on immediate rewards.\n",
    "\n",
    "The idea is captured mathematically when computing the **return**.\n",
    "\n",
    "### Return ($G$)\n",
    "The **return** is the total accumulated reward over time, but with future rewards discounted to reflect their delayed benefit. It is given by the formula:\n",
    "\n",
    "$$\n",
    "G = R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\gamma^3 R_4 + \\cdots\n",
    "$$\n",
    "\n",
    "This formula shows that rewards received later are worth less than rewards received immediately, depending on the value of $\\gamma$. For example, if $\\gamma = 0.9$, the reward received two steps later is multiplied by $0.9^2$.\n",
    "\n",
    "### Policy ($\\pi$)\n",
    "A **policy** is a function that maps each state to an action. It defines the agent’s strategy for decision-making. In mathematical terms, a policy is written as:\n",
    "\n",
    "$$\n",
    "\\pi(s) = a\n",
    "$$\n",
    "\n",
    "The objective in reinforcement learning is to find an optimal policy that maximizes the expected return for every state.\n",
    "\n",
    "## The Markov Decision Process (MDP)\n",
    "\n",
    "Reinforcement learning problems are commonly formulated as a **Markov decision process (MDP)**, which provides a formal framework consisting of:\n",
    "\n",
    "- **States:** All possible situations the agent can encounter.\n",
    "- **Actions:** All available decisions or moves.\n",
    "- **Rewards:** Immediate feedback for actions taken.\n",
    "- **Transition Dynamics:** Rules that determine how actions change the state.\n",
    "- **Markov Property:** The principle that the future state depends only on the current state and the action taken, not on the history of past states.\n",
    "\n",
    "This framework allows for a clear mathematical treatment of decision-making problems and helps in designing algorithms that learn effective policies.\n",
    "\n",
    "## Examples\n",
    "\n",
    "### Autonomous Systems (e.g., Helicopter)\n",
    "- **State:** The helicopter’s position, speed, orientation, and sensor data (such as GPS and accelerometer readings).\n",
    "- **Action:** Adjustments to control sticks.\n",
    "- **Reward:** A small positive reward (e.g., +1 per second) when flying stably, and a large negative reward (e.g., -1000) if it crashes.\n",
    "- **Goal:** To learn a policy that keeps the helicopter flying safely and performing maneuvers like aerobatic stunts.\n",
    "\n",
    "### Mars Rover Navigation\n",
    "Imagine a simplified rover that can be in one of six positions (states 1 through 6):\n",
    "- **States:** Six positions with specific rewards assigned to some states (e.g., state 1 gives a reward of 100, state 6 gives 40, while states 2–5 provide zero reward).\n",
    "- **Actions:** The rover can choose to move left or right.\n",
    "- **Return Calculation:**  \n",
    "  For instance, if the rover starts at state 4 and moves left with a discount factor of $\\gamma = 0.5$, the sequence of rewards (assuming zero rewards for intermediate states until reaching state 1) is weighted as follows:\n",
    "\n",
    "$$\n",
    "G = 0 + 0.5 \\times 0 + 0.5^2 \\times 0 + 0.5^3 \\times 100 = 12.5\n",
    "$$\n",
    "\n",
    "- **Policy:** The strategy might involve choosing actions that lead to state 1 faster to maximize the return.\n",
    "\n",
    "### Game Playing (e.g., Chess)\n",
    "- **State:** The configuration of the chess board.\n",
    "- **Action:** A legal move from the current board position.\n",
    "- **Reward:** Typically defined as +1 for winning, -1 for losing, and 0 for a draw.\n",
    "- **Discount Factor:** Often chosen very close to 1 (e.g., $\\gamma = 0.99$) to emphasize long-term outcomes.\n",
    "- **Policy:** A function that selects the best move from a given board state to maximize the chance of winning the game.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In reinforcement learning, an agent learns to maximize its accumulated reward through:\n",
    "- Observing the **state** of the environment.\n",
    "- Taking an **action** based on a **policy**.\n",
    "- Receiving a **reward** that provides feedback.\n",
    "- Considering future rewards using a **discount factor**.\n",
    "- Evaluating the overall success using the **return**.\n",
    "\n",
    "The problem is typically modeled as a **Markov decision process (MDP)**, where the next state depends solely on the current state and the chosen action. This framework is versatile and can be applied to robotics, autonomous systems, game playing, financial trading, and many other complex decision-making tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db3087-7513-4d02-9f28-7484c610efe7",
   "metadata": {},
   "source": [
    "## The State-Action Value Function (Q Function)\n",
    "\n",
    "The **state-action value function** is denoted as $Q(s, a)$. \n",
    "It represents the **expected return** when:\n",
    "- Starting in state $s$,\n",
    "- Taking action $a$ once,\n",
    "- And then following an **optimal policy** thereafter.\n",
    "\n",
    "In other words:\n",
    "\n",
    "$$\n",
    "Q(s, a) = \\text{Return obtained from } s \\text{ after taking } a \\text{ and acting optimally henceforth.}\n",
    "$$\n",
    "\n",
    "### Return and Discounting\n",
    "\n",
    "The **return** is the sum of discounted rewards:\n",
    "\n",
    "$$\n",
    " G_t = R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\cdots\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $R_1$ is the immediate reward,\n",
    "- $R_2, R_3, \\dots$ are future rewards,\n",
    "- $\\gamma$ (gamma) is the **discount factor** with $0 < \\gamma \\leq 1$.\n",
    "  \n",
    "> A high $\\gamma$ (close to 1) means the agent is more patient, valuing future rewards nearly as much as immediate ones. A low $\\gamma$ causes the agent to be more short-sighted.\n",
    "\n",
    "### Example: Mars Rover\n",
    "Imagine a Mars Rover that can take two actions (e.g., **left** or **right**) in each state:\n",
    "\n",
    "**Rewards:**\n",
    "- Terminal states yield a reward (e.g., 100 or 40).\n",
    "- Intermediate states often yield a reward of 0.\n",
    "\n",
    "**Numerical Example:**\n",
    "- From **state 2**, taking **left** might lead directly to a terminal state with a reward of 100. If $\\gamma = 0.5$, then:\n",
    "\n",
    "$$\n",
    "Q(2, \\text{left}) = 0 + 0.5 \\times 100 = 50.\n",
    "$$\n",
    "\n",
    "- Alternatively, taking **right** from state 2 may lead to state 3, and following the optimal policy from there might yield:\n",
    "\n",
    "$$\n",
    "Q(2, \\text{right}) = 0 + 0.5 \\times 25 = 12.5.\n",
    "$$\n",
    "\n",
    "### Determining the Optimal Policy\n",
    "\n",
    "The optimal action $\\pi(s)$ in any state $s$ is the one that maximizes the Q value:\n",
    "\n",
    "$$\n",
    "\\pi(s) = \\arg\\max_{a} Q(s, a).\n",
    "$$\n",
    "\n",
    "In the above example, since $50 > 12.5$, the best action in state 2 is to go **left**.\n",
    "\n",
    "---\n",
    "\n",
    "## The Bellman Equation\n",
    "\n",
    "The **Bellman equation** breaks down the Q function into two parts:\n",
    "- **Immediate reward:** The reward $R(s)$ received in the current state.\n",
    "- **Future rewards:** The discounted optimal return from the next state.\n",
    "\n",
    "The equation is written as:\n",
    "\n",
    "$$\n",
    "Q(s, a) = R(s) + \\gamma \\max_{a'} Q(s', a')\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $s'$ is the state reached after taking action $a$ in state $s$.\n",
    "- $a'$ represents all possible actions in state $s'$.\n",
    "\n",
    "### Intuition\n",
    "- **Immediate Reward ($R(s)$):** What you gain right away.\n",
    "- **Future Return:** $\\gamma \\max_{a'} Q(s', a')$ is the best discounted return possible from the next state.\n",
    "- The **total return** from taking action $a$ in state $s$ is the sum of these two parts.\n",
    "\n",
    "### Detailed Examples\n",
    "1. **State 2, Action Right:**\n",
    "   - Suppose $R(2) = 0$, and taking right leads to state 3 with $\\max_{a'} Q(3, a') = 25$.\n",
    "   - Then:\n",
    "\n",
    "$$\n",
    "Q(2, \\text{right}) = 0 + 0.5 \\times 25 = 12.5.\n",
    "$$\n",
    "\n",
    "2. **State 4, Action Left:**\n",
    "   - With $R(4) = 0$, if going left takes you to state 3 (again with $\\max_{a'} Q(3, a') = 25$):\n",
    "\n",
    "$$\n",
    "Q(4, \\text{left}) = 0 + 0.5 \\times 25 = 12.5.\n",
    "$$\n",
    "\n",
    "3. **Terminal States:**\n",
    "   - In terminal states, no future rewards exist. Hence, the equation simplifies to:\n",
    "\n",
    "$$\n",
    "Q(s,a) = R(s).\n",
    "$$\n",
    "\n",
    "### Alternative View of Returns\n",
    "The return can be re-written as:\n",
    "\n",
    "$$\n",
    "G_t = R_1 + \\gamma (R_2 + \\gamma R_3 + \\gamma^2 R_4 + \\cdots),\n",
    "$$\n",
    "\n",
    "and the term in parentheses represents the optimal future return:\n",
    "\n",
    "$$\n",
    "\\max_{a'} Q(s', a').\n",
    "$$\n",
    "\n",
    "Thus, the Bellman equation encapsulates this recursive structure.\n",
    "\n",
    "---\n",
    "\n",
    "## Stochastic Environments\n",
    "\n",
    "In many real-world problems, the effect of an action is **stochastic** (random).\n",
    "\n",
    "**Example:** A Mars Rover commanded to go left might:\n",
    "- Successfully move left with probability 0.9.\n",
    "- Slip and move right with probability 0.1.\n",
    "\n",
    "### Expected Return in Stochastic Settings\n",
    "\n",
    "Since the outcome is uncertain, we focus on the **expected return** rather than a single deterministic return.\n",
    "\n",
    "We imagine running the same experiment many times and then calculating the mean of all the results. For example, if you have a Mars Rover and you let it perform a mission repeatedly, each run (or trajectory) may give you a slightly different sum of discounted rewards because of randomness in the environment. \n",
    "\n",
    "Instead of getting one fixed value, you run the experiment over and over, record the total reward for each run, and then take the average of all those total rewards. \n",
    "\n",
    "$$\n",
    "\\mathbb{E}[G_t] = \\mathbb{E}\\left[ R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\cdots \\right],\n",
    "$$\n",
    "\n",
    "- In statistics `Expected` $\\mathbb{E}$ just means average in this context.\n",
    "- This process of averaging is done over all possible outcomes weighted by how likely each outcome is to occur. In practice, if you repeated the mission 1000 times, you would add up all the discounted rewards from each run and then divide by 1000. That gives you the expected return, which is what the $\\mathbb{E}$ operator represents.\n",
    "\n",
    "The **Bellman equation** is modified to account for this expectation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = R(s) + \\gamma \\, \\mathbb{E}\\left[ \\max_{a'} Q(s', a') \\right].\n",
    "$$\n",
    "\n",
    "### Impact of Stochasticity\n",
    "- **Increased Uncertainty:** As the misstep (or error) probability increases, the control over the state transitions decreases.\n",
    "- **Effect on Q Values:** Higher misstep probabilities generally lead to lower Q values because the expected return is reduced.\n",
    "- **Lab Experiments:** Modifying parameters such as the terminal reward, discount factor $\\gamma$, or the misstep probability in simulation notebooks can help build intuition on how these values and the optimal policy change.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "**Q Function ($Q(s,a)$):** Represents the expected return starting from state $s$, taking action $a$, and then following the optimal policy. Enables deriving the optimal policy:\n",
    "\n",
    "$$\n",
    "\\pi(s) = \\arg\\max_{a} Q(s,a).\n",
    "$$\n",
    "\n",
    "**Bellman Equation:** Decomposes the return into an immediate reward plus the discounted future return. Provides a recursive way to compute $Q(s,a)$:\n",
    "\n",
    "$$\n",
    "Q(s, a) = R(s) + \\gamma \\max_{a'} Q(s', a').\n",
    "$$\n",
    "\n",
    "**Stochastic Environments:** Require considering the **expected value** of returns due to uncertainty in action outcomes. Modify the Bellman equation with an expectation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = R(s) + \\gamma \\, \\mathbb{E}\\left[ \\max_{a'} Q(s', a') \\right].\n",
    "$$\n",
    "\n",
    "**Parameter Effects:**\n",
    "- **Discount Factor ($\\gamma$):**\n",
    "    - High $\\gamma$ (closer to 1): Agent is more patient; future rewards have higher weight.\n",
    "    - Low $\\gamma$: Agent is more focused on immediate rewards.\n",
    "- **Reward Structure:** Directly influences the computed Q values.\n",
    "- **Misstep Probability:** Higher probability of action failure lowers the expected returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70df9ae3-01ee-4571-a801-a57bddeaf5a2",
   "metadata": {},
   "source": [
    "## Continuous State Spaces\n",
    "\n",
    "### Discrete vs. Continuous States\n",
    "**Discrete State Spaces:**  \n",
    "- Example: A simplified Mars rover that can only occupy one of six fixed positions.\n",
    "\n",
    "**Continuous State Spaces:**  \n",
    "- Most robots operate in environments where the state is defined by a continuous range of values.\n",
    "- **Examples:**\n",
    "    1. A Mars rover moving along a line with positions from 0 to 6 kilometers can be at 2.7 km, 4.8 km, etc.\n",
    "    2. A self-driving car or truck:\n",
    "        - **State vector components:**  \n",
    "            - Position: $x$, $y$\n",
    "            - Orientation: $\\theta$\n",
    "            - Velocities: $x\\_dot$, $y\\_dot$\n",
    "            - Angular velocity: $\\theta\\_dot$  \n",
    "        - The state is a vector of six continuous numbers.\n",
    "    3. Autonomous Helicopter\n",
    "        - **State representation:**  \n",
    "            - Position: $x$, $y$, $z$\n",
    "            - Orientation (using Euler angles):\n",
    "            - Roll ($\\phi$)\n",
    "            - Pitch ($\\theta$)\n",
    "            - Yaw ($\\omega$)\n",
    "            - Velocities: Linear velocities along $x$, $y$, $z$ and angular velocities (rates of change for $\\phi$, $\\theta$, $\\omega$\n",
    "        - The helicopter's full state might be a vector of 12 numbers.\n",
    "\n",
    "---\n",
    "\n",
    "### The Lunar Lander Environment\n",
    "\n",
    "- **Task:**  \n",
    "  Land a simulated lunar lander safely on the moon by firing appropriate thrusters.\n",
    "- **Actions:**  \n",
    "  The agent can take one of four actions at every time step:\n",
    "  - `nothing` – No thrust; gravity and inertia act.\n",
    "  - `left` – Fire the left thruster to push right.\n",
    "  - `main` – Fire the main engine (downward thrust).\n",
    "  - `right` – Fire the right thruster to push left.\n",
    "  \n",
    "- **State Variables:**  \n",
    "  The state vector contains:\n",
    "  - **Position:** $x$, $y$\n",
    "  - **Velocity:** $x\\_dot$, $y\\_dot$\n",
    "  - **Orientation:** Angle $\\theta$ and angular velocity $\\theta\\_dot$\n",
    "  - **Leg Contact Indicators:**  \n",
    "    - $l$: Whether the left leg is touching the ground (binary: 0 or 1)\n",
    "    - $r$: Whether the right leg is touching the ground (binary: 0 or 1)\n",
    "\n",
    "#### Reward Function\n",
    "- **Landing Reward:**  \n",
    "  - A reward between +100 and +140 is given when the lander reaches the pad.\n",
    "- **Additional Rewards:**  \n",
    "  - Positive reward for moving toward the pad.\n",
    "  - Negative reward for drifting away.\n",
    "  - Crash penalty: $-100$ for crashing.\n",
    "  - Soft landing bonus: $+100$ for each leg that touches down, plus an extra $+10$ reward.\n",
    "- **Fuel Penalties:**  \n",
    "  - Main engine fire: $-0.3$\n",
    "  - Side thrusters (left/right): $-0.03$\n",
    "\n",
    "#### Objective\n",
    "\n",
    "Learn a policy $\\pi$ that maps states $s$ to actions $a$ (i.e., $a = \\pi(s)$) in order to maximize the **return**, defined as the sum of discounted rewards:\n",
    "  \n",
    "$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k}$$\n",
    "\n",
    "**Discount Factor:**  For Lunar Lander, a high discount factor (e.g., $\\gamma = 0.985$) is typically used.\n",
    "\n",
    "---\n",
    "\n",
    "## Deep Q-Network (DQN) Algorithm\n",
    "\n",
    "**Basic Architecture (Inefficient Approach):**\n",
    "- **Input:** Concatenation of state ($8$ numbers for Lunar Lander) and action (one-hot encoding of $4$ possible actions) resulting in a $12$-dimensional vector.\n",
    "- **Hidden Layers:** Two hidden layers with, e.g., 64 units each.\n",
    "- **Output:** A single scalar value approximating $Q(s, a)$.\n",
    "\n",
    "\n",
    "**Improved Architecture:**\n",
    "- **Input:** Only the state vector (8 numbers).\n",
    "- **Output:**  A vector of 4 values, each corresponding to $Q(s, \\texttt{nothing})$, $Q(s, \\texttt{left})$, $Q(s, \\texttt{main})$, and $Q(s, \\texttt{right})$.\n",
    "- **Benefit:**  \n",
    "    - Inference is performed once per state rather than separately for each action.\n",
    "    - Easily computes $\\max_{a'} Q(s', a')$ during the Bellman update.\n",
    "\n",
    "### Training via Bellman's Equation\n",
    "\n",
    "**Bellman Equation:**\n",
    "\n",
    "$$\n",
    "Q(s, a) = R(s) + \\gamma \\max_{a'} Q(s', a')\n",
    "$$\n",
    "\n",
    "**Training Examples:**\n",
    "From experiences collected as tuples $(s, a, R(s), s')$, form training pairs:\n",
    "- **Input:** $x = [s; a]$\n",
    "- **Target Value:** $y = R(s) + \\gamma \\max_{a'} Q(s', a')$\n",
    "- **Supervised Learning:** Use mean squared error (MSE) loss to update network parameters so that the network approximates the Q-function.\n",
    "\n",
    "### Experience Replay\n",
    "\n",
    "**Replay Buffer:**  \n",
    "- Store the most recent (e.g., 10,000) experiences.\n",
    "- Helps break correlations between sequential data and improves stability.\n",
    "\n",
    "---\n",
    "\n",
    "## Epsilon-Greedy Policy (Exploration vs. Exploitation)\n",
    "\n",
    "### Policy Definition\n",
    "- **Greedy Action:** With high probability (e.g., 95%), select the action that maximizes $Q(s, a)$.\n",
    "- **Exploration:** With a small probability ($\\epsilon$, e.g., 5%), choose an action at random.\n",
    "  \n",
    "### Purpose\n",
    "- **Avoid Local Optima:** Random actions prevent the algorithm from getting stuck due to initial random weight assignments that might incorrectly penalize good actions.\n",
    "- **Adaptive $\\epsilon$:** Often, $\\epsilon$ starts high (e.g., 1.0) to encourage exploration and is gradually decreased to a low value (e.g., 0.01) as learning improves.\n",
    "\n",
    "---\n",
    "\n",
    "## Refinements to the Learning Algorithm\n",
    "\n",
    "### Mini-Batching\n",
    "\n",
    "Instead of training on the full replay buffer (e.g., 10,000 examples) each time, select a smaller batch (e.g., 1,000 examples).\n",
    "\n",
    "**Benefits:**\n",
    "- Speeds up each training iteration.\n",
    "- Introduces noise which may help in escaping local minima.\n",
    "\n",
    "**Analogy with Supervised Learning:**\n",
    "- Similar to mini-batch gradient descent in linear regression where a subset of the full dataset is used to compute parameter updates.\n",
    "\n",
    "### Soft Updates\n",
    "\n",
    "**Problem with Abrupt Updates:** \n",
    "\n",
    "Directly copying parameters from a newly trained network can lead to instability if the new network performs worse.\n",
    "\n",
    "**Solution – Soft Update Rule:**\n",
    "\n",
    "Instead of setting parameters $\\theta \\leftarrow \\theta_{new}$, update gradually:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\tau \\theta_{new} + (1 - \\tau) \\theta\n",
    "$$\n",
    "\n",
    "Example with $\\tau = 0.01$:\n",
    "\n",
    "For weights $W$:\n",
    "\n",
    "$$W \\leftarrow 0.01 \\, W_{new} + 0.99 \\, W$$\n",
    "\n",
    "For biases $B$:\n",
    "\n",
    "$$\n",
    "B \\leftarrow 0.01 \\, B_{new} + 0.99 \\, B\n",
    "$$\n",
    "\n",
    "- **Benefit:** Increases the stability of the learning process and helps the algorithm converge more reliably.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary and Practical Considerations\n",
    "\n",
    "### Overview of the Learning Process\n",
    "1. **Initialization:**\n",
    "   - Randomly initialize the neural network parameters.\n",
    "2. **Experience Collection:**\n",
    "   - Interact with the environment (e.g., Lunar Lander) using an epsilon-greedy policy.\n",
    "   - Collect tuples $(s, a, R(s), s')$ and store them in the replay buffer.\n",
    "3. **Training Loop:**\n",
    "   - Periodically sample mini-batches from the replay buffer.\n",
    "   - Form training pairs $(x, y)$ using the Bellman equation.\n",
    "   - Train the neural network to minimize the MSE between $Q(s, a)$ and $y$.\n",
    "   - Update network parameters using either a direct copy or a soft update strategy.\n",
    "4. **Policy Improvement:**\n",
    "   - As the network improves its estimate of the Q-function, use it to select actions that maximize the expected return.\n",
    "\n",
    "### Context in Machine Learning\n",
    "- **Reinforcement Learning vs. Supervised/Unsupervised Learning:**\n",
    "  - RL focuses on learning policies to maximize cumulative reward, whereas supervised learning maps inputs to outputs and unsupervised learning finds structure in data.\n",
    "- **Challenges:**\n",
    "  - RL can be more sensitive to hyperparameter choices (e.g., learning rate, $\\epsilon$ scheduling) and may require extensive tuning.\n",
    "- **Real-World Application:**\n",
    "  - While RL shows impressive results in simulations (e.g., video games, simulated robotics), transferring these methods to real-world robotics remains challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f10d9f8-a371-45f4-be99-a96c96f200fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
