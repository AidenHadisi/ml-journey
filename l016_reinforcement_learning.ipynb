{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4c7993b-4e3b-4c34-a0b9-6a2ec8622634",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "Reinforcement learning (RL) is a framework where an agent learns to make decisions by interacting with an environment. The agent receives feedback through rewards, and the ultimate goal is to develop a strategy that maximizes the total accumulated reward.\n",
    "\n",
    "## Fundamental Components\n",
    "\n",
    "### State ($s$)\n",
    "The **state** represents the current situation or configuration of the system. It may include various factors such as:\n",
    "- **Position and Orientation:** For an autonomous vehicle or robot, the state includes its current position, orientation, and speed.\n",
    "- **Environment Details:** In a game, the state could be the arrangement of pieces on a board or the configuration of a game level.\n",
    "\n",
    "### Action ($a$)\n",
    "An **action** is any decision or move that the agent can make. Actions affect the state of the system. For example:\n",
    "- **Control Inputs:** For an autonomous helicopter, actions might involve adjusting the control sticks.\n",
    "- **Movement Decisions:** For a Mars rover, actions could include moving left or right to reach a desired location.\n",
    "- **Game Moves:** In chess, an action is one of the legal moves available from a given board position.\n",
    "\n",
    "### Reward ($R(s)$)\n",
    "The **reward** is a numerical value that provides feedback on the outcome of an action. It indicates how favorable a particular state or action is:\n",
    "- **Positive Reward:** Signals that the agent is performing well (e.g., maintaining stable flight or achieving a winning move in a game).\n",
    "- **Negative Reward:** Indicates poor performance (e.g., crashing an aircraft or making a losing move).\n",
    "\n",
    "The reward function helps guide the learning process by reinforcing behaviors that lead to higher rewards.\n",
    "\n",
    "### Discount Factor ($\\gamma$)\n",
    "The **discount factor** ($\\gamma$) is a number between 0 and 1 that determines the importance of future rewards relative to immediate rewards. A discount factor close to 1 means future rewards are nearly as valuable as immediate ones, while a lower value makes the agent focus more on immediate rewards.\n",
    "\n",
    "The idea is captured mathematically when computing the **return**.\n",
    "\n",
    "### Return ($G$)\n",
    "The **return** is the total accumulated reward over time, but with future rewards discounted to reflect their delayed benefit. It is given by the formula:\n",
    "\n",
    "$$\n",
    "G = R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\gamma^3 R_4 + \\cdots\n",
    "$$\n",
    "\n",
    "This formula shows that rewards received later are worth less than rewards received immediately, depending on the value of $\\gamma$. For example, if $\\gamma = 0.9$, the reward received two steps later is multiplied by $0.9^2$.\n",
    "\n",
    "### Policy ($\\pi$)\n",
    "A **policy** is a function that maps each state to an action. It defines the agent’s strategy for decision-making. In mathematical terms, a policy is written as:\n",
    "\n",
    "$$\n",
    "\\pi(s) = a\n",
    "$$\n",
    "\n",
    "The objective in reinforcement learning is to find an optimal policy that maximizes the expected return for every state.\n",
    "\n",
    "## The Markov Decision Process (MDP)\n",
    "\n",
    "Reinforcement learning problems are commonly formulated as a **Markov decision process (MDP)**, which provides a formal framework consisting of:\n",
    "\n",
    "- **States:** All possible situations the agent can encounter.\n",
    "- **Actions:** All available decisions or moves.\n",
    "- **Rewards:** Immediate feedback for actions taken.\n",
    "- **Transition Dynamics:** Rules that determine how actions change the state.\n",
    "- **Markov Property:** The principle that the future state depends only on the current state and the action taken, not on the history of past states.\n",
    "\n",
    "This framework allows for a clear mathematical treatment of decision-making problems and helps in designing algorithms that learn effective policies.\n",
    "\n",
    "## Examples for Better Understanding\n",
    "\n",
    "### Autonomous Systems (e.g., Helicopter)\n",
    "- **State:** The helicopter’s position, speed, orientation, and sensor data (such as GPS and accelerometer readings).\n",
    "- **Action:** Adjustments to control sticks.\n",
    "- **Reward:** A small positive reward (e.g., +1 per second) when flying stably, and a large negative reward (e.g., -1000) if it crashes.\n",
    "- **Goal:** To learn a policy that keeps the helicopter flying safely and performing maneuvers like aerobatic stunts.\n",
    "\n",
    "### Mars Rover Navigation\n",
    "Imagine a simplified rover that can be in one of six positions (states 1 through 6):\n",
    "- **States:** Six positions with specific rewards assigned to some states (e.g., state 1 gives a reward of 100, state 6 gives 40, while states 2–5 provide zero reward).\n",
    "- **Actions:** The rover can choose to move left or right.\n",
    "- **Return Calculation:**  \n",
    "  For instance, if the rover starts at state 4 and moves left with a discount factor of $\\gamma = 0.5$, the sequence of rewards (assuming zero rewards for intermediate states until reaching state 1) is weighted as follows:\n",
    "\n",
    "$$\n",
    "G = 0 + 0.5 \\times 0 + 0.5^2 \\times 0 + 0.5^3 \\times 100 = 12.5\n",
    "$$\n",
    "\n",
    "- **Policy:** The strategy might involve choosing actions that lead to state 1 faster to maximize the return.\n",
    "\n",
    "### Game Playing (e.g., Chess)\n",
    "- **State:** The configuration of the chess board.\n",
    "- **Action:** A legal move from the current board position.\n",
    "- **Reward:** Typically defined as +1 for winning, -1 for losing, and 0 for a draw.\n",
    "- **Discount Factor:** Often chosen very close to 1 (e.g., $\\gamma = 0.99$) to emphasize long-term outcomes.\n",
    "- **Policy:** A function that selects the best move from a given board state to maximize the chance of winning the game.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In reinforcement learning, an agent learns to maximize its accumulated reward through:\n",
    "- Observing the **state** of the environment.\n",
    "- Taking an **action** based on a **policy**.\n",
    "- Receiving a **reward** that provides feedback.\n",
    "- Considering future rewards using a **discount factor**.\n",
    "- Evaluating the overall success using the **return**.\n",
    "\n",
    "The problem is typically modeled as a **Markov decision process (MDP)**, where the next state depends solely on the current state and the chosen action. This framework is versatile and can be applied to robotics, autonomous systems, game playing, financial trading, and many other complex decision-making tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bf2047-bc53-4fcb-a400-2ee19f6cbdd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
