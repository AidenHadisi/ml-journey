{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a880dfa-a1ca-4abf-a8f1-ab61bccc17af",
   "metadata": {},
   "source": [
    "# Matrix Properties in Linear Algebra\n",
    "\n",
    "\n",
    "## Frobenius Norm\n",
    "\n",
    "The **Frobenius norm** quantifies the size of a matrix in a way that is analogous to the $L_2$ norm for vectors. For a matrix $X$ with elements $x_{ij}$, the Frobenius norm is defined as:\n",
    "\n",
    "$$\n",
    "\\|X\\|_F = \\sqrt{\\sum_{i,j} x_{ij}^2}\n",
    "$$\n",
    "\n",
    "*Interpretation:** This norm represents the Euclidean distance when the matrix is treated as a long vector containing all its entries.\n",
    "\n",
    "**Example:** For the matrix \n",
    "\n",
    "$$X = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix},$$\n",
    "\n",
    "the Frobenius norm is:\n",
    "\n",
    "$$\n",
    "\\|X\\|_F = \\sqrt{1^2 + 2^2 + 3^2 + 4^2} = \\sqrt{30} \\approx 5.477.\n",
    "$$\n",
    "\n",
    "### Python Example (NumPy)\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "frobenius_norm = np.linalg.norm(X, 'fro')\n",
    "print(\"Frobenius norm:\", frobenius_norm)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Matrix Multiplication\n",
    "\n",
    "Matrix multiplication is a fundamental operation in linear algebra and is extensively used in machine learning.\n",
    "\n",
    "Given:\n",
    "- A matrix $A$ of size $M \\times N$, and\n",
    "- A matrix $B$ of size $N \\times P$,\n",
    "\n",
    "their product $C = AB$ is defined by:\n",
    "\n",
    "$$\n",
    "c_{ik} = \\sum_{j=1}^{N} a_{ij} \\, b_{jk},\n",
    "$$\n",
    "\n",
    "where the resulting matrix $C$ has dimensions $M \\times P$.\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- **Matrix by Vector:** A vector can be seen as a matrix with one column.\n",
    "- **Non-Commutativity:** In general, $AB \\neq BA$. Even when both products are defined (typically when matrices are square), the results can differ.\n",
    "- **Element-wise Process:** Each element in the resulting matrix is computed as a sum of products of corresponding elements.\n",
    "\n",
    "### Python Example (Matrix-by-Vector Multiplication with NumPy)\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([[3, 4],\n",
    "              [5, 6],\n",
    "              [7, 8]])\n",
    "B = np.array([[1],\n",
    "              [2]])\n",
    "C = np.dot(A, B)  # Alternatively, use A @ B\n",
    "print(\"Result of matrix by vector multiplication:\\n\", C)\n",
    "```\n",
    "\n",
    "### Python Example (Matrix-by-Matrix Multiplication)\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([[3, 4],\n",
    "              [5, 6],\n",
    "              [7, 8]])\n",
    "B = np.array([[1, 9],\n",
    "              [2, 0]])\n",
    "C = np.dot(A, B)\n",
    "print(\"Result of matrix by matrix multiplication:\\n\", C)\n",
    "```\n",
    "\n",
    "*Explanation:*  \n",
    "For the first column of $C$, for example, the calculation is:\n",
    "- Row 1: $3 \\times 1 + 4 \\times 2 = 3 + 8 = 11$\n",
    "- Row 2: $5 \\times 1 + 6 \\times 2 = 5 + 12 = 17$\n",
    "- Row 3: $7 \\times 1 + 8 \\times 2 = 7 + 16 = 23$\n",
    "\n",
    "---\n",
    "\n",
    "## Special Matrices\n",
    "\n",
    "### Symmetric Matrices\n",
    "\n",
    "A matrix $S$ is **symmetric** if it equals its transpose:\n",
    "\n",
    "$$\n",
    "S = S^T.\n",
    "$$\n",
    "\n",
    "- **Requirement:** $S$ must be a square matrix.\n",
    "- **Property:** The elements are mirrored across the main diagonal.\n",
    "\n",
    "#### Python Example (NumPy)\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "S = np.array([[1, 2, 3],\n",
    "              [2, 4, 5],\n",
    "              [3, 5, 6]])\n",
    "print(\"Matrix S:\\n\", S)\n",
    "print(\"Transpose of S:\\n\", S.T)\n",
    "```\n",
    "\n",
    "### Identity Matrices\n",
    "\n",
    "An **identity matrix** $I_n$ is a diagonal matrix with ones on the main diagonal and zeros elsewhere:\n",
    "\n",
    "$$\n",
    "I_n = \\begin{bmatrix}\n",
    "1 & 0 & \\cdots & 0 \\\\\n",
    "0 & 1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & 1\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "- **Property:** Multiplying any vector $v$ by $I_n$ returns $v$, i.e., $I_n v = v$.\n",
    "\n",
    "#### Python Example (PyTorch)\n",
    "```python\n",
    "import torch\n",
    "\n",
    "I = torch.eye(3)  # 3x3 identity matrix\n",
    "v = torch.tensor([25, 2, 5], dtype=torch.float32)\n",
    "result = torch.matmul(I, v)\n",
    "print(\"Multiplication with identity matrix:\\n\", result)\n",
    "```\n",
    "\n",
    "### Diagonal Matrices\n",
    "\n",
    "A **diagonal matrix** has nonzero elements only along its main diagonal:\n",
    "\n",
    "$$\n",
    "D = \\operatorname{diag}(d_1, d_2, \\dots, d_n).\n",
    "$$\n",
    "\n",
    "- **Efficient Operations:** Both multiplication and inversion (if no diagonal element is zero) are computationally efficient.\n",
    "\n",
    "**Inversion:** The inverse is given by:\n",
    "\n",
    "$$D^{-1} = \\operatorname{diag}\\left(\\frac{1}{d_1}, \\frac{1}{d_2}, \\dots, \\frac{1}{d_n}\\right),$$\n",
    "\n",
    "provided that $d_i \\neq 0$ for all $i$.\n",
    "\n",
    "---\n",
    "\n",
    "## Matrix Inversion\n",
    "\n",
    "Matrix inversion is a powerful tool for solving systems of linear equations. For a square matrix $X$, its inverse $X^{-1}$ satisfies:\n",
    "\n",
    "$$\n",
    "X^{-1} X = I.\n",
    "$$\n",
    "\n",
    "### Solving Linear Systems\n",
    "\n",
    "For a system given by:\n",
    "\n",
    "$$\n",
    "y = Xw,\n",
    "$$\n",
    "\n",
    "if $X^{-1}$ exists, the weight vector $w$ can be determined by:\n",
    "\n",
    "$$\n",
    "w = X^{-1} y.\n",
    "$$\n",
    "\n",
    "#### Python Example (NumPy)\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Define a 2x2 system for simplicity.\n",
    "X = np.array([[2, -5],\n",
    "              [-3, 4]])\n",
    "y = np.array([400000, -700000])  # e.g., house prices in hundreds of thousands\n",
    "\n",
    "X_inv = np.linalg.inv(X)\n",
    "w = np.dot(X_inv, y)\n",
    "print(\"Solved weights w:\", w)\n",
    "\n",
    "# Verify the solution: y should equal X @ w\n",
    "y_pred = np.dot(X, w)\n",
    "print(\"Predicted y:\", y_pred)\n",
    "```\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Singularity:** A matrix cannot be inverted if it is singular (i.e., its columns are linearly dependent).\n",
    "- **Square Requirement:** Only square matrices (equal number of rows and columns) can have an inverse.\n",
    "- **Overdetermined/Underdetermined Systems:** In these cases, standard inversion is not applicable; methods such as the Moore-Penrose pseudoinverse are used.\n",
    "\n",
    "---\n",
    "\n",
    "## Orthogonal Matrices\n",
    "\n",
    "An **orthogonal matrix** $A$ is composed entirely of orthonormal columns (and rows):\n",
    "\n",
    "$$\n",
    "A^T A = I.\n",
    "$$\n",
    "\n",
    "**Key Property:** The transpose of an orthogonal matrix is its inverse:\n",
    "\n",
    "$$\n",
    "A^T = A^{-1}.\n",
    "$$\n",
    "\n",
    "- **Efficiency:** Calculating the inverse of an orthogonal matrix is computationally inexpensiveâ€”simply compute the transpose.\n",
    "\n",
    "### Verifying Orthogonality\n",
    "\n",
    "For an orthogonal matrix, the dot product between any two distinct columns is zero and each column has a unit norm:\n",
    "- For columns $\\mathbf{a}_i$ and $\\mathbf{a}_j$ (with $i \\neq j$):\n",
    "\n",
    "$$\n",
    "\\langle \\mathbf{a}_i, \\mathbf{a}_j \\rangle = 0.\n",
    "$$\n",
    "\n",
    "- For each column:\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{a}_i\\|_2 = 1.\n",
    "$$\n",
    "\n",
    "#### Python Example (PyTorch)\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Define a matrix K (using float values)\n",
    "K = torch.tensor([[ 2/3,  -2/3,  1/3],\n",
    "                  [ 1/3,   2/3,  2/3],\n",
    "                  [ 2/3,   1/3, -2/3]], dtype=torch.float32)\n",
    "\n",
    "# Compute K^T * K; for an orthogonal matrix, this should be the identity.\n",
    "identity_approx = torch.matmul(K.t(), K)\n",
    "print(\"K^T * K (should be close to identity):\\n\", identity_approx)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Applications in Machine Learning\n",
    "\n",
    "### Regression and Weight Estimation\n",
    "\n",
    "Matrix operations are foundational in linear regression. Given:\n",
    "\n",
    "$$\n",
    "y = Xw,\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $y$ is a vector of outcomes (e.g., house prices),\n",
    "- $X$ is the feature matrix (including a column of ones for the intercept), and\n",
    "- $w$ is the weight vector (parameters to be estimated),\n",
    "\n",
    "if $X^{-1}$ exists, we solve for $w$ using:\n",
    "\n",
    "$$\n",
    "w = X^{-1} y.\n",
    "$$\n",
    "\n",
    "For overdetermined systems (more equations than unknowns), the Moore-Penrose pseudoinverse is often used.\n",
    "\n",
    "### Deep Learning\n",
    "\n",
    "Matrix multiplication is ubiquitous in deep learning:\n",
    "- **Forward Pass:** Input vectors are multiplied by weight matrices.\n",
    "- **Backpropagation:** Derivatives (gradients) are computed using matrix operations.\n",
    "Even in high-level libraries (e.g., PyTorch, TensorFlow), matrix multiplication happens behind the scenes.\n",
    "\n",
    "#### Python Example for Verifying Orthogonality (Identity Matrix)\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "I3 = np.eye(3)\n",
    "col1 = I3[:, 0]\n",
    "col2 = I3[:, 1]\n",
    "col3 = I3[:, 2]\n",
    "\n",
    "print(\"Dot product of col1 and col2:\", np.dot(col1, col2))\n",
    "print(\"Norm of col1:\", np.linalg.norm(col1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d2a6f4-920e-46dc-b47f-9df9a0003def",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
