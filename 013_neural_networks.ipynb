{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "202ade74-0e5a-45fc-bdd0-fc5a2b0f67c9",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Neural networks are advanced computational models inspired by the human brain. They can be thought of as sophisticated forms of logistic regression that automatically learn hierarchical features from data—eliminating the need for manual feature engineering. Instead of pre-selecting features, the network learns the best representations during training.\n",
    "\n",
    "**Key Advantages:**\n",
    "- **Automatic Feature Learning:**  \n",
    "  Neural networks discover and extract useful features directly from raw data.  \n",
    "  *Analogy:* Rather than a chef pre-selecting ingredients, the network experiments with combinations until it finds the best recipe for prediction.\n",
    "- **Flexible Architecture:**  \n",
    "  Design choices include the number of hidden layers and neurons per layer, which can be adapted to the problem at hand.\n",
    "\n",
    "---\n",
    "\n",
    "## Network Architecture\n",
    "\n",
    "A typical neural network consists of several layers:\n",
    "\n",
    "- **Input Layer:**  Receives the raw data as a feature vector.\n",
    "- **Hidden Layers:** Intermediate layers where the network extracts increasingly complex features.\n",
    "- **Output Layer:** Produces the final prediction (e.g., a probability or classification label).\n",
    "\n",
    "**Fully Connected Layers:**  \n",
    "    In these layers, each neuron receives input from every neuron in the previous layer, allowing the network to learn which features are most important.\n",
    "    \n",
    "**Activation Functions:**  \n",
    "    Each neuron applies an activation function (such as the sigmoid function) to a linear combination of its inputs. The sigmoid function is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "where $z$ is the weighted sum of inputs plus a bias term.\n",
    "\n",
    "**Illustration of a Neuron:**\n",
    "- **Linear Combination:**  \n",
    "  For an input vector $\\mathbf{x}$, weights $\\mathbf{w}$, and bias $b$, the neuron computes:\n",
    "\n",
    "$$z = \\mathbf{w} \\cdot \\mathbf{x} + b$$\n",
    "\n",
    "- **Activation:**  \n",
    "  The output (or activation) is:\n",
    "\n",
    "$$a = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "---\n",
    "\n",
    "## Examples\n",
    "\n",
    "### T-Shirt Demand Prediction (Single Feature)\n",
    "\n",
    "Suppose we want to predict whether a T-shirt is a top seller based solely on its price.\n",
    "\n",
    "- **Input Feature:** Price, denoted by $x$.\n",
    "- **Neuron Computation:**  \n",
    "  The neuron calculates a weighted sum plus a bias and passes it through the sigmoid activation:\n",
    "\n",
    "$$\n",
    "a = \\sigma(wx + b) = \\frac{1}{1 + e^{-(wx + b)}}\n",
    "$$\n",
    "\n",
    "  Here, $w$ is the weight, $b$ is the bias, and $a$ represents the probability that the T-shirt is a top seller.\n",
    "\n",
    "### Expanded Case (Multiple Features)\n",
    "Now, consider a model that uses several features:\n",
    "- **Input Features:** Price, Shipping Cost, Marketing Spend, Material Quality.\n",
    "- **Key Factors:**\n",
    "  1. **Affordability:** A function of price and shipping cost.\n",
    "  2. **Awareness:** Driven by marketing spend.\n",
    "  3. **Perceived Quality:** Influenced by material quality and price.\n",
    "\n",
    "**Network Structure:**\n",
    "- **Input Layer:**  \n",
    "  The feature vector is:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = [\\text{Price}, \\text{Shipping Cost}, \\text{Marketing Spend}, \\text{Material Quality}]\n",
    "$$\n",
    "\n",
    "- **Hidden Layer:**  \n",
    "  Contains 3 neurons, each learning one of the key factors (Affordability, Awareness, Perceived Quality).\n",
    "- **Output Layer:**  \n",
    "  A single neuron that combines the hidden layer activations to compute the final probability that the T-shirt is a top seller.\n",
    "\n",
    "**Diagram:**\n",
    "\n",
    "| **Layer**       | **Number of Neurons** | **Role**                                                       |\n",
    "|-----------------|-----------------------|----------------------------------------------------------------|\n",
    "| **Input Layer** | 4                     | Receives raw features.                                         |\n",
    "| **Hidden Layer**| 3                     | Learns intermediate features (Affordability, Awareness, Quality). |\n",
    "| **Output Layer**| 1                     | Outputs the final probability prediction.                      |\n",
    "\n",
    "---\n",
    "\n",
    "### Neural Networks in Computer Vision – Face Recognition\n",
    "\n",
    "Consider training a neural network to recognize a face in an image.\n",
    "\n",
    "- **Image Details:**  \n",
    "  A grayscale image of size $1000 \\times 1000$ pixels. Each pixel has an intensity value from 0 to 255.\n",
    "- **Data Representation:**  \n",
    "  The image is represented as a $1000 \\times 1000$ matrix, which is flattened into a vector:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} \\in \\mathbb{R}^{1\\,000\\,000}\n",
    "$$\n",
    "\n",
    "**Network Architecture for Face Recognition:**\n",
    "- **Input Layer:**  \n",
    "  Receives the flattened pixel intensity vector.\n",
    "- **Hidden Layers:**  \n",
    "  - **First Hidden Layer:** Detects low-level features such as edges (using small image regions).\n",
    "  - **Second Hidden Layer:** Combines edges to form facial parts (like eyes and nose) using larger regions.\n",
    "  - **Third Hidden Layer (optional):** Aggregates parts to recognize complete face shapes.\n",
    "- **Output Layer:**  \n",
    "  Outputs a probability distribution over possible identities (often using a softmax function).\n",
    "\n",
    "| **Layer**         | **Role**                                       | **Learned Features**                                  |\n",
    "|-------------------|------------------------------------------------|-------------------------------------------------------|\n",
    "| **Input Layer**   | Raw pixel intensities                          | N/A                                                   |\n",
    "| **1st Hidden Layer** | Extracts basic features                      | Edges and simple lines                                |\n",
    "| **2nd Hidden Layer** | Combines features into facial parts          | Eyes, nose, and other facial features                |\n",
    "| **3rd Hidden Layer** | Aggregates parts into full face shapes       | Complete facial structure (if used)                   |\n",
    "| **Output Layer**  | Classifies the image into a person’s identity  | Identity probabilities (via softmax)                  |\n",
    "\n",
    "*Tip:* The network learns these features automatically from the training data.\n",
    "\n",
    "---\n",
    "\n",
    "### Handwritten Digit Recognition\n",
    "\n",
    "- **Task:** Classify an $8 \\times 8$ grayscale image as either the digit 0 or 1.\n",
    "- **Input:**  \n",
    "  The image is flattened into a 64-dimensional vector:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} \\in \\mathbb{R}^{64}\n",
    "$$\n",
    "\n",
    "- **Neural Network Architecture:**\n",
    "  - **Input Layer (Layer 0):** 64 features.\n",
    "  - **Hidden Layer 1 (Layer 1):** 25 neurons.\n",
    "  - **Hidden Layer 2 (Layer 2):** 15 neurons.\n",
    "  - **Output Layer (Layer 3):** 1 neuron that outputs a probability for the digit 1.\n",
    "\n",
    "## Forward Propagation Process\n",
    "\n",
    "Forward propagation computes the network output by passing the input through each layer:\n",
    "\n",
    "1. **From Input to First Hidden Layer:**\n",
    "\n",
    "$$\n",
    "\\mathbf{a}^{[1]} = \\sigma\\left(W^{[1]} \\mathbf{x} + b^{[1]}\\right)\n",
    "$$\n",
    "\n",
    "   - $W^{[1]}$ is the weight matrix and $b^{[1]}$ is the bias vector for Layer 1.\n",
    "\n",
    "2. **From First to Second Hidden Layer:**\n",
    "\n",
    "$$\n",
    "\\mathbf{a}^{[2]} = \\sigma\\left(W^{[2]} \\mathbf{a}^{[1]} + b^{[2]}\\right)\n",
    "$$\n",
    "\n",
    "3. **From Second Hidden Layer to Output Layer:**\n",
    "\n",
    "$$\n",
    "a^{[3]} = \\sigma\\left(W^{[3]} \\mathbf{a}^{[2]} + b^{[3]}\\right)\n",
    "$$\n",
    "\n",
    "   - Here, $a^{[3]}$ is a scalar representing the probability that the image is the digit 1.\n",
    "\n",
    "4. **Prediction:**  \n",
    "   A threshold is applied (commonly 0.5) to convert the probability into a binary label:\n",
    "   - If $a^{[3]} \\ge 0.5$, predict **1**.\n",
    "   - Otherwise, predict **0**.\n",
    "\n",
    "**Summary of Forward Propagation:**\n",
    "- **Step 1:** Process input features through the network layer by layer.\n",
    "- **Step 2:** Compute activations using:\n",
    "\n",
    "$$a_j^{[l]} = \\sigma\\left(w_j^{[l]} \\cdot \\mathbf{a}^{[l-1]} + b_j^{[l]}\\right)$$\n",
    "\n",
    "- Where $l$ is the layer number and $j$ is the $j^{th}$ neuron in that layer.\n",
    "    \n",
    "- **Step 3:** Use the final activation to make a prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Computation and Notation\n",
    "\n",
    "Understanding the computation within each layer is crucial. The following sections summarize the mathematical foundations and notation conventions used in neural networks.\n",
    "\n",
    "### Neuron Computation\n",
    "\n",
    "Each neuron performs two key operations:\n",
    "1. **Linear Combination:**\n",
    "\n",
    "$$\n",
    "z = \\mathbf{w} \\cdot \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "   where:\n",
    "   - $\\mathbf{x}$ is the input vector.\n",
    "   - $\\mathbf{w}$ is the weight vector.\n",
    "   - $b$ is the bias term.\n",
    "2. **Activation:**\n",
    "\n",
    "$$\n",
    "a = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "   The activation function $\\sigma(\\cdot)$ (here, the sigmoid) determines the neuron's output.\n",
    "\n",
    "### Notation Conventions\n",
    "\n",
    "- **Layer Indexing:**  \n",
    "  The input layer is denoted as $\\mathbf{a}^{[0]}$, while subsequent layers are labeled with superscripts in square brackets. For example:\n",
    "  - $\\mathbf{a}^{[1]}$ is the activation vector of the first hidden layer.\n",
    "  - $W^{[l]}$ and $b^{[l]}$ denote the weights and biases for Layer $l$.\n",
    "- **Neuron-Specific Parameters:**  \n",
    "  For the $j^\\text{th}$ neuron in layer $l$, the activation is:\n",
    "\n",
    "$$\n",
    "a_j^{[l]} = \\sigma\\left(w_j^{[l]} \\cdot \\mathbf{a}^{[l-1]} + b_j^{[l]}\\right)\n",
    "$$\n",
    "\n",
    "  where $w_j^{[l]}$ is the weight vector and $b_j^{[l]}$ is the bias for that neuron.\n",
    "\n",
    "### Example Computation in a Hidden Layer\n",
    "\n",
    "Suppose a hidden layer has three neurons. For each neuron $i$:\n",
    "\n",
    "1. **Compute the Linear Combination:**\n",
    "\n",
    "$$\n",
    "z_i^{[1]} = w_i^{[1]} \\cdot \\mathbf{x} + b_i^{[1]}\n",
    "$$\n",
    "\n",
    "2. **Apply the Activation Function:**\n",
    "\n",
    "$$\n",
    "a_i^{[1]} = \\sigma(z_i^{[1]})\n",
    "$$\n",
    "   \n",
    "If the computed activations are:\n",
    "\n",
    "$$\n",
    "a_1^{[1]} \\approx 0.3,\\quad a_2^{[1]} \\approx 0.7,\\quad a_3^{[1]} \\approx 0.2,\n",
    "$$\n",
    "\n",
    "the activation vector for this layer is:\n",
    "\n",
    "$$\n",
    "\\mathbf{a}^{[1]} = \\begin{bmatrix} 0.3 \\\\ 0.7 \\\\ 0.2 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "This vector is then used as the input to the next layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358c460-d672-42ee-bdd2-42a6b8a1085d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
