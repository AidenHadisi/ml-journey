{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e327aa5-47e1-417c-9cf4-9974bb27747f",
   "metadata": {},
   "source": [
    "# Backpropagation & Computation Graphs\n",
    "\n",
    "## Derivatives: Intuition and Calculation\n",
    "\n",
    "Imagine a simple cost function:  \n",
    "\n",
    "$$\n",
    "J(w) = w^2.\n",
    "$$\n",
    "\n",
    "At $w=3$, we have $J(3)=9$. If we increase $w$ by a small amount $\\epsilon$ (for example, $\\epsilon=0.001$), then $w$ becomes $3.001$, and  \n",
    "\n",
    "$$\n",
    "J(3.001) \\approx 9.006001.\n",
    "$$\n",
    "\n",
    "The change in $J$ is roughly $0.006001$, which is about $6\\epsilon$. This tells us that for a tiny change in $w$, the cost changes by approximately  \n",
    "\n",
    "$$\n",
    "\\frac{dJ}{dw} \\approx 6,\n",
    "$$\n",
    "\n",
    "when $w=3$. More generally, by calculus we know:  \n",
    "\n",
    "$$\n",
    "\\frac{dJ}{dw} = 2w.\n",
    "$$\n",
    "\n",
    "Thus, when $w=2$, $\\frac{dJ}{dw}=4$, and when $w=-3$, $\\frac{dJ}{dw}=-6$. The derivative tells us how sensitive $J$ is to changes in $w$, much like a hill's slope indicates how steep it is.\n",
    "\n",
    "---\n",
    "\n",
    "## The Computation Graph Concept\n",
    "\n",
    "A computation graph represents the sequence of operations in a neural network. Instead of writing one long equation, the networkâ€™s operations are broken into smaller, manageable steps. Consider a simple network defined by:  \n",
    "\n",
    "$$\n",
    "a = wx + b,\n",
    "$$\n",
    "\n",
    "with a cost function:  \n",
    "\n",
    "$$\n",
    "J = \\frac{1}{2}(a - y)^2.\n",
    "$$\n",
    "\n",
    "We can break this computation into these steps:\n",
    "- **Step 1:** Compute an intermediate value $c = w \\times x$.\n",
    "- **Step 2:** Compute the output $a = c + b$.\n",
    "- **Step 3:** Compute the error $d = a - y$.\n",
    "- **Step 4:** Compute the cost $J = \\frac{1}{2}d^2$.\n",
    "\n",
    "During **forward propagation**, we calculate these steps from input to output. In **backpropagation**, we reverse the process, starting from the cost $J$ and working backwards. At each node, we use the chain rule to determine how a small change in that node affects $J$. For instance, a change in $d$ affects $J$ by  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial d} = d \\quad \\text{(or, more precisely, $d$ times a constant factor depending on the cost function)}.\n",
    "$$\n",
    "\n",
    "This information is then used to compute the gradient with respect to earlier variables, eventually yielding the gradients with respect to $w$ and $b$.\n",
    "\n",
    "---\n",
    "\n",
    "## A Larger Neural Network Example\n",
    "\n",
    "Consider a neural network with one hidden layer. Suppose we have:\n",
    "\n",
    "Input $x = 1$ and target $y = 5$.\n",
    "\n",
    "Hidden layer computation:  \n",
    "\n",
    "$$z_1 = w_1 \\cdot x + b_1,\\quad a_1 = g(z_1),$$  \n",
    "  \n",
    "- where $g(z)$ is the ReLU activation $g(z)=\\max(0,z)$. If $w_1=2$ and $b_1=0$, then $z_1=2$ and $a_1=2$.\n",
    "\n",
    "Output layer computation:  \n",
    "\n",
    "$$z_2 = w_2 \\cdot a_1 + b_2,\\quad a_2 = g(z_2).$$  \n",
    "  \n",
    "- If $w_2=3$ and $b_2=1$, then $z_2=7$ and $a_2=7$.\n",
    "\n",
    "Cost calculation:  \n",
    "\n",
    "$$J = \\frac{1}{2}(a_2 - y)^2 = \\frac{1}{2}(7-5)^2 = 2.$$\n",
    "\n",
    "The computation graph now includes nodes for:\n",
    "- Multiplying $w_1$ and $x$,\n",
    "- Adding $b_1$ to get $z_1$,\n",
    "- Applying ReLU to obtain $a_1$,\n",
    "- Multiplying $w_2$ and $a_1$,\n",
    "- Adding $b_2$ to get $z_2$,\n",
    "- Applying ReLU to obtain $a_2$, and\n",
    "- Computing the final cost $J$.\n",
    "\n",
    "Backpropagation begins at the cost $J$ and computes gradients at each node using the chain rule. For example, a small increase in $w_1$ will affect $z_1$, then $a_1$, and so on until it changes $J$. By combining these effects, we can calculate $\\frac{\\partial J}{\\partial w_1}$ accurately. A numerical check (increasing $w_1$ by $\\epsilon$ and observing the change in $J$) confirms the calculated gradient.\n",
    "\n",
    "---\n",
    "\n",
    "## Efficiency and Automatic Differentiation\n",
    "\n",
    "Backpropagation is efficient because it reuses intermediate gradient calculations rather than computing them independently for every parameter. In a network with $n$ nodes and $p$ parameters, backpropagation takes roughly $O(n+p)$ steps rather than $O(n \\times p)$, which is essential for large models.\n",
    "\n",
    "Modern frameworks like TensorFlow and PyTorch implement **automatic differentiation (autodiff)**. Autodiff automatically constructs the computation graph and applies backpropagation to compute all the necessary gradients. This automation spares researchers from manually deriving gradients and helps in building complex models more reliably.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5367ce-c35d-4981-98cb-4e6c808e83de",
   "metadata": {},
   "source": [
    "# Detailed Backpropagation Example with the Chain Rule\n",
    "\n",
    "\n",
    "\n",
    "## The Example Network\n",
    "\n",
    "Consider a network with one input, one weight, and one bias. The network computes an output $a$ from an input $x$ as follows:\n",
    "\n",
    "$$\n",
    "a = wx + b.\n",
    "$$\n",
    "\n",
    "The cost function (or loss) is given by:\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{2}(a - y)^2,\n",
    "$$\n",
    "\n",
    "- where $y$ is the true target value.\n",
    "\n",
    "---\n",
    "\n",
    "## Breaking Down the Computation: The Computation Graph\n",
    "\n",
    "We can break the computation into four sequential steps (nodes):\n",
    "\n",
    "1. **Compute the weighted input:**  \n",
    "\n",
    "$$ c = w \\times x $$\n",
    "\n",
    "2. **Add the bias:**  \n",
    "\n",
    "$$ a = c + b $$\n",
    "\n",
    "3. **Compute the error (difference):**  \n",
    "\n",
    "$$ d = a - y $$\n",
    "\n",
    "4. **Compute the cost:**  \n",
    "\n",
    "$$ J = \\frac{1}{2}d^2 $$\n",
    "\n",
    "A simplified diagram of the computation graph is:\n",
    "\n",
    "```\n",
    "   w, x         b, y\n",
    "    |            |\n",
    "    v            |\n",
    "   [*]          (constant y)\n",
    "    |            |\n",
    "    v            |\n",
    "   (c = w*x)  --> (+)  -->  a = c + b  -->  [-]  -->  d = a - y  -->  [Square]  -->  J\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Applying the Chain Rule in Backpropagation\n",
    "\n",
    "The **chain rule** lets us compute the derivative of a composite function. If a variable $z$ depends on $y$, and $y$ depends on $x$, then:\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}.\n",
    "$$\n",
    "\n",
    "Our goal is to compute the gradient $\\frac{\\partial J}{\\partial w}$, i.e., how a small change in $w$ affects the cost $J$.\n",
    "\n",
    "### Step-by-Step Backpropagation\n",
    "\n",
    "**Step 1: From $J$ to $d$**\n",
    "\n",
    "At Node 4, the cost is:\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{2}d^2.\n",
    "$$\n",
    "\n",
    "Differentiate with respect to $d$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial d} = d.\n",
    "$$\n",
    "\n",
    "- *Interpretation:* A small change $\\delta d$ in $d$ causes a change in $J$ of approximately $d \\cdot \\delta d$.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 2: From $d$ to $a$**\n",
    "\n",
    "At Node 3, the error is:\n",
    "\n",
    "$$\n",
    "d = a - y.\n",
    "$$\n",
    "\n",
    "Differentiate with respect to $a$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial d}{\\partial a} = 1.\n",
    "$$\n",
    "\n",
    "By the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial a} = \\frac{\\partial J}{\\partial d} \\cdot \\frac{\\partial d}{\\partial a} = d \\cdot 1 = d.\n",
    "$$\n",
    "\n",
    "- *Interpretation:* Changes in $a$ propagate directly to $d$.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 3: From $a$ to $c$**\n",
    "\n",
    "At Node 2, we have:\n",
    "\n",
    "$$\n",
    "a = c + b.\n",
    "$$\n",
    "\n",
    "Differentiate with respect to $c$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial c} = 1.\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial c} = \\frac{\\partial J}{\\partial a} \\cdot \\frac{\\partial a}{\\partial c} = d \\cdot 1 = d.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 4: From $c$ to $w$**\n",
    "\n",
    "At Node 1, $c$ is computed as:\n",
    "\n",
    "$$\n",
    "c = w \\times x.\n",
    "$$\n",
    "\n",
    "Differentiate with respect to $w$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial c}{\\partial w} = x.\n",
    "$$\n",
    "\n",
    "Finally, by the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w} = \\frac{\\partial J}{\\partial c} \\cdot \\frac{\\partial c}{\\partial w} = d \\cdot x.\n",
    "$$\n",
    "\n",
    "- *Interpretation:* The sensitivity of $J$ with respect to $w$ is the product of the propagated error $d$ and the input $x$.\n",
    "\n",
    "---\n",
    "\n",
    "**Bonus: Gradient with Respect to $b$**\n",
    "\n",
    "Since $a = c + b$, differentiating with respect to $b$ gives:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial b} = 1.\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial a} \\cdot \\frac{\\partial a}{\\partial b} = d \\cdot 1 = d.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Updating Parameters Using Gradients\n",
    "\n",
    "Once we have the gradients, we update the parameters using a method such as gradient descent. For a learning rate $\\alpha$, the update rules are:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\alpha \\frac{\\partial J}{\\partial w} \\quad \\text{and} \\quad b \\leftarrow b - \\alpha \\frac{\\partial J}{\\partial b}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Recap: The Role of the Chain Rule\n",
    "\n",
    "The chain rule allows us to break a complex derivative into simpler parts. In our example:\n",
    "- We first computed how $J$ changes with $d$.\n",
    "- Then, we saw how changes in $a$ affect $d$, and so on.\n",
    "- Multiplying these derivatives together gave us the gradient with respect to $w$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w} = \\frac{\\partial J}{\\partial d} \\cdot \\frac{\\partial d}{\\partial a} \\cdot \\frac{\\partial a}{\\partial c} \\cdot \\frac{\\partial c}{\\partial w} = d \\cdot 1 \\cdot 1 \\cdot x = d \\cdot x.\n",
    "$$\n",
    "\n",
    "This systematic backtracking through the computation graph is the essence of backpropagation. It enables neural networks to efficiently compute gradients and update parameters during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b4756-5acc-4b1e-81bd-ccb560d206f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
