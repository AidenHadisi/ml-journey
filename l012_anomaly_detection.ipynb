{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a1b5f2-ace4-43dc-8d76-501b56d88518",
   "metadata": {},
   "source": [
    "# Anomaly Detection & Gaussian Models\n",
    "\n",
    "Anomaly detection is an unsupervised learning technique that models the behavior of “normal” data and flags unusual examples that may indicate a problem. The core idea is to learn a probability model from a dataset of typical (unlabeled) events. When a new data point (e.g., a newly manufactured aircraft engine) is evaluated, the algorithm computes its probability under the learned model. If the probability is below a chosen threshold (denoted by $\\epsilon$), the example is flagged as anomalous.\n",
    "\n",
    "### Practical Example: Aircraft Engines\n",
    "**Context:** Aircraft engine reliability is critical. Even a small number of defective engines can have severe consequences.\n",
    "\n",
    "**Features:** Consider measurable features such as:\n",
    "- $x_1$: Heat generated by the engine.\n",
    "- $x_2$: Vibration intensity.\n",
    "\n",
    "**Process:**  \n",
    "1. **Training:** Collect $m$ examples (most of which are normal) and compute features.\n",
    "2. **Modeling:** Learn the probability distribution $p(x)$ over these features.\n",
    "3. **Testing:** For a new engine with feature vector $X_{\\text{test}}$, compute $p(X_{\\text{test}})$.  \n",
    "    - If $p(X_{\\text{test}}) < \\epsilon$, flag as anomalous.\n",
    "    - Otherwise, consider the engine normal.\n",
    "\n",
    "This method generalizes to many domains, such as fraud detection, monitoring data centers, and quality control in manufacturing.\n",
    "\n",
    "---\n",
    "\n",
    "## Density Estimation with the Gaussian Distribution\n",
    "\n",
    "The most common approach to anomaly detection is **density estimation**. By modeling the probability distribution of the training data, one can decide whether a new example fits within the “normal” regions.\n",
    "\n",
    "### The Gaussian (Normal) Distribution\n",
    "\n",
    "When $x$ is a random variable, a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$ is defined as:\n",
    "  \n",
    "$$p(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\n",
    "  \n",
    "- **Mean ($\\mu$):** The center of the distribution.\n",
    "- **Standard Deviation ($\\sigma$):** Determines the “width” or spread.\n",
    "- **Variance ($\\sigma^2$):** The square of the standard deviation.\n",
    "  \n",
    "- **Visual Analogy:** Think of the classic bell-shaped curve, similar to the outline of a traditional bell (or even the Liberty Bell, in a loose visual analogy). The peak of the bell is at $\\mu$, and the curve’s width is determined by $\\sigma$.\n",
    "\n",
    "### Effects of Changing Parameters\n",
    "\n",
    "**Changing $\\sigma$:**\n",
    "- A smaller $\\sigma$ (e.g., 0.5) results in a narrow, taller curve (because the area under the curve remains 1).\n",
    "- A larger $\\sigma$ (e.g., 2) results in a wider, shorter curve.\n",
    "\n",
    "**Changing $\\mu$:**\n",
    "- Shifts the entire distribution left or right without affecting the shape.\n",
    "\n",
    "### Estimating Parameters from Data\n",
    "\n",
    "For a training set with $m$ examples:\n",
    "\n",
    "**Mean Estimate:**\n",
    "  \n",
    "$$\\mu = \\frac{1}{m} \\sum_{i=1}^{m} x^{(i)}$$\n",
    "  \n",
    "**Variance Estimate:**\n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{m} \\sum_{i=1}^{m} \\left(x^{(i)} - \\mu\\right)^2$$\n",
    "\n",
    "These are the **maximum likelihood estimates** for the Gaussian parameters. (Some texts use $\\frac{1}{m-1}$ for an unbiased estimate, but the difference is usually negligible in practice.)\n",
    "\n",
    "---\n",
    "\n",
    "## Multi-Feature Anomaly Detection\n",
    "\n",
    "In real-world applications, each example is a vector of $n$ features. For instance, an aircraft engine might be characterized by more than just heat and vibration. The anomaly detection model assumes that each feature is independent, allowing us to write:\n",
    "\n",
    "$$\n",
    "p(x) = \\prod_{j=1}^{n} p(x_j; \\mu_j, \\sigma_j^2)\n",
    "$$\n",
    "\n",
    "- **Interpretation:**  \n",
    "  The probability of the entire feature vector is the product of the individual Gaussian probabilities for each feature.\n",
    "- **Parameter Estimation:**  \n",
    "  For each feature $x_j$, calculate its own $\\mu_j$ and $\\sigma_j^2$ using the same formulas as above.\n",
    "- **Testing New Examples:**  \n",
    "  For a new vector $x$, compute $p(x)$ by plugging in the values for each feature. If $p(x) < \\epsilon$, the example is flagged as anomalous.\n",
    "\n",
    "### Intuitive Example:\n",
    "Imagine a scenario where:\n",
    "- There is a 1/10 chance that an engine is unusually hot ($x_1$) and a 1/20 chance that it vibrates unusually hard ($x_2$).\n",
    "- The joint probability (assuming independence) is $\\frac{1}{10} \\times \\frac{1}{20} = \\frac{1}{200}$, which is very low and would indicate a high likelihood of anomaly.\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluating the Anomaly Detection System\n",
    "\n",
    "### Using a Cross-Validation Set\n",
    "While the training set consists of unlabeled data (assumed normal), a small number of labeled anomalies (positive examples, $y=1$) can be used in a cross-validation set to tune parameters like $\\epsilon$.\n",
    "\n",
    "**Dataset Splitting Example:**\n",
    "- **Training Set:** 6,000 normal examples.\n",
    "- **Cross-Validation Set:** 2,000 normal examples + 10 known anomalies.\n",
    "- **Test Set:** Another 2,000 normal examples + 10 known anomalies.\n",
    "  \n",
    "**Evaluation Metrics:**\n",
    "- Compute predictions using the rule:\n",
    "- Predict $y=1$ (anomaly) if $p(x) < \\epsilon$.\n",
    "- Predict $y=0$ (normal) otherwise.\n",
    "- Use metrics such as precision, recall, and $F_1$ score, especially since the dataset is highly imbalanced.\n",
    "\n",
    "### Handling Skewed Data\n",
    "Anomaly detection often involves very few positive examples relative to negatives. This imbalance means standard accuracy may not be the best measure. Instead, evaluate:\n",
    "- **True Positive Rate (TPR)**\n",
    "- **False Positive Rate (FPR)**\n",
    "- **Precision and Recall**\n",
    "- **$F_1$ Score**\n",
    "\n",
    "The aim is to choose an $\\epsilon$ that minimizes false alarms (flagging normal examples) while still catching anomalies.\n",
    "\n",
    "---\n",
    "\n",
    "## Anomaly Detection vs. Supervised Learning\n",
    "\n",
    "### When to Use Anomaly Detection:\n",
    "- **Few Positive Examples:** When there are very few anomalies (often 0–20 examples).\n",
    "- **New/Unknown Anomalies:** When future anomalies might differ significantly from known cases.\n",
    "- **Modeling Normal Behavior:** The model is built solely on normal data, flagging deviations.\n",
    "\n",
    "### When to Use Supervised Learning:\n",
    "- **Sufficient Labeled Data:** When there are enough examples of both normal and anomalous cases.\n",
    "- **Predictable Patterns:** When future positive examples are expected to resemble the historical positive examples.\n",
    "  \n",
    "**Example:**  \n",
    "- **Financial Fraud:** New types of fraud may be unpredictable, making anomaly detection a better fit.\n",
    "- **Email Spam Detection:** Spam emails tend to share characteristics over time; supervised learning can be more effective.\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Engineering for Anomaly Detection\n",
    "\n",
    "Choosing the right features is crucial because anomaly detection algorithms work solely from the structure of normal data. Unlike supervised learning, they cannot “learn” which features to ignore from labeled examples.\n",
    "\n",
    "### Making Features More Gaussian\n",
    "\n",
    "Many anomaly detection models assume that the data follows a Gaussian distribution. If a feature's distribution is skewed, you can transform it:\n",
    "- **Log Transformation:**  \n",
    "  Replace $x$ with $\\log(x)$ (add a small constant to avoid $\\log(0)$). This can often transform a skewed distribution into one that is more symmetric.\n",
    "  \n",
    "- **Power Transformations:**  \n",
    "  For example, using $x^{0.4}$ or $x^{1/2}$ can sometimes yield a distribution that is closer to Gaussian.\n",
    "  \n",
    "- **Practical Tip:**  \n",
    "  Plot histograms of your raw and transformed features. Experiment with different transformations until the distribution appears symmetric (bell-shaped). Remember to apply the same transformation to the training, cross-validation, and test sets.\n",
    "\n",
    "### Creating New Features\n",
    "\n",
    "Sometimes a single feature may not reveal anomalies. You might need to combine features:\n",
    "- **Example:**  \n",
    "  In monitoring computers in a data center:\n",
    "  - **Original Features:** CPU load, network traffic.\n",
    "  - **New Feature:** The ratio of CPU load to network traffic, which might highlight anomalies that are not visible when looking at each feature individually.\n",
    "  \n",
    "This process of iterative feature tuning is essential for improving the anomaly detection system.\n",
    "\n",
    "---\n",
    "\n",
    "## Development Process Recap\n",
    "\n",
    "1. **Feature Selection:**  \n",
    "   Identify and engineer features that capture key aspects of normal behavior.\n",
    "2. **Model Training:**  \n",
    "   Estimate Gaussian parameters ($\\mu_j$, $\\sigma_j^2$) for each feature using normal data.\n",
    "3. **Thresholding:**  \n",
    "   Set a threshold $\\epsilon$ such that if $p(x) < \\epsilon$, flag the example as anomalous.\n",
    "4. **Evaluation:**  \n",
    "   Use a cross-validation set (with a few anomalies) to tune $\\epsilon$ and assess performance with appropriate metrics.\n",
    "5. **Error Analysis:**  \n",
    "   Examine false negatives (missed anomalies) and false positives (normal examples flagged) to inspire new feature transformations or additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27fb572-9f43-41f0-b0bb-8409b86961c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
