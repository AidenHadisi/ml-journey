{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e926de7-5a9b-4e7b-9ffa-adb3ec085573",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision trees are a popular type of model used in machine learning for both classification (predicting discrete categories) and regression (predicting continuous numbers). They work by recursively splitting a dataset based on feature values to create a model that is easy to interpret and visualize. Despite being very powerful and widely used in real-world applications (including many machine learning competitions), decision trees sometimes receive less academic attention than other methods.\n",
    "\n",
    "*Example Context:*  \n",
    "Imagine running a cat adoption center. You have a dataset with 10 examples where each animal is described by features such as **ear shape**, **face shape**, and **whiskers**. The task is to classify each animal as a cat or not a cat. In this example, the input features ($X$) are categorical (e.g., \"pointy\" vs. \"floppy\" ears) and the target variable ($Y$) is binary (cat = 1, not cat = 0).\n",
    "\n",
    "---\n",
    "\n",
    "## Structure of a Decision Tree\n",
    "\n",
    "A decision tree consists of several types of nodes that form a branching structure:\n",
    "\n",
    "- **Root Node:**  \n",
    "  The topmost node that represents the entire dataset. All examples start here.\n",
    "\n",
    "- **Decision Nodes:**  \n",
    "  These are the internal nodes (often depicted as ovals) where a test is performed on one of the features. Depending on the outcome (e.g., \"pointy ear\" vs. \"floppy ear\"), the example is sent down different branches.\n",
    "\n",
    "- **Leaf Nodes:**  \n",
    "  Terminal nodes (often depicted as rectangles) that provide a final prediction. For classification, a leaf might say \"cat\" or \"not cat\"; for regression, it might output an average value (e.g., predicted weight).\n",
    "\n",
    "*Clarification:*  \n",
    "Although we call it a \"tree,\" its structure is more like a flowchart. Think of it as asking a series of yes/no questions until you reach a conclusion. The convention of having the \"root\" at the top and \"leaves\" at the bottom is just a diagrammatic choice—similar to how indoor hanging plants display their roots above ground.\n",
    "\n",
    "---\n",
    "\n",
    "## How to Build a Decision Tree\n",
    "\n",
    "The process of constructing a decision tree from a training set involves two main challenges:\n",
    "\n",
    "### Choosing a Feature to Split On\n",
    "\n",
    "At each node, you must decide which feature to use to divide the data. For instance, at the root node you might decide to split based on **ear shape**:\n",
    "- All examples with **pointy ears** go to the left branch.\n",
    "- All examples with **floppy ears** go to the right branch.\n",
    "\n",
    "This decision is made by evaluating which feature best separates the classes (i.e., makes the groups as \"pure\" as possible).\n",
    "\n",
    "### Deciding When to Stop Splitting\n",
    "\n",
    "You cannot split indefinitely. Common stopping criteria include:\n",
    "- **Pure Node:** Stop if all examples at a node belong to a single class (i.e., the node is pure).\n",
    "- **Maximum Depth:** Set a limit on how many splits (or “hops” from the root) are allowed to prevent overly complex trees.\n",
    "- **Minimal Information Gain:** If a potential split does not reduce impurity by a significant amount, you may decide to stop.\n",
    "- **Minimum Sample Size:** If a node contains too few examples, further splitting might be unreliable.\n",
    "\n",
    "---\n",
    "\n",
    "## Measuring Purity with Entropy\n",
    "\n",
    "To decide the best split, you need a way to quantify the “purity” of a set of examples. **Entropy** is a measure of impurity in a dataset.\n",
    "\n",
    "### Entropy Definition\n",
    "\n",
    "Let $p_1$ be the fraction of positive examples (e.g., cats). Then the entropy $H(p_1)$ is defined as:\n",
    "\n",
    "$$\n",
    "H(p_1) = -p_1 \\log_2(p_1) - (1-p_1) \\log_2(1-p_1)\n",
    "$$\n",
    "\n",
    "- **Maximum Entropy:**  \n",
    "  When $p_1 = 0.5$, the classes are evenly mixed and $H(0.5) = 1$. This is the worst-case (most impure) scenario.\n",
    "- **Minimum Entropy:**  \n",
    "  When $p_1 = 0$ or $1$, the node is pure and $H(0) = H(1) = 0$.\n",
    "\n",
    "*Example:*  \n",
    "- If you have 3 cats and 3 dogs ($p_1 = 0.5$), the entropy is $1$.\n",
    "- If you have 5 cats and 1 dog ($p_1 \\approx 0.83$), the entropy is lower (around $0.65$).\n",
    "- If all examples are cats ($p_1 = 1$), the entropy is $0$.\n",
    "\n",
    "---\n",
    "\n",
    "## Information Gain: Choosing the Best Split\n",
    "\n",
    "**Information Gain (IG)** measures the reduction in entropy after a split. It tells you how much more “pure” the resulting subsets are compared to the original set.\n",
    "\n",
    "### Information Gain Formula\n",
    "\n",
    "If you split a node into two branches (left and right), the information gain is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{IG} = H(p_1^\\text{root}) - \\left( w^\\text{left} \\cdot H(p_1^\\text{left}) + w^\\text{right} \\cdot H(p_1^\\text{right}) \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $H(p_1^\\text{root})$ is the entropy at the root node.\n",
    "- $H(p_1^\\text{left})$ and $H(p_1^\\text{right})$ are the entropies of the left and right subsets.\n",
    "- $w^\\text{left}$ and $w^\\text{right}$ are the proportions of examples in the left and right branches.\n",
    "\n",
    "*Practical Example:*  \n",
    "Suppose at the root node, there are 10 examples (5 cats, 5 dogs), so $p_1^\\text{root} = 0.5$ and $H(0.5)=1$. Now, consider splitting on **ear shape**:\n",
    "- **Left branch (pointy ears):** 5 examples with 4 cats ($p_1^\\text{left} = 0.8$, entropy roughly $0.72$).\n",
    "- **Right branch (floppy ears):** 5 examples with 1 cat ($p_1^\\text{right} = 0.2$, entropy roughly $0.72$).\n",
    "- With weights $w^\\text{left}=w^\\text{right}=0.5$, the weighted entropy is $0.5 \\times 0.72 + 0.5 \\times 0.72 = 0.72$.\n",
    "- Information gain is then $1 - 0.72 = 0.28$.\n",
    "\n",
    "The algorithm computes the information gain for each possible feature split and selects the one with the highest gain.\n",
    "\n",
    "---\n",
    "\n",
    "## Handling Different Feature Types\n",
    "\n",
    "Decision trees can handle various types of input features, but different methods are used depending on whether the feature is categorical or continuous.\n",
    "\n",
    "### Categorical Features\n",
    "\n",
    "#### Binary Features\n",
    "For features that take on only two values (e.g., \"whiskers\" can be either present or absent), the tree splits directly on the binary condition.\n",
    "\n",
    "#### Multi-valued Features and One-Hot Encoding\n",
    "When a feature has more than two possible values (e.g., **ear shape** might be \"pointy\", \"floppy\", or \"oval\"), one common approach is **one-hot encoding**:\n",
    "- Replace the single multi-valued feature with $k$ binary features (where $k$ is the number of possible values).\n",
    "- For example, if ear shape has three values, create three new features:\n",
    "  - **Pointy Ear:** $1$ if pointy, otherwise $0$.\n",
    "  - **Floppy Ear:** $1$ if floppy, otherwise $0$.\n",
    "  - **Oval Ear:** $1$ if oval, otherwise $0$.\n",
    "- Each example will have exactly one of these features set to $1$ (\"hot\") and the others set to $0$.\n",
    "\n",
    "*Benefit:*  \n",
    "This transformation allows the decision tree algorithm to work with each feature as a simple binary indicator.\n",
    "\n",
    "### Continuous Features\n",
    "\n",
    "Continuous features (like **weight**) can take any numerical value. The decision tree must choose an optimal threshold to split the data.\n",
    "\n",
    "#### How to Split on Continuous Features:\n",
    "1. **Sort the Examples:**  \n",
    "   Arrange the data points by the continuous feature (e.g., weight).\n",
    "\n",
    "2. **Consider Candidate Thresholds:**  \n",
    "   Use the midpoints between consecutive values as potential thresholds.\n",
    "\n",
    "3. **Evaluate Information Gain:**  \n",
    "   For each candidate threshold, split the data into two subsets (e.g., weight $\\leq t$ and weight $> t$) and calculate the resulting weighted entropy. Choose the threshold that maximizes information gain.\n",
    "\n",
    "*Example:*  \n",
    "For weight, testing thresholds like $8$, $9$, or $13$ might result in different splits. If splitting at $t=9$ gives the highest reduction in entropy (say, information gain of $0.61$ compared to lower gains at $t=8$ or $t=13$), then the threshold $9$ is chosen.\n",
    "\n",
    "---\n",
    "\n",
    "## Regression Trees: Predicting Continuous Values\n",
    "\n",
    "While classification trees predict categories, **regression trees** are designed to predict continuous values (e.g., predicting an animal's weight).\n",
    "\n",
    "### Key Differences from Classification Trees\n",
    "\n",
    "- **Target Variable:**  \n",
    "  In regression trees, the target is a number (e.g., weight) rather than a category.\n",
    "\n",
    "- **Splitting Criterion:**  \n",
    "  Instead of reducing entropy, regression trees aim to reduce the variance of the target variable. The goal is to create subsets where the target values are as similar as possible.\n",
    "\n",
    "### Variance Reduction\n",
    "\n",
    "For a node with target values, calculate the variance. When splitting, compute the weighted variance of the resulting subsets:\n",
    "\n",
    "$$\n",
    "\\text{Reduction in Variance} = \\text{Variance}_\\text{root} - \\left( w^\\text{left} \\cdot \\text{Variance}_\\text{left} + w^\\text{right} \\cdot \\text{Variance}_\\text{right} \\right)\n",
    "$$\n",
    "\n",
    "- **Leaf Node Prediction:**  \n",
    "  At a leaf node, the prediction is the average of the target values in that node.\n",
    "\n",
    "*Practical Example:*  \n",
    "Suppose at the root node, the overall variance of weight is $20.51$. If splitting on **ear shape** results in two subsets with weighted variances that add up to $11.67$, then the reduction in variance is $20.51 - 11.67 = 8.84$. The algorithm chooses the feature and threshold that maximize this reduction.\n",
    "\n",
    "---\n",
    "\n",
    "## Recursive Construction and Ensemble Methods\n",
    "\n",
    "### Recursion in Decision Trees\n",
    "\n",
    "The process of building a decision tree is inherently recursive:\n",
    "- **At the root:** Evaluate all features and choose the best split.\n",
    "- **For each branch:** Treat the branch as a new dataset and repeat the splitting process.\n",
    "- **Stop Splitting:** When a stopping criterion is met (pure node, maximum depth, etc.).\n",
    "\n",
    "*Analogy:*  \n",
    "Think of recursion like peeling layers of an onion—each layer (or branch) is processed in the same way until you reach the core (a leaf node).\n",
    "\n",
    "### Tree Ensembles\n",
    "\n",
    "Often, a single decision tree may not provide the best performance. By combining multiple trees, you can build a more robust model. Methods include:\n",
    "- **Bagging:** Building multiple trees on random subsets of data and averaging their predictions (e.g., Random Forests).\n",
    "- **Boosting:** Sequentially building trees where each new tree focuses on correcting the errors of the previous trees.\n",
    "\n",
    "Ensembles reduce overfitting and improve generalization by aggregating the strengths of many trees.\n",
    "\n",
    "---\n",
    "\n",
    "## Recap and Practical Considerations\n",
    "\n",
    "### Summary of Key Points\n",
    "\n",
    "- **Decision Trees:**  \n",
    "  - Break the problem into a series of binary decisions.\n",
    "  - Use a recursive algorithm to build a tree structure with root, decision, and leaf nodes.\n",
    "  \n",
    "- **Purity Measures:**  \n",
    "  - **Entropy:** Quantifies impurity using the formula \n",
    "\n",
    "$$H(p_1) = -p_1 \\log_2(p_1) - (1-p_1) \\log_2(1-p_1)$$\n",
    "\n",
    "  - **Information Gain:** Measures how much a split reduces entropy.\n",
    "  \n",
    "- **Handling Features:**\n",
    "  - **Categorical Features:** Use one-hot encoding for multi-valued features.\n",
    "  - **Continuous Features:** Determine optimal split thresholds by evaluating candidate values.\n",
    "  \n",
    "- **Regression Trees:**  \n",
    "  - Predict continuous values by minimizing the variance in the target variable.\n",
    "  - Use variance reduction as the splitting criterion.\n",
    "\n",
    "- **Recursive and Ensemble Methods:**  \n",
    "  - Build trees recursively.\n",
    "  - Improve performance using ensembles like bagging and boosting.\n",
    "\n",
    "### Practical Tips\n",
    "\n",
    "- **Parameter Tuning:**  \n",
    "  Parameters such as maximum depth, minimum samples per node, or information gain threshold can be tuned using cross-validation.\n",
    "  \n",
    "- **Overfitting:**  \n",
    "  Complex trees may overfit training data. Pruning strategies and ensemble methods help mitigate this risk.\n",
    "\n",
    "- **Implementation:**  \n",
    "  While understanding the theory is essential, many open-source libraries (like scikit-learn) implement these algorithms efficiently, allowing you to focus on model tuning rather than low-level implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b13f7a-12c2-4d82-819e-c64f492d27e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
